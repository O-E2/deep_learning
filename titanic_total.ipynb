{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29fcf32b-7b92-4cd3-8eec-b3d9e65b013d",
   "metadata": {},
   "source": [
    "# [ìš”êµ¬ì‚¬í•­ 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e4879d-b21c-47e2-a824-38e9b5cd5b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "import argparse\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "class TitanicDataset(Dataset):\n",
    "  def __init__(self, X, y):\n",
    "    self.X = torch.FloatTensor(X)\n",
    "    self.y = torch.LongTensor(y)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    feature = self.X[idx]\n",
    "    target = self.y[idx]\n",
    "    return {'input': feature, 'target': target}\n",
    "\n",
    "  def __str__(self):\n",
    "    str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "      len(self.X), self.X.shape, self.y.shape\n",
    "    )\n",
    "    return str\n",
    "\n",
    "\n",
    "class TitanicTestDataset(Dataset):\n",
    "  def __init__(self, X):\n",
    "    self.X = torch.FloatTensor(X)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    feature = self.X[idx]\n",
    "    return {'input': feature}\n",
    "\n",
    "  def __str__(self):\n",
    "    str = \"Data Size: {0}, Input Shape: {1}\".format(\n",
    "      len(self.X), self.X.shape\n",
    "    )\n",
    "    return str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c95fbb-7f7d-4e0b-bdca-4fcba5346df1",
   "metadata": {},
   "source": [
    "__init__: ì…ë ¥ ë°ì´í„°(íŠ¹ì§• X, íƒ€ê²Ÿ y)ë¥¼ íŒŒì´í† ì¹˜ê°€ ì‚¬ìš©í•˜ëŠ” ë°ì´í„° í˜•ì‹ì¸ í…ì„œ(Tensor)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì€ yë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ ëª©í‘œì´ë¯€ë¡œ Xë§Œ ê°€ì§‘ë‹ˆë‹¤.\n",
    "\n",
    "__len__: ë°ì´í„°ì…‹ì— ìˆëŠ” ì´ ìƒ˜í”Œ ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "__getitem__: ì¸ë±ìŠ¤(ì˜ˆ: dataset[10])ë¥¼ ì‚¬ìš©í•´ í•˜ë‚˜ì˜ ë°ì´í„° ìƒ˜í”Œ(í•™ìŠµìš©ì€ íŠ¹ì§•ê³¼ íƒ€ê²Ÿ, í…ŒìŠ¤íŠ¸ìš©ì€ íŠ¹ì§•ë§Œ)ì„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.\n",
    "\n",
    "__str__: print() í•¨ìˆ˜ë¡œ ì¶œë ¥ë  ë•Œ ë°ì´í„°ì…‹ í¬ê¸°ì™€ í˜•íƒœì— ëŒ€í•œ ê°„ë‹¨í•œ ë¬¸ìì—´ ì„¤ëª…ì„ ì œê³µí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a86bed-144d-4055-a024-fe64d3067e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset():\n",
    "    # ìŠ¤í¬ë¦½íŠ¸ê°€ ì‹¤í–‰ë˜ëŠ” í˜„ì¬ íŒŒì¼ ê²½ë¡œë¥¼ ê¸°ì¤€ìœ¼ë¡œ CSV íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "    try:\n",
    "        CURRENT_FILE_PATH = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError:\n",
    "        # __file__ì´ ì •ì˜ë˜ì§€ ì•Šì€ í™˜ê²½(ì˜ˆ: Jupyter)ì—ì„œëŠ” í˜„ì¬ ì‘ì—… ë””ë ‰í„°ë¦¬ë¥¼ ì‚¬ìš©\n",
    "        CURRENT_FILE_PATH = os.getcwd()\n",
    "\n",
    "    train_data_path = os.path.join(CURRENT_FILE_PATH, \"train.csv\")\n",
    "    test_data_path = os.path.join(CURRENT_FILE_PATH, \"test.csv\")\n",
    "\n",
    "    train_df = pd.read_csv(train_data_path)\n",
    "    test_df = pd.read_csv(test_data_path)\n",
    "\n",
    "    all_df = pd.concat([train_df, test_df], sort=False)\n",
    "\n",
    "    all_df = get_preprocessed_dataset_1(all_df)\n",
    "    all_df = get_preprocessed_dataset_2(all_df)\n",
    "    all_df = get_preprocessed_dataset_3(all_df)\n",
    "    all_df = get_preprocessed_dataset_4(all_df)\n",
    "    all_df = get_preprocessed_dataset_5(all_df)\n",
    "    all_df = get_preprocessed_dataset_6(all_df)\n",
    "\n",
    "    print(\"--- Preprocessed DataFrame Columns ---\")\n",
    "    print(all_df.columns)\n",
    "    print(\"--- Preprocessed DataFrame Head ---\")\n",
    "    print(all_df.head(5))\n",
    "\n",
    "    train_X = all_df[~all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1).reset_index(drop=True)\n",
    "    train_y = train_df[\"Survived\"]\n",
    "\n",
    "    test_X = all_df[all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1).reset_index(drop=True)\n",
    "\n",
    "    print(f\"\\nInput Features ({len(train_X.columns)}): {train_X.columns.tolist()}\")\n",
    "\n",
    "    dataset = TitanicDataset(train_X.values, train_y.values)\n",
    "    print(\"--- Full Train Dataset ---\")\n",
    "    print(dataset)\n",
    "\n",
    "    train_dataset, validation_dataset = random_split(dataset, [0.8, 0.2])\n",
    "    test_dataset = TitanicTestDataset(test_X.values)\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2658c92-0b0c-4da8-94c5-91f54cec0855",
   "metadata": {},
   "source": [
    "CSV ì°¾ê¸° & ë¡œë”©: íŒë‹¤ìŠ¤ë¥¼ ì´ìš©í•´ train.csvì™€ test.csv íŒŒì¼ì„ ì°¾ì•„ ì½ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ê²°í•©: í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ì „ì²˜ë¦¬ ë‹¨ê³„(ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°, ë²”ì£¼ ì¸ì½”ë”© ë“±)ë¥¼ ì–‘ìª½ ë°ì´í„°ì— ì¼ê´€ë˜ê²Œ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì „ì²˜ë¦¬: ë³´ì¡° í•¨ìˆ˜ë“¤(_1ë¶€í„° _6ê¹Œì§€)ì„ ìˆœì„œëŒ€ë¡œ í˜¸ì¶œí•˜ì—¬ ë°ì´í„°ë¥¼ ì •ì œí•˜ê³  ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë¶„ë¦¬: ê²°í•©í–ˆë˜ ë°ì´í„°ë¥¼ ë‹¤ì‹œ í•™ìŠµìš© íŠ¹ì§•(train_X), í•™ìŠµìš© ë ˆì´ë¸”(train_y), í…ŒìŠ¤íŠ¸ìš© íŠ¹ì§•(test_X)ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.\n",
    "\n",
    "Dataset ìƒì„±: ì‚¬ìš©ì ì •ì˜ í´ë˜ìŠ¤(TitanicDataset, TitanicTestDataset)ë¥¼ ì‚¬ìš©í•´ ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ê°ìŒ‰ë‹ˆë‹¤.\n",
    "\n",
    "í•™ìŠµ/ê²€ì¦ ë¶„í• : í•™ìŠµ ë°ì´í„°ë¥¼ ëª¨ë¸ í›ˆë ¨ì— ì‚¬ìš©í•  ë” í° ì„¸íŠ¸ì™€ í›ˆë ¨ ì¤‘ ì„±ëŠ¥ ê²€ì¦ì— ì‚¬ìš©í•  ë” ì‘ì€ ì„¸íŠ¸ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤ (80/20 ë¹„ìœ¨).\n",
    "\n",
    "ë°˜í™˜: íŒŒì´í† ì¹˜ì˜ DataLoaderì—ì„œ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ìµœì¢… Dataset ê°ì²´ë“¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fd7314-c2b5-4d11-a6c6-d11de2357e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset_1(all_df):\n",
    "    # Pclassë³„ Fare (ìš”ê¸ˆ) í‰ê· ê°’ì„ ì‚¬ìš©í•˜ì—¬ Fare ê²°ì¸¡ì¹˜ ë©”ìš°ê¸°\n",
    "    Fare_mean = all_df[[\"Pclass\", \"Fare\"]].groupby(\"Pclass\").mean().reset_index()\n",
    "    Fare_mean.columns = [\"Pclass\", \"Fare_mean\"]\n",
    "    all_df = pd.merge(all_df, Fare_mean, on=\"Pclass\", how=\"left\")\n",
    "    all_df.loc[(all_df[\"Fare\"].isnull()), \"Fare\"] = all_df[\"Fare_mean\"]\n",
    "    all_df = all_df.drop(columns=[\"Fare_mean\"])\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_2(all_df):\n",
    "    # nameì„ ì„¸ ê°œì˜ ì»¬ëŸ¼ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ë‹¤ì‹œ all_dfì— í•©ì¹¨\n",
    "    name_df = all_df[\"Name\"].str.split(\"[,.]\", n=2, expand=True)\n",
    "    name_df.columns = [\"family_name\", \"title\", \"name\"]\n",
    "    name_df[\"family_name\"] = name_df[\"family_name\"].str.strip()\n",
    "    name_df[\"title\"] = name_df[\"title\"].str.strip()\n",
    "    name_df[\"name\"] = name_df[\"name\"].str.strip()\n",
    "    all_df = pd.concat([all_df, name_df], axis=1)\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_3(all_df):\n",
    "    # titleë³„ Age í‰ê· ê°’ì„ ì‚¬ìš©í•˜ì—¬ Age ê²°ì¸¡ì¹˜ ë©”ìš°ê¸°\n",
    "    title_age_mean = all_df[[\"title\", \"Age\"]].groupby(\"title\").median().round().reset_index()\n",
    "    title_age_mean.columns = [\"title\", \"title_age_mean\", ]\n",
    "    all_df = pd.merge(all_df, title_age_mean, on=\"title\", how=\"left\")\n",
    "    all_df.loc[(all_df[\"Age\"].isnull()), \"Age\"] = all_df[\"title_age_mean\"]\n",
    "    all_df = all_df.drop([\"title_age_mean\"], axis=1)\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_4(all_df):\n",
    "    # ê°€ì¡±ìˆ˜(family_num) ì»¬ëŸ¼ ìƒˆë¡­ê²Œ ì¶”ê°€\n",
    "    all_df[\"family_num\"] = all_df[\"Parch\"] + all_df[\"SibSp\"]\n",
    "    # í˜¼ìíƒ‘ìŠ¹(alone) ì»¬ëŸ¼ ìƒˆë¡­ê²Œ ì¶”ê°€\n",
    "    all_df.loc[all_df[\"family_num\"] == 0, \"alone\"] = 1\n",
    "    all_df[\"alone\"].fillna(0, inplace=True)\n",
    "    # í•™ìŠµì— ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°\n",
    "    all_df = all_df.drop([\"PassengerId\", \"Name\", \"family_name\", \"name\", \"Ticket\", \"Cabin\"], axis=1)\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_5(all_df):\n",
    "    # title ê°’ ê°œìˆ˜ ì¤„ì´ê¸°\n",
    "    all_df.loc[\n",
    "    ~(\n",
    "            (all_df[\"title\"] == \"Mr\") |\n",
    "            (all_df[\"title\"] == \"Miss\") |\n",
    "            (all_df[\"title\"] == \"Mrs\") |\n",
    "            (all_df[\"title\"] == \"Master\")\n",
    "    ),\n",
    "    \"title\"\n",
    "    ] = \"other\"\n",
    "    all_df[\"Embarked\"].fillna(\"missing\", inplace=True)\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_6(all_df):\n",
    "    # ì¹´í…Œê³ ë¦¬ ë³€ìˆ˜ë¥¼ LabelEncoderë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ˜ì¹˜ê°’ìœ¼ë¡œ ë³€ê²½í•˜ê¸°\n",
    "    category_features = all_df.columns[all_df.dtypes == \"object\"]\n",
    "    for category_feature in category_features:\n",
    "        le = LabelEncoder()\n",
    "        if all_df[category_feature].dtypes == \"object\":\n",
    "          le = le.fit(all_df[category_feature])\n",
    "          all_df[category_feature] = le.transform(all_df[category_feature])\n",
    "    return all_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fba863-8cde-4aec-a139-d0edd92f0f2f",
   "metadata": {},
   "source": [
    "_1: ëˆ„ë½ëœ ìš”ê¸ˆ ê°’ì„ ê° ìŠ¹ê° ë“±ê¸‰ì˜ í‰ê·  ìš”ê¸ˆìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤. \n",
    "\n",
    "_2: Name ì—´ì—ì„œ í˜¸ì¹­ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. \n",
    "\n",
    "_3: ëˆ„ë½ëœ ë‚˜ì´ê°’ì„ ì¶”ì¶œëœ titleê³¼ ì—°ê´€ëœ ë‚˜ì´ì˜ ì¤‘ì•™ê°’ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤. \n",
    "\n",
    "_4: ìƒˆë¡œìš´ íŠ¹ì§•ì¸ family_numê³¼ aloneì„ ë§Œë“­ë‹ˆë‹¤. ëª¨ë¸ë§ì— ë¶ˆí•„ìš”í•˜ë‹¤ê³  íŒë‹¨ë˜ëŠ” ì—´ì„ ì œê±°í•©ë‹ˆë‹¤.\n",
    "\n",
    "_5: ë“œë¬¸ í˜¸ì¹­ë“¤ì„ 'other'ë¡œ ê·¸ë£¹í™”í•˜ì—¬ title ì—´ì„ ë‹¨ìˆœí™”í•©ë‹ˆë‹¤. ëˆ„ë½ëœ íƒ‘ìŠ¹ í•­êµ¬ ê°’ì„ 'missing'ì´ë¼ëŠ” ì„ì‹œ ê°’ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤.\n",
    "\n",
    "_6: ë²”ì£¼í˜• ë¬¸ìì—´ ì—´ì„ Label Encodingì„ ì‚¬ìš©í•´ ìˆ«ì í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "  # 1ë²ˆ ë¸”ë¡ì˜ ì „ì²˜ë¦¬ í•¨ìˆ˜ í˜¸ì¶œ (ì´ í•¨ìˆ˜ëŠ” ë‹¤ë¥¸ íŒŒì¼ì— ì •ì˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •)\n",
    "  train_dataset, validation_dataset, test_dataset = get_preprocessed_dataset()\n",
    "\n",
    "  print(f\"\\nTrain dataset size: {len(train_dataset)}\")\n",
    "  print(f\"Validation dataset size: {len(validation_dataset)}\")\n",
    "  print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "  # wandb.configì—ì„œ ë°°ì¹˜ í¬ê¸°ë¥¼ ê°€ì ¸ì™€ DataLoader ìƒì„±\n",
    "  train_data_loader = DataLoader(dataset=train_dataset, batch_size=wandb.config.batch_size, shuffle=True)\n",
    "  validation_data_loader = DataLoader(dataset=validation_dataset, batch_size=len(validation_dataset))\n",
    "  test_data_loader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset))\n",
    "\n",
    "  return train_data_loader, validation_data_loader, test_data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a17a40-a384-43be-8bbb-1e24bfc112d3",
   "metadata": {},
   "source": [
    "ë°ì´í„°ë“¤ì„ DataLoaderë¡œ ê°ì‹¸ì„œ ëª¨ë¸ í•™ìŠµ ì‹œ ë°ì´í„°ë¥¼ ë¯¸ë‹ˆë°°ì¹˜ ë‹¨ìœ„ë¡œ íš¨ìœ¨ì ìœ¼ë¡œ ê³µê¸‰í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17d1c1c-9b50-4202-9183-476e204f2890",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "  def __init__(self, n_input, n_output):\n",
    "    super().__init__()\n",
    "\n",
    "    self.model = nn.Sequential(\n",
    "      nn.Linear(n_input, wandb.config.n_hidden_unit_list[0]),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(wandb.config.n_hidden_unit_list[0], wandb.config.n_hidden_unit_list[1]),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(wandb.config.n_hidden_unit_list[1], n_output),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.model(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_model_and_optimizer():\n",
    "  # ì…ë ¥ í”¼ì²˜ 10ê°œ (Pclass, Sex, Age, SibSp, Parch, Fare, Embarked, title, family_num, alone)\n",
    "  # ì¶œë ¥ í´ë˜ìŠ¤ 2ê°œ (0: ì‚¬ë§, 1: ìƒì¡´)\n",
    "  my_model = MyModel(n_input=10, n_output=2)\n",
    "  optimizer = optim.SGD(my_model.parameters(), lr=wandb.config.learning_rate)\n",
    "\n",
    "  return my_model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157f3e50-c463-43a2-a0ac-af757451220e",
   "metadata": {},
   "source": [
    "__init__: ëª¨ë¸ì„ êµ¬ì„±í•˜ëŠ” ì¸µë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ì…ë ¥ì¸µ, 2ê°œì˜ ì€ë‹‰ì¸µ, ì¶œë ¥ì¸µìœ¼ë¡œ êµ¬ì„±ëœ ê°„ë‹¨í•œ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ ì…ë‹ˆë‹¤. ê° ì¸µì˜ ë‰´ëŸ° ìˆ˜ëŠ” wandb.configì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "\n",
    "forward: ì…ë ¥ ë°ì´í„°(x)ê°€ ëª¨ë¸ì˜ ì¸µë“¤ì„ ì–´ë–¤ ìˆœì„œë¡œ í†µê³¼í•˜ì—¬ ìµœì¢… ì¶œë ¥ì„ ë§Œë“¤ì–´ë‚´ëŠ”ì§€ ì •ì˜í•©ë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca94715-daae-4c95-b640-0664499ea533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, optimizer, train_data_loader, validation_data_loader):\n",
    "  n_epochs = wandb.config.epochs\n",
    "  loss_fn = nn.CrossEntropyLoss()  # ë¶„ë¥˜ ë¬¸ì œì´ë¯€ë¡œ CrossEntropyLoss ì‚¬ìš©\n",
    "  next_print_epoch = 100\n",
    "\n",
    "  for epoch in range(1, n_epochs + 1):\n",
    "    loss_train = 0.0\n",
    "    num_trains = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    model.train() # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì •\n",
    "    for batch in train_data_loader:\n",
    "      # Datasetì´ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì´ë¯€ë¡œ í‚¤ë¡œ ì ‘ê·¼\n",
    "      input = batch['input']\n",
    "      target = batch['target']\n",
    "\n",
    "      output_train = model(input)\n",
    "      loss = loss_fn(output_train, target)\n",
    "      loss_train += loss.item()\n",
    "      num_trains += 1\n",
    "\n",
    "      # ì •í™•ë„ ê³„ì‚°\n",
    "      _, predicted = torch.max(output_train.data, 1)\n",
    "      total_train += target.size(0)\n",
    "      correct_train += (predicted == target).sum().item()\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    loss_validation = 0.0\n",
    "    num_validations = 0\n",
    "    correct_validation = 0\n",
    "    total_validation = 0\n",
    "\n",
    "    model.eval() # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •\n",
    "    with torch.no_grad():\n",
    "      for batch in validation_data_loader:\n",
    "        input = batch['input']\n",
    "        target = batch['target']\n",
    "\n",
    "        output_validation = model(input)\n",
    "        loss = loss_fn(output_validation, target)\n",
    "        loss_validation += loss.item()\n",
    "        num_validations += 1\n",
    "\n",
    "        # ì •í™•ë„ ê³„ì‚°\n",
    "        _, predicted = torch.max(output_validation.data, 1)\n",
    "        total_validation += target.size(0)\n",
    "        correct_validation += (predicted == target).sum().item()\n",
    "\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    validation_accuracy = 100 * correct_validation / total_validation\n",
    "\n",
    "    wandb.log({\n",
    "      \"Epoch\": epoch,\n",
    "      \"Training loss\": loss_train / num_trains,\n",
    "      \"Validation loss\": loss_validation / num_validations,\n",
    "      \"Training accuracy\": train_accuracy,\n",
    "      \"Validation accuracy\": validation_accuracy\n",
    "    })\n",
    "\n",
    "    if epoch % next_print_epoch == 0 or epoch == 1:\n",
    "      print(\n",
    "        f\"Epoch {epoch}, \"\n",
    "        f\"Training loss {loss_train / num_trains:.4f}, \"\n",
    "        f\"Validation loss {loss_validation / num_validations:.4f}, \"\n",
    "        f\"Training Acc {train_accuracy:.2f}%, \"\n",
    "        f\"Validation Acc {validation_accuracy:.2f}%\"\n",
    "      )\n",
    "      if epoch >= next_print_epoch:\n",
    "          next_print_epoch += 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38943a86-cdd2-48a4-924c-d406f05f6cba",
   "metadata": {},
   "source": [
    "* ì—í¬í¬ ë°˜ë³µ: ì •í•´ì§„ íšŸìˆ˜ë§Œí¼ ì „ì²´ ë°ì´í„°ì…‹ í•™ìŠµì„ ë°˜ë³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "* í•™ìŠµ ëª¨ë“œ: model.train()ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµ ìƒíƒœë¡œ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "* ë¯¸ë‹ˆë°°ì¹˜ í•™ìŠµ: train_data_loaderì—ì„œ ë°ì´í„°ë¥¼ ë¯¸ë‹ˆë°°ì¹˜ ë‹¨ìœ„ë¡œ ê°€ì ¸ì™€ ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "* ëª¨ë¸ ì˜ˆì¸¡ (model(input))\n",
    "\n",
    "* ì†ì‹¤ ê³„ì‚° (loss_fn)\n",
    "\n",
    "* ì—­ì „íŒŒ (loss.backward())\n",
    "\n",
    "* ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ (optimizer.step())\n",
    "\n",
    "* í•™ìŠµ ì†ì‹¤ê³¼ ì •í™•ë„ ëˆ„ì  ê³„ì‚°\n",
    "\n",
    "* í‰ê°€ ëª¨ë“œ: model.eval()ìœ¼ë¡œ ëª¨ë¸ì„ í‰ê°€ ìƒíƒœë¡œ ì„¤ì •í•©ë‹ˆë‹¤ (ë“œë¡­ì•„ì›ƒ ë“± ë¹„í™œì„±í™”).\n",
    "\n",
    "* ê²€ì¦: validation_data_loaderì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ëª¨ë¸ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³ , ê²€ì¦ ì†ì‹¤ê³¼ ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤ (ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ëŠ” ì•ˆ í•¨).\n",
    "\n",
    "* ë¡œê¹…: ê° ì—í¬í¬ì˜ í•™ìŠµ/ê²€ì¦ ì†ì‹¤ê³¼ ì •í™•ë„ë¥¼ wandbì— ê¸°ë¡í•©ë‹ˆë‹¤.\n",
    "\n",
    "* ì¶œë ¥: ì£¼ê¸°ì ìœ¼ë¡œ í•™ìŠµ ì§„í–‰ ìƒí™©(ì†ì‹¤, ì •í™•ë„)ì„ í™”ë©´ì— ì¶œë ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7edecf4-c9f1-4da6-9493-9dd64275b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "  current_time_str = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "  config = {\n",
    "    'epochs': args.epochs,\n",
    "    'batch_size': args.batch_size,\n",
    "    'learning_rate': 1e-3,\n",
    "    'n_hidden_unit_list': [20, 20], # ì€ë‹‰ì¸µ ì„¤ì •ì€ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "  }\n",
    "\n",
    "  wandb.init(\n",
    "    mode=\"online\" if args.wandb else \"disabled\",\n",
    "    project=\"titanic_survival_prediction\", # wandb í”„ë¡œì íŠ¸ëª… ë³€ê²½\n",
    "    notes=\"Titanic survival prediction with MLP\", # wandb ë…¸íŠ¸ ë³€ê²½\n",
    "    tags=[\"mlp\", \"titanic\"], # wandb íƒœê·¸ ë³€ê²½\n",
    "    name=current_time_str,\n",
    "    config=config\n",
    "  )\n",
    "  print(\"--- wandb arguments ---\")\n",
    "  print(args)\n",
    "  print(\"--- wandb config ---\")\n",
    "  print(wandb.config)\n",
    "\n",
    "  # test_data_loaderë„ ë°˜í™˜ë˜ì§€ë§Œ, training_loopì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ\n",
    "  train_data_loader, validation_data_loader, test_data_loader = get_data()\n",
    "\n",
    "  linear_model, optimizer = get_model_and_optimizer()\n",
    "\n",
    "  print(\"\\n\" + \"#\" * 50)\n",
    "  print(\"Start Training...\")\n",
    "  training_loop(\n",
    "    model=linear_model,\n",
    "    optimizer=optimizer,\n",
    "    train_data_loader=train_data_loader,\n",
    "    validation_data_loader=validation_data_loader\n",
    "  )\n",
    "  print(\"Training Finished.\")\n",
    "  print(\"#\" * 50 + \"\\n\")\n",
    "\n",
    "  wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07456eb6-9fe5-4bf7-b9f5-174b145b372d",
   "metadata": {},
   "source": [
    "ì„¤ì • ë¡œë“œ: wandb ì‹¤í—˜ ì„¤ì •(config)ì„ ì •ì˜í•©ë‹ˆë‹¤ (ì—í¬í¬ ìˆ˜, ë°°ì¹˜ í¬ê¸° ë“±).\n",
    "\n",
    "wandb ì´ˆê¸°í™”: ì‹¤í—˜ ì¶”ì ì„ ìœ„í•´ wandbë¥¼ ì„¤ì •í•˜ê³  ì‹œì‘í•©ë‹ˆë‹¤. í”„ë¡œì íŠ¸ ì´ë¦„, ë…¸íŠ¸, íƒœê·¸ ë“±ì„ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë°ì´í„° ë¡œë”©: get_data() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "\n",
    "ëª¨ë¸/ì˜µí‹°ë§ˆì´ì € ìƒì„±: get_model_and_optimizer() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ëª¨ë¸ê³¼ ì˜µí‹°ë§ˆì´ì €ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤.\n",
    "\n",
    "í•™ìŠµ ì‹œì‘: training_loop() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ëª¨ë¸ í•™ìŠµ ë° ê²€ì¦ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
    "\n",
    "wandb ì¢…ë£Œ: ì‹¤í—˜ ê¸°ë¡ì„ ë§ˆì¹˜ê³  wandbë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68787fd8-7b3e-4f84-b61d-b86b6b3fbf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  parser = argparse.ArgumentParser()\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"--wandb\", action=argparse.BooleanOptionalAction, default=False, help=\"True or False\"\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"-b\", \"--batch_size\", type=int, default=16, help=\"Batch size (int, default: 16)\" # ê¸°ë³¸ ë°°ì¹˜ ì‚¬ì´ì¦ˆ 16ìœ¼ë¡œ ë³€ê²½\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"-e\", \"--epochs\", type=int, default=1_000, help=\"Number of training epochs (int, default:1_000)\"\n",
    "  )\n",
    "\n",
    "  args = parser.parse_args()\n",
    "\n",
    "  main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83765600-acf9-41c7-a241-8e4168faff7f",
   "metadata": {},
   "source": [
    "í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰í•  ë•Œ --epochs, --batch_size ê°™ì€ ì¸ìë¥¼ ë°›ì„ ìˆ˜ ìˆê²Œ ì„¤ì •í•©ë‹ˆë‹¤. ë°›ì€ ì¸ìë¥¼ main í•¨ìˆ˜ì— ì „ë‹¬í•˜ì—¬ ì‹¤í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b2bc30-c744-4282-be83-da1778bd80d4",
   "metadata": {},
   "source": [
    "Wandb URL\n",
    "https://wandb.ai/cyun0407-korea-university-of-technology-and-education/titanic_survival_prediction/runs/i4pqcafr  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8138a5-2878-47bd-9a44-2853765f5a3c",
   "metadata": {},
   "source": [
    "[ìš”êµ¬ì‚¬í•­ 2]\n",
    "\n",
    "Wansb URL \n",
    "https://wandb.ai/cyun0407-korea-university-of-technology-and-education/titanic_hyperparameter_tuning?nw=nwusercyun0407\n",
    "\n",
    "ë” ë‚˜ì€ ì„±ëŠ¥ì„ ì‚°ì¶œí•˜ëŠ” Activation Function : ReLU\n",
    "ë” ë‚˜ì€ ì„±ëŠ¥ì„ ì‚°ì¶œí•˜ëŠ” Batch Size : 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287d3b1d-cb29-48fc-b45e-6670319fe9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_best_hyperparameters.py\n",
    "import os, json, sys, argparse, copy\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pathlib import Path\n",
    "\n",
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, X, y): self.X = torch.FloatTensor(X); self.y = torch.LongTensor(y)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx): return {'input': self.X[idx], 'target': self.y[idx]}\n",
    "    def __str__(self): return f\"Data Size: {len(self.X)}, Input Shape: {self.X.shape}, Target Shape: {self.y.shape}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d55eed-b462-412b-86bc-1cff947973dc",
   "metadata": {},
   "source": [
    "íŒŒì´í† ì¹˜ DataLoaderê°€ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë°ì´í„°ì…‹ì˜ êµ¬ì¡°(ë°ì´í„° ë¡œë”©, ê¸¸ì´ ë°˜í™˜, íŠ¹ì • í•­ëª© ì ‘ê·¼ ë°©ë²•)ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. TitanicDatasetì€ í•™ìŠµ ë° ê²€ì¦ ë°ì´í„°ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1814570a-a867-4058-b351-16f0fcd68e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_preprocessed_dataset_1(all_df): \n",
    "    Fare_mean = all_df[[\"Pclass\", \"Fare\"]].groupby(\"Pclass\").mean().reset_index(); Fare_mean.columns = [\"Pclass\", \"Fare_mean\"]\n",
    "    all_df = pd.merge(all_df, Fare_mean, on=\"Pclass\", how=\"left\"); all_df.loc[all_df[\"Fare\"].isnull(), \"Fare\"] = all_df[\"Fare_mean\"]\n",
    "    return all_df.drop(columns=[\"Fare_mean\"])\n",
    "def get_preprocessed_dataset_2(all_df): \n",
    "    name_df = all_df[\"Name\"].str.split(\"[,.]\", n=2, expand=True)\n",
    "    if name_df.shape[1] == 3:\n",
    "        name_df.columns = [\"family_name\", \"title\", \"name\"]; name_df[\"family_name\"] = name_df[\"family_name\"].str.strip(); name_df[\"title\"] = name_df[\"title\"].str.strip(); name_df[\"name\"] = name_df[\"name\"].str.strip()\n",
    "        all_df = pd.concat([all_df, name_df], axis=1)\n",
    "    else: all_df['title'] = 'unknown' # ì´ë¦„ ë¶„ë¦¬ ì‹¤íŒ¨ ì‹œ 'unknown' title ì¶”ê°€\n",
    "    return all_df\n",
    "def get_preprocessed_dataset_3(all_df): \n",
    "    if 'title' not in all_df.columns:\n",
    "        if all_df['Age'].isnull().any(): all_df['Age'] = all_df['Age'].fillna(all_df['Age'].median())\n",
    "        return all_df\n",
    "    title_age_median = all_df[[\"title\", \"Age\"]].groupby(\"title\").median().round().reset_index(); title_age_median.columns = [\"title\", \"title_age_median\"]\n",
    "    all_df = pd.merge(all_df, title_age_median, on=\"title\", how=\"left\"); all_df.loc[all_df[\"Age\"].isnull(), \"Age\"] = all_df[\"title_age_median\"]\n",
    "    if \"title_age_median\" in all_df.columns: all_df = all_df.drop([\"title_age_median\"], axis=1)\n",
    "    return all_df\n",
    "def get_preprocessed_dataset_4(all_df): \n",
    "    all_df[\"family_num\"] = all_df[\"Parch\"] + all_df[\"SibSp\"]; all_df.loc[all_df[\"family_num\"] == 0, \"alone\"] = 1; all_df[\"alone\"] = all_df[\"alone\"].fillna(0).astype(float)\n",
    "    cols_to_drop = [\"PassengerId\", \"Name\", \"family_name\", \"name\", \"Ticket\", \"Cabin\"]; existing_cols_to_drop = [col for col in cols_to_drop if col in all_df.columns]\n",
    "    return all_df.drop(existing_cols_to_drop, axis=1)\n",
    "def get_preprocessed_dataset_5(all_df):\n",
    "    if 'title' in all_df.columns: all_df.loc[~((all_df[\"title\"] == \"Mr\") | (all_df[\"title\"] == \"Miss\") | (all_df[\"title\"] == \"Mrs\") | (all_df[\"title\"] == \"Master\")), \"title\"] = \"other\"\n",
    "    all_df[\"Embarked\"] = all_df[\"Embarked\"].fillna(\"missing\")\n",
    "    return all_df\n",
    "def get_preprocessed_dataset_6(all_df): \n",
    "    category_features = all_df.select_dtypes(include=['object']).columns\n",
    "    for cat_feat in category_features:\n",
    "        le = LabelEncoder(); valid_indices = all_df[cat_feat].notna()\n",
    "        if valid_indices.any():\n",
    "            all_df.loc[valid_indices, cat_feat] = le.fit_transform(all_df.loc[valid_indices, cat_feat])\n",
    "            try: all_df[cat_feat] = pd.to_numeric(all_df[cat_feat])\n",
    "            except ValueError: pass\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bdd747-11c6-4b6d-a3f3-11deb5c95ab7",
   "metadata": {},
   "source": [
    "íƒ€ì´íƒ€ë‹‰ ì›ë³¸ ë°ì´í„°ë¥¼ ë‹¨ê³„ë³„ë¡œ ê°€ê³µí•˜ëŠ” í•¨ìˆ˜ë“¤ì…ë‹ˆë‹¤. ê²°ì¸¡ì¹˜ ì²˜ë¦¬, íŠ¹ì§• ìƒì„± ë° ì¶”ì¶œ, ë²”ì£¼í˜• ë°ì´í„°ì˜ ìˆ˜ì¹˜í™” ë“±ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ed857c-d889-4049-be19-e4349cee6b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_data_raw(): \n",
    "    \"\"\"CSV ë¡œë“œ, ì „ì²˜ë¦¬, í•™ìŠµ/ê²€ì¦ìš© ë°ì´í„° ë°°ì—´ ë°˜í™˜\"\"\"\n",
    "    try: CURRENT_FILE_PATH = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError: CURRENT_FILE_PATH = os.getcwd()\n",
    "    train_data_path = os.path.join(CURRENT_FILE_PATH, \"train.csv\")\n",
    "    test_data_path = os.path.join(CURRENT_FILE_PATH, \"test.csv\") # testëŠ” ë¡œë“œë§Œ í•˜ê³  ì‚¬ìš© ì•ˆí•¨\n",
    "    try:\n",
    "        train_df = pd.read_csv(train_data_path)\n",
    "        test_df = pd.read_csv(test_data_path)\n",
    "    except FileNotFoundError as e: print(f\"Error: {e}\"); sys.exit(1)\n",
    "\n",
    "    all_df = pd.concat([train_df, test_df], sort=False) # ì „ì²˜ë¦¬ ì¼ê´€ì„± ìœ„í•´ í•©ì¹¨\n",
    "    all_df = get_preprocessed_dataset_1(all_df); all_df = get_preprocessed_dataset_2(all_df)\n",
    "    all_df = get_preprocessed_dataset_3(all_df); all_df = get_preprocessed_dataset_4(all_df)\n",
    "    all_df = get_preprocessed_dataset_5(all_df); all_df = get_preprocessed_dataset_6(all_df)\n",
    "    train_X_df = all_df[~all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1)\n",
    "    train_y = train_df[\"Survived\"]\n",
    "\n",
    "    # í•™ìŠµ ë°ì´í„°ì— ëŒ€í•´ì„œë§Œ ìµœì¢… NaN ë° íƒ€ì… í™•ì¸\n",
    "    print(\"\\n--- Final Data Check & Fill NaNs (Train Data Only) ---\")\n",
    "    columns_to_check = train_X_df.columns\n",
    "    df = train_X_df\n",
    "    for col in columns_to_check:\n",
    "        if df[col].dtype == 'object':\n",
    "            try: df[col] = pd.to_numeric(df[col])\n",
    "            except ValueError: df[col] = 0 # ë³€í™˜ ë¶ˆê°€ì‹œ 0ìœ¼ë¡œ ì±„ì›€\n",
    "        if df[col].isnull().any(): df[col] = df[col].fillna(0) # NaN 0ìœ¼ë¡œ ì±„ì›€\n",
    "    train_X = train_X_df.reset_index(drop=True)\n",
    "\n",
    "    print(\"Train data preprocessing finished.\")\n",
    "    return train_X.values, train_y.values # í•™ìŠµ ë°ì´í„°(X, y)ë§Œ ë°˜í™˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc9033a-40eb-4db0-94e3-abb9f1127b09",
   "metadata": {},
   "source": [
    "CSV íŒŒì¼ì„ ë¡œë“œí•˜ê³  ìœ„ì˜ ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤ì„ ìˆœì„œëŒ€ë¡œ í˜¸ì¶œí•©ë‹ˆë‹¤. ìµœì¢…ì ìœ¼ë¡œ í•™ìŠµ ë°ì´í„°ë§Œ ë„˜íŒŒì´ ë°°ì—´ í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bef9a48-c6d2-4eda-b441-0f4f570a9e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(train_X, train_y, batch_size_config): \n",
    "    \"\"\"í•™ìŠµ/ê²€ì¦ DataLoader ìƒì„±\"\"\"\n",
    "    dataset = TitanicDataset(train_X, train_y)\n",
    "    generator = torch.Generator().manual_seed(42) # ë°ì´í„° ë¶„í•  ì¬í˜„ì„± ìœ„í•œ ì‹œë“œ ê³ ì •\n",
    "    train_dataset, validation_dataset = random_split(dataset, [0.8, 0.2], generator=generator)\n",
    "    print(f\"\\nTrain size: {len(train_dataset)}, Validation size: {len(validation_dataset)}\")\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size_config, shuffle=True)\n",
    "    validation_loader = DataLoader(dataset=validation_dataset, batch_size=len(validation_dataset)) # ê²€ì¦ì€ ì „ì²´ ë°ì´í„° ì‚¬ìš©\n",
    "    return train_loader, validation_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3552c139-f80c-4061-8b6f-c789e8713075",
   "metadata": {},
   "source": [
    "ì „ì²˜ë¦¬ëœ í•™ìŠµ ë°ì´í„°(X, y)ì™€ ë°°ì¹˜ í¬ê¸°ë¥¼ ë°›ì•„, ë°ì´í„°ë¥¼ í•™ìŠµìš©ê³¼ ê²€ì¦ìš©ìœ¼ë¡œ 8:2 ë¹„ìœ¨ë¡œ ë‚˜ëˆˆ ë’¤ ê°ê°ì— ëŒ€í•œ DataLoader ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a9fe2a-cfb2-4ad1-a35d-8a4ed5686014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module): \n",
    "    \"\"\"í™œì„±í™” í•¨ìˆ˜ì™€ ì€ë‹‰ì¸µ í¬ê¸°ë¥¼ ì¸ìë¡œ ë°›ëŠ” MLP ëª¨ë¸\"\"\"\n",
    "    def __init__(self, n_input, n_output, activation_fn_class, n_hidden1=20, n_hidden2=20):\n",
    "        super().__init__(); activation = activation_fn_class()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_input, n_hidden1), activation,\n",
    "            nn.Linear(n_hidden1, n_hidden2), activation,\n",
    "            nn.Linear(n_hidden2, n_output)\n",
    "        )\n",
    "    def forward(self, x): return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21456c8-02d3-4a63-a32c-af3be398f8f6",
   "metadata": {},
   "source": [
    "íŒŒì´í† ì¹˜ nn.Moduleì„ ìƒì†ë°›ì•„ ê°„ë‹¨í•œ ëª¨ë¸ êµ¬ì¡°ë¥¼ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6302d8b-8c56-495f-90d5-579c118a788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_optimizer(activation_fn_class, learning_rate, n_hidden1=20, n_hidden2=20): # ... (ì´ì „ê³¼ ë™ì¼) ...\n",
    "    \"\"\"ëª¨ë¸ ê°ì²´ì™€ SGD ì˜µí‹°ë§ˆì´ì € ìƒì„±\"\"\"\n",
    "    model = MyModel(10, 2, activation_fn_class, n_hidden1, n_hidden2) # ì…ë ¥ 10ê°œ, ì¶œë ¥ 2ê°œ ê³ ì •\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303ad0e1-8893-42d9-8d1a-c90b0851a04c",
   "metadata": {},
   "source": [
    "MyModel í´ë˜ìŠ¤ë¥¼ ì´ìš©í•´ ëª¨ë¸ ê°ì²´ë¥¼ ìƒì„±í•˜ê³ , í•™ìŠµì— ì‚¬ìš©í•  SGD ì˜µí‹°ë§ˆì´ì €ë¥¼ ì„¤ì •í•˜ì—¬ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7e520c-37a3-49ac-ad4a-084895580032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- í•™ìŠµ ë£¨í”„: ìµœì¢… ì •í™•ë„ë§Œ ë°˜í™˜ ---\n",
    "def simplified_training_loop(model, optimizer, train_data_loader, validation_data_loader, n_epochs, loss_fn):\n",
    "    \"\"\"ëª¨ë¸ í•™ìŠµ/ê²€ì¦ ìˆ˜í–‰ í›„ ìµœì¢… ê²€ì¦ ì •í™•ë„ ë°˜í™˜ (ìµœê³  ê¸°ë¡ ì—†ìŒ)\"\"\"\n",
    "    print(f\"\\nStart Training for {n_epochs} epochs...\")\n",
    "    final_validation_accuracy = 0.0 # ë§ˆì§€ë§‰ ì—í¬í¬ ì •í™•ë„ ì €ì¥ìš©\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # --- í•™ìŠµ \n",
    "        model.train(); loss_train_epoch = 0.0; correct_train_epoch = 0; total_train_samples = 0\n",
    "        for batch in train_data_loader: # ... (í•™ìŠµ ë¡œì§ ë™ì¼) ...\n",
    "            input_data = batch['input']; target = batch['target']; optimizer.zero_grad(); output_train = model(input_data); loss = loss_fn(output_train, target); loss.backward(); optimizer.step()\n",
    "            loss_train_epoch += loss.item() * input_data.size(0); _, predicted = torch.max(output_train.data, 1); total_train_samples += target.size(0); correct_train_epoch += (predicted == target).sum().item()\n",
    "        avg_loss_train = loss_train_epoch / total_train_samples if total_train_samples else 0\n",
    "        train_accuracy = 100 * correct_train_epoch / total_train_samples if total_train_samples else 0\n",
    "\n",
    "        # --- ê²€ì¦ \n",
    "        model.eval(); loss_val_epoch = 0.0; correct_val_epoch = 0; total_val_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in validation_data_loader: # ... (ê²€ì¦ ë¡œì§ ë™ì¼) ...\n",
    "                input_data = batch['input']; target = batch['target']; output_validation = model(input_data); loss = loss_fn(output_validation, target); loss_val_epoch += loss.item() * input_data.size(0)\n",
    "                _, predicted = torch.max(output_validation.data, 1); total_val_samples += target.size(0); correct_val_epoch += (predicted == target).sum().item()\n",
    "        avg_loss_validation = loss_val_epoch / total_val_samples if total_val_samples else 0\n",
    "        validation_accuracy = 100 * correct_val_epoch / total_val_samples if total_val_samples else 0\n",
    "        final_validation_accuracy = validation_accuracy # ë§ˆì§€ë§‰ ê°’ ê°±ì‹ \n",
    "\n",
    "        # ë¡œê¹… ë° ì¶œë ¥ \n",
    "        if wandb.run: wandb.log({\"Epoch\": epoch, \"Training loss\": avg_loss_train, \"Validation loss\": avg_loss_validation, \"Training accuracy\": train_accuracy, \"Validation accuracy\": validation_accuracy})\n",
    "        if epoch % 100 == 0 or epoch == 1: print(f\"Epoch {epoch:>{len(str(n_epochs))}}/{n_epochs}, Train Loss: {avg_loss_train:.4f}, Acc: {train_accuracy:.2f}% | Val Loss: {avg_loss_validation:.4f}, Acc: {validation_accuracy:.2f}%\")\n",
    "\n",
    "    print(f\"\\n--- Run Finished --- Final Val Acc: {final_validation_accuracy:.2f}%\")\n",
    "    return final_validation_accuracy # ìµœì¢… ê²€ì¦ ì •í™•ë„ë§Œ ë°˜í™˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd276ad-6f18-44b4-9e7b-f2729f69d2b9",
   "metadata": {},
   "source": [
    "ëª¨ë¸ í•™ìŠµê³¼ ê²€ì¦ì„ ì§€ì •ëœ ì—í¬í¬ë§Œí¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21936d0-7ce6-4ab7-bc56-095143a45526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ë° ìµœì  ì¡°í•© ì €ì¥ ---\n",
    "def find_hyperparameters(args):\n",
    "    \"\"\"ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° 'ì¡°í•©'ë§Œ ì°¾ì•„ json íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "    current_time_str = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    # ì‹¤í—˜ ì„¤ì •\n",
    "    batch_sizes_to_test = [16, 32, 64, 128]\n",
    "    activation_functions_to_test = {\"Sigmoid\": nn.Sigmoid, \"ReLU\": nn.ReLU, \"ELU\": nn.ELU, \"LeakyReLU\": nn.LeakyReLU}\n",
    "    learning_rate = 1e-3; n_hidden_units = [20, 20]; loss_fn = nn.CrossEntropyLoss()\n",
    "    # ë°ì´í„° ë¡œë”©\n",
    "    print(\"Preprocessing data for tuning...\"); train_X, train_y = get_preprocessed_data_raw()[:2] # í•™ìŠµ ë°ì´í„°ë§Œ í•„ìš”\n",
    "    # ìµœê³  ì„±ëŠ¥ ì¶”ì \n",
    "    overall_best_accuracy = -1.0; overall_best_config = {}\n",
    "\n",
    "    # ì‹¤í—˜ ë£¨í”„\n",
    "    for batch_size in batch_sizes_to_test:\n",
    "        print(f\"\\n{'='*25} Testing Batch Size: {batch_size} {'='*25}\")\n",
    "        train_loader, validation_loader = get_dataloaders(train_X, train_y, batch_size)\n",
    "        for activation_name, activation_fn_class in activation_functions_to_test.items():\n",
    "            print(f\"\\n--- Testing Activation: {activation_name} ---\")\n",
    "            config = {'epochs': args.epochs, 'batch_size': batch_size, 'learning_rate': learning_rate, 'n_hidden_unit_list': n_hidden_units, 'activation_function': activation_name}\n",
    "            run_name = f\"Tune_{activation_name}_BS{batch_size}_{current_time_str}\"\n",
    "            run = wandb.init(mode=\"online\" if args.wandb else \"disabled\", project=\"titanic_hyperparameter_tuning\", name=run_name, config=config, reinit=True)\n",
    "            print(\"--- Run Config ---\"); print(config)\n",
    "            model, optimizer = get_model_and_optimizer(activation_fn_class, config['learning_rate'], n_hidden1=config['n_hidden_unit_list'][0], n_hidden2=config['n_hidden_unit_list'][1])\n",
    "\n",
    "            # ê°„ì†Œí™”ëœ í•™ìŠµ ë£¨í”„ ì‹¤í–‰, ìµœì¢… ì •í™•ë„ ë°›ê¸°\n",
    "            final_accuracy_run = simplified_training_loop(model, optimizer, train_loader, validation_loader, config['epochs'], loss_fn)\n",
    "\n",
    "            # ìµœê³  ì¡°í•© ê°±ì‹  í™•ì¸ (ìµœì¢… ì •í™•ë„ ê¸°ì¤€)\n",
    "            if final_accuracy_run > overall_best_accuracy:\n",
    "                overall_best_accuracy = final_accuracy_run\n",
    "                overall_best_config = config # configë§Œ ì €ì¥\n",
    "                print(f\"ğŸ… New Best Config Found! Final Val Acc: {overall_best_accuracy:.2f}%, Config: {activation_name}/BS={batch_size}\")\n",
    "            run.finish()\n",
    "\n",
    "    # ìµœì¢… ê²°ê³¼ ìš”ì•½ ë° ì €ì¥ (configë§Œ)\n",
    "    print(\"\\n\" + \"=\" * 60); print(\" Hyperparameter Tuning Finished (Config Only) \"); print(\"=\" * 60)\n",
    "    if overall_best_config:\n",
    "        print(f\"Overall Best Final Val Acc: {overall_best_accuracy:.2f}%\")\n",
    "        print(f\"Best Config Found: {overall_best_config}\")\n",
    "        best_config_path = \"best_hyperparameters.json\"\n",
    "        try:\n",
    "            with open(best_config_path, 'w') as f: json.dump(overall_best_config, f, indent=4)\n",
    "            print(f\"Best hyperparameters saved to {best_config_path}\")\n",
    "        except Exception as e: print(f\"Error saving best config: {e}\")\n",
    "    else: print(\"No successful training run recorded.\")\n",
    "    print(\"=\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db456e86-f7e7-4c8a-9dd2-0f785f5c4b33",
   "metadata": {},
   "source": [
    "ì§€ì •ëœ ë°°ì¹˜ í¬ê¸°ì™€ í™œì„±í™” í•¨ìˆ˜ë“¤ì˜ ëª¨ë“  ì¡°í•©ì— ëŒ€í•´ ë£¨í”„ë¥¼ ë•ë‹ˆë‹¤. ê° ì¡°í•©ë§ˆë‹¤ simplified_training_loopë¥¼ ì‹¤í–‰í•˜ì—¬ ìµœì¢… ê²€ì¦ ì •í™•ë„ë¥¼ ì–»ê³ , ì´ ì •í™•ë„ê°€ ì§€ê¸ˆê¹Œì§€ ê¸°ë¡ëœ ìµœê³  ì •í™•ë„ë³´ë‹¤ ë†’ìœ¼ë©´ í•´ë‹¹ ì¡°í•©ì˜ ì„¤ì •ì„ overall_best_configì— ì €ì¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3f2cbc-90c7-4fed-a961-230562e2a4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ëª…ë ¹ì¤„ ì¸ì ì²˜ë¦¬ ë° ì‹¤í–‰ ---\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Find best hyperparameters (config only) for Titanic.\")\n",
    "    parser.add_argument(\"--wandb\", action=argparse.BooleanOptionalAction, default=False, help=\"Enable WandB logging\")\n",
    "    parser.add_argument(\"-e\", \"--epochs\", type=int, default=1000, help=\"Epochs per combination (default: 1000)\")\n",
    "\n",
    "    # Jupyter í™˜ê²½ ê°ì§€ ë° ì²˜ë¦¬\n",
    "    if 'ipykernel' in sys.modules:\n",
    "        print(\"Running in interactive mode. Using default args: epochs=1000, wandb=False\")\n",
    "        args = argparse.Namespace(wandb=False, epochs=1000) # ê¸°ë³¸ê°’ ì„¤ì •\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    find_hyperparameters(args) # ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca0c956-a814-452d-ae3f-a1ca7d93608e",
   "metadata": {},
   "source": [
    "# [ìš”êµ¬ì‚¬í•­ 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c8417e-85dd-4501-9fdc-5c81e21f23bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, sys, argparse, copy\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim # optimì€ ì¬í•™ìŠµ ì‹œ í•„ìš”\n",
    "from torch.utils.data import Dataset, DataLoader, random_split # random_splitì€ ì¬í•™ìŠµ ì‹œ í•„ìš”\n",
    "from sklearn.preprocessing import LabelEncoder # ì „ì²˜ë¦¬ í•¨ìˆ˜ì— í•„ìš”\n",
    "from pathlib import Path\n",
    "\n",
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, X, y): self.X = torch.FloatTensor(X); self.y = torch.LongTensor(y)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx): return {'input': self.X[idx], 'target': self.y[idx]}\n",
    "    def __str__(self): return f\"Data Size: {len(self.X)}, Input Shape: {self.X.shape}, Target Shape: {self.y.shape}\"\n",
    "\n",
    "class TitanicTestDataset(Dataset):\n",
    "    def __init__(self, X): self.X = torch.FloatTensor(X)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx): return {'input': self.X[idx]}\n",
    "    def __str__(self): return f\"Data Size: {len(self.X)}, Input Shape: {self.X.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363e62f7-413d-4a46-8158-c3717471bee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... ë°ì´í„° ì „ì²˜ë¦¬ ë³´ì¡° í•¨ìˆ˜ get_preprocessed_dataset_1 ~ _6 ì •ì˜\n",
    "def get_preprocessed_dataset_1(all_df): # ... (ë™ì¼) ...\n",
    "    Fare_mean = all_df[[\"Pclass\", \"Fare\"]].groupby(\"Pclass\").mean().reset_index(); Fare_mean.columns = [\"Pclass\", \"Fare_mean\"]\n",
    "    all_df = pd.merge(all_df, Fare_mean, on=\"Pclass\", how=\"left\"); all_df.loc[all_df[\"Fare\"].isnull(), \"Fare\"] = all_df[\"Fare_mean\"]\n",
    "    return all_df.drop(columns=[\"Fare_mean\"])\n",
    "# get_preprocessed_dataset_2 ~ _6 ì •ì˜\n",
    "def get_preprocessed_dataset_2(all_df):\n",
    "    name_df = all_df[\"Name\"].str.split(\"[,.]\", n=2, expand=True)\n",
    "    if name_df.shape[1] == 3:\n",
    "        name_df.columns = [\"family_name\", \"title\", \"name\"]; name_df[\"family_name\"] = name_df[\"family_name\"].str.strip(); name_df[\"title\"] = name_df[\"title\"].str.strip(); name_df[\"name\"] = name_df[\"name\"].str.strip()\n",
    "        all_df = pd.concat([all_df, name_df], axis=1)\n",
    "    else: all_df['title'] = 'unknown'\n",
    "    return all_df\n",
    "def get_preprocessed_dataset_3(all_df):\n",
    "    if 'title' not in all_df.columns:\n",
    "        if all_df['Age'].isnull().any(): all_df['Age'] = all_df['Age'].fillna(all_df['Age'].median())\n",
    "        return all_df\n",
    "    title_age_median = all_df[[\"title\", \"Age\"]].groupby(\"title\").median().round().reset_index(); title_age_median.columns = [\"title\", \"title_age_median\"]\n",
    "    all_df = pd.merge(all_df, title_age_median, on=\"title\", how=\"left\"); all_df.loc[all_df[\"Age\"].isnull(), \"Age\"] = all_df[\"title_age_median\"]\n",
    "    if \"title_age_median\" in all_df.columns: all_df = all_df.drop([\"title_age_median\"], axis=1)\n",
    "    return all_df\n",
    "def get_preprocessed_dataset_4(all_df):\n",
    "    all_df[\"family_num\"] = all_df[\"Parch\"] + all_df[\"SibSp\"]; all_df.loc[all_df[\"family_num\"] == 0, \"alone\"] = 1; all_df[\"alone\"] = all_df[\"alone\"].fillna(0).astype(float)\n",
    "    cols_to_drop = [\"PassengerId\", \"Name\", \"family_name\", \"name\", \"Ticket\", \"Cabin\"]; existing_cols_to_drop = [col for col in cols_to_drop if col in all_df.columns]\n",
    "    return all_df.drop(existing_cols_to_drop, axis=1)\n",
    "def get_preprocessed_dataset_5(all_df):\n",
    "    if 'title' in all_df.columns: all_df.loc[~((all_df[\"title\"] == \"Mr\") | (all_df[\"title\"] == \"Miss\") | (all_df[\"title\"] == \"Mrs\") | (all_df[\"title\"] == \"Master\")), \"title\"] = \"other\"\n",
    "    all_df[\"Embarked\"] = all_df[\"Embarked\"].fillna(\"missing\")\n",
    "    return all_df\n",
    "def get_preprocessed_dataset_6(all_df):\n",
    "    category_features = all_df.select_dtypes(include=['object']).columns\n",
    "    for cat_feat in category_features:\n",
    "        le = LabelEncoder(); valid_indices = all_df[cat_feat].notna()\n",
    "        if valid_indices.any():\n",
    "            all_df.loc[valid_indices, cat_feat] = le.fit_transform(all_df.loc[valid_indices, cat_feat])\n",
    "            try: all_df[cat_feat] = pd.to_numeric(all_df[cat_feat])\n",
    "            except ValueError: pass\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a73b2f-02d4-4696-9f51-517520e7981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_data_raw():\n",
    "    \"\"\"CSV ë¡œë“œ, ì „ì²˜ë¦¬, ìµœì¢… ë°ì´í„° ë°°ì—´ ë°˜í™˜ (í…ŒìŠ¤íŠ¸ ID í¬í•¨)\"\"\"\n",
    "    try: CURRENT_FILE_PATH = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError: CURRENT_FILE_PATH = os.getcwd()\n",
    "    train_data_path = os.path.join(CURRENT_FILE_PATH, \"train.csv\")\n",
    "    test_data_path = os.path.join(CURRENT_FILE_PATH, \"test.csv\")\n",
    "    try:\n",
    "        train_df = pd.read_csv(train_data_path)\n",
    "        test_df = pd.read_csv(test_data_path)\n",
    "    except FileNotFoundError as e: print(f\"Error: {e}\"); sys.exit(1)\n",
    "\n",
    "    test_passenger_ids = test_df['PassengerId'] # í…ŒìŠ¤íŠ¸ ìŠ¹ê° ID ì €ì¥\n",
    "    all_df = pd.concat([train_df, test_df], sort=False)\n",
    "\n",
    "    # ì „ì²˜ë¦¬ ë‹¨ê³„ ì ìš©\n",
    "    all_df = get_preprocessed_dataset_1(all_df); all_df = get_preprocessed_dataset_2(all_df)\n",
    "    all_df = get_preprocessed_dataset_3(all_df); all_df = get_preprocessed_dataset_4(all_df)\n",
    "    all_df = get_preprocessed_dataset_5(all_df); all_df = get_preprocessed_dataset_6(all_df)\n",
    "\n",
    "    # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„ë¦¬\n",
    "    train_X_df = all_df[~all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1)\n",
    "    train_y = train_df[\"Survived\"]\n",
    "    test_X_df = all_df[all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1)\n",
    "\n",
    "    # ìµœì¢… ë°ì´í„° í™•ì¸ ë° NaN ì²˜ë¦¬ (ë¶„ë¦¬ëœ ë°ì´í„°í”„ë ˆì„ì— ê°ê° ì ìš©)\n",
    "    print(\"\\n--- Final Data Check & Fill NaNs ---\")\n",
    "    columns_to_check = train_X_df.columns\n",
    "    for df in [train_X_df, test_X_df]:\n",
    "        df_name = \"Train\" if df is train_X_df else \"Test\"\n",
    "        for col in columns_to_check:\n",
    "            if df[col].dtype == 'object':\n",
    "                try: df[col] = pd.to_numeric(df[col])\n",
    "                except ValueError: df[col] = 0 # ë³€í™˜ ë¶ˆê°€ì‹œ 0ìœ¼ë¡œ ì±„ì›€\n",
    "            if df[col].isnull().any(): df[col] = df[col].fillna(0) # NaN 0ìœ¼ë¡œ ì±„ì›€\n",
    "\n",
    "    # ìµœì¢… ì¸ë±ìŠ¤ ë¦¬ì…‹ ë° Numpy ë°°ì—´ ë³€í™˜\n",
    "    train_X = train_X_df.reset_index(drop=True).values\n",
    "    test_X = test_X_df.reset_index(drop=True).values\n",
    "    train_y = train_y.values # Numpy ë°°ì—´ë¡œ ë³€í™˜\n",
    "\n",
    "    print(\"Data preprocessing finished for submission generation.\")\n",
    "    return train_X, train_y, test_X, test_passenger_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0a517c-3979-4a38-8208-ecd01771701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DataLoader ìƒì„± í•¨ìˆ˜ (get_dataloaders) ---\n",
    "def get_dataloaders(train_X, train_y, test_X, batch_size_config):\n",
    "    \"\"\"í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ DataLoader ëª¨ë‘ ìƒì„±\"\"\"\n",
    "    dataset = TitanicDataset(train_X, train_y)\n",
    "    generator = torch.Generator().manual_seed(42) # ì‹œë“œ ê³ ì •\n",
    "    train_dataset, validation_dataset = random_split(dataset, [0.8, 0.2], generator=generator)\n",
    "    test_dataset = TitanicTestDataset(test_X)\n",
    "\n",
    "    print(f\"\\nTrain size: {len(train_dataset)}, Validation size: {len(validation_dataset)}, Test size: {len(test_dataset)}\")\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size_config, shuffle=True)\n",
    "    validation_loader = DataLoader(dataset=validation_dataset, batch_size=len(validation_dataset))\n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë” ìƒì„± (ë¹„ì–´ìˆì§€ ì•Šì„ ë•Œë§Œ)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset)) if len(test_dataset) > 0 else None\n",
    "\n",
    "    return train_loader, validation_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e98f35b-459d-429c-8c94-e9811465ab99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ì‹ ê²½ë§ ëª¨ë¸ ì •ì˜ (MyModel) ---\n",
    "class MyModel(nn.Module):\n",
    "    \"\"\"í™œì„±í™” í•¨ìˆ˜ì™€ ì€ë‹‰ì¸µ í¬ê¸°ë¥¼ ì¸ìë¡œ ë°›ëŠ” MLP ëª¨ë¸\"\"\"\n",
    "    def __init__(self, n_input, n_output, activation_fn_class, n_hidden1=20, n_hidden2=20):\n",
    "        super().__init__(); activation = activation_fn_class()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_input, n_hidden1), activation,\n",
    "            nn.Linear(n_hidden1, n_hidden2), activation,\n",
    "            nn.Linear(n_hidden2, n_output)\n",
    "        )\n",
    "    def forward(self, x): return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af291b7c-94cf-4f16-bde0-ad33f7af4fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ëª¨ë¸ ë° ì˜µí‹°ë§ˆì´ì € ìƒì„± í•¨ìˆ˜ (get_model_and_optimizer) ---\n",
    "def get_model_and_optimizer(activation_fn_class, learning_rate, n_hidden1=20, n_hidden2=20):\n",
    "    \"\"\"ëª¨ë¸ ê°ì²´ì™€ SGD ì˜µí‹°ë§ˆì´ì € ìƒì„±\"\"\"\n",
    "    model = MyModel(10, 2, activation_fn_class, n_hidden1, n_hidden2) # ì…ë ¥ 10ê°œ, ì¶œë ¥ 2ê°œ ê³ ì •\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ae0167-8ade-44c5-ba7b-16519d5882bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ëª¨ë¸ í•™ìŠµ ë° ê²€ì¦ ë£¨í”„ ---\n",
    "def training_loop_with_early_stopping(model, optimizer, train_data_loader, validation_data_loader, n_epochs, loss_fn):\n",
    "    \"\"\"ëª¨ë¸ í•™ìŠµ/ê²€ì¦ ìˆ˜í–‰ ë° 'ìµœê³  ì„±ëŠ¥' ëª¨ë¸ ìƒíƒœ ë°˜í™˜ (ì¡°ê¸° ì¢…ë£Œ ë¡œì§)\"\"\"\n",
    "    best_validation_accuracy = -1.0\n",
    "    best_model_state = None\n",
    "    best_epoch = 0\n",
    "\n",
    "    print(f\"\\nStart Final Training for {n_epochs} epochs (with early stopping logic)...\")\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # --- í•™ìŠµ ---\n",
    "        model.train(); loss_train_epoch = 0.0; correct_train_epoch = 0; total_train_samples = 0\n",
    "        for batch in train_data_loader:\n",
    "            input_data = batch['input']; target = batch['target']; optimizer.zero_grad(); output_train = model(input_data); loss = loss_fn(output_train, target); loss.backward(); optimizer.step()\n",
    "            loss_train_epoch += loss.item() * input_data.size(0); _, predicted = torch.max(output_train.data, 1); total_train_samples += target.size(0); correct_train_epoch += (predicted == target).sum().item()\n",
    "        avg_loss_train = loss_train_epoch / total_train_samples if total_train_samples else 0\n",
    "        train_accuracy = 100 * correct_train_epoch / total_train_samples if total_train_samples else 0\n",
    "\n",
    "        # --- ê²€ì¦ ---\n",
    "        model.eval(); loss_val_epoch = 0.0; correct_val_epoch = 0; total_val_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in validation_data_loader:\n",
    "                input_data = batch['input']; target = batch['target']; output_validation = model(input_data); loss = loss_fn(output_validation, target); loss_val_epoch += loss.item() * input_data.size(0)\n",
    "                _, predicted = torch.max(output_validation.data, 1); total_val_samples += target.size(0); correct_val_epoch += (predicted == target).sum().item()\n",
    "        avg_loss_validation = loss_val_epoch / total_val_samples if total_val_samples else 0\n",
    "        validation_accuracy = 100 * correct_val_epoch / total_val_samples if total_val_samples else 0\n",
    "\n",
    "        # ìµœê³  ì„±ëŠ¥ ê°±ì‹  í™•ì¸,ì¡°ê¸° ì¢…ë£Œ ë¡œì§\n",
    "        if validation_accuracy > best_validation_accuracy:\n",
    "            best_validation_accuracy = validation_accuracy\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = epoch\n",
    "            print(f\" New best validation accuracy: {best_validation_accuracy:.2f}% at epoch {epoch}\")\n",
    "\n",
    "        # ì£¼ê¸°ì  ì¶œë ¥\n",
    "        if epoch % 100 == 0 or epoch == 1:\n",
    "            print(f\"Epoch {epoch:>{len(str(n_epochs))}}/{n_epochs}, Train Loss: {avg_loss_train:.4f}, Acc: {train_accuracy:.2f}% | Val Loss: {avg_loss_validation:.4f}, Acc: {validation_accuracy:.2f}%{' *Best*' if epoch == best_epoch else ''}\")\n",
    "\n",
    "    print(f\"\\n--- Final Training Summary --- Best Val Acc: {best_validation_accuracy:.2f}% achieved at epoch {best_epoch}\")\n",
    "    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ìƒíƒœì™€ ì—í¬í¬ ë°˜í™˜\n",
    "    return best_validation_accuracy, best_model_state, best_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1110e837-5d69-443a-bff3-02b10c7aee6b",
   "metadata": {},
   "source": [
    "ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ì„ ì¬í•™ìŠµì‹œí‚¤ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. í•™ìŠµ ì¤‘ ë§¤ ì—í¬í¬ë§ˆë‹¤ ê²€ì¦ ë°ì´í„° ì„±ëŠ¥ì„ í™•ì¸í•˜ê³ , ìµœê³  ì„±ëŠ¥ì„ ë³´ì¸ ì‹œì ì˜ ëª¨ë¸ ìƒíƒœì™€ ì—í¬í¬ ë²ˆí˜¸ë¥¼ ê¸°ë¡í•˜ì—¬ ìµœì¢…ì ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfaedad-038d-4fc9-ba30-2330f43071a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ (test_model) ---\n",
    "# (find_best_hyperparameters.py ì™€ ë™ì¼í•˜ê²Œ ì •ì˜)\n",
    "def test_model(model, test_data_loader):\n",
    "    \"\"\"í•™ìŠµëœ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ìˆ˜í–‰\"\"\"\n",
    "    model.eval(); predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_data_loader: input_data = batch['input']; output = model(input_data); _, predicted = torch.max(output.data, 1); predictions.extend(predicted.cpu().numpy())\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b07bae1-78c8-411d-8943-36a641af838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Submission íŒŒì¼ ìƒì„± í•¨ìˆ˜ (create_submission) ---\n",
    "# (find_best_hyperparameters.py ì™€ ë™ì¼í•˜ê²Œ ì •ì˜)\n",
    "def create_submission(predictions, passenger_ids, output_file=\"submission.csv\"):\n",
    "    \"\"\"ì˜ˆì¸¡ ê²°ê³¼ì™€ ìŠ¹ê° IDë¡œ submission CSV íŒŒì¼ ìƒì„±\"\"\"\n",
    "    if len(predictions) != len(passenger_ids): print(f\"Error: Prediction count ({len(predictions)}) != Passenger ID count ({len(passenger_ids)})\"); return\n",
    "    submission_df = pd.DataFrame({\"PassengerId\": passenger_ids, \"Survived\": predictions})\n",
    "    try: submission_df.to_csv(output_file, index=False); print(f\"Submission file saved: {output_file}\")\n",
    "    except Exception as e: print(f\"Error saving submission file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc7e5e6-c574-4536-84d3-9adde74e523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ìµœì  ëª¨ë¸ ë¡œë“œ ë° ì˜ˆì¸¡) ---\n",
    "def generate_submission_main(config_path=\"best_hyperparameters.json\", retrain_epochs=None):\n",
    "    \"\"\"ìµœì  ì„¤ì • ë¡œë“œ, í•´ë‹¹ ì„¤ì •ìœ¼ë¡œ ì¬í•™ìŠµ(ì¡°ê¸°ì¢…ë£Œ ì ìš©), ì˜ˆì¸¡, submission íŒŒì¼ ìƒì„±\"\"\"\n",
    "    # ìµœì  ì„¤ì • ë¡œë“œ\n",
    "    try:\n",
    "        with open(config_path, 'r') as f: best_config = json.load(f)\n",
    "    except FileNotFoundError: print(f\"Error: Config file '{config_path}' not found.\"); sys.exit(1)\n",
    "    except json.JSONDecodeError: print(f\"Error: Could not decode JSON from '{config_path}'.\"); sys.exit(1)\n",
    "\n",
    "    print(\"--- Loaded Best Config ---\"); print(best_config)\n",
    "\n",
    "    # ì„¤ì •ê°’ ì¶”ì¶œ\n",
    "    best_activation_name = best_config.get('activation_function')\n",
    "    best_batch_size = best_config.get('batch_size')\n",
    "    learning_rate = best_config.get('learning_rate', 1e-3)\n",
    "    n_hidden1 = best_config.get('n_hidden_unit_list', [20, 20])[0]\n",
    "    n_hidden2 = best_config.get('n_hidden_unit_list', [20, 20])[1]\n",
    "    # ì¬í•™ìŠµ ì—í¬í¬ ìˆ˜ ê²°ì • (ì¸ì ìš°ì„  > config ê°’ > ê¸°ë³¸ê°’ 1000)\n",
    "    epochs_to_train = retrain_epochs if retrain_epochs is not None else best_config.get('epochs', 1000)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # í™œì„±í™” í•¨ìˆ˜ í´ë˜ìŠ¤ ê°€ì ¸ì˜¤ê¸°\n",
    "    activation_functions_map = {\"Sigmoid\": nn.Sigmoid, \"ReLU\": nn.ReLU, \"ELU\": nn.ELU, \"LeakyReLU\": nn.LeakyReLU}\n",
    "    if not best_activation_name or best_activation_name not in activation_functions_map:\n",
    "        print(f\"Error: Invalid activation function '{best_activation_name}' in config.\"); sys.exit(1)\n",
    "    best_activation_class = activation_functions_map[best_activation_name]\n",
    "\n",
    "    print(\"\\nPreprocessing data for final training and submission...\")\n",
    "    train_X, train_y, test_X, test_passenger_ids = get_preprocessed_data_raw()\n",
    "\n",
    "    if not best_batch_size: print(\"Error: 'batch_size' not found in config.\"); sys.exit(1)\n",
    "    train_loader, validation_loader, test_loader = get_dataloaders(train_X, train_y, test_X, best_batch_size)\n",
    "    if test_loader is None: print(\"Error: Test data is empty, cannot generate submission.\"); sys.exit(1)\n",
    "\n",
    "    model, optimizer = get_model_and_optimizer(best_activation_class, learning_rate, n_hidden1, n_hidden2)\n",
    "\n",
    "    # === ìµœì¢… ëª¨ë¸ ì¬í•™ìŠµ (ì¡°ê¸° ì¢…ë£Œ ë¡œì§ ì ìš©) ===\n",
    "    final_best_accuracy, final_best_model_state, final_best_epoch = training_loop_with_early_stopping(\n",
    "        model, optimizer, train_loader, validation_loader, epochs_to_train, loss_fn\n",
    "    )\n",
    "\n",
    "    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ìƒíƒœ ë¡œë“œ\n",
    "    if final_best_model_state:\n",
    "        model.load_state_dict(final_best_model_state)\n",
    "        print(f\"\\nLoaded model state from epoch {final_best_epoch} with validation accuracy {final_best_accuracy:.2f}%\")\n",
    "\n",
    "        # ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "        print(\"Generating predictions with the best epoch model...\")\n",
    "        test_predictions = test_model(model, test_loader)\n",
    "\n",
    "        # Submission íŒŒì¼ ìƒì„±\n",
    "        submission_filename = f\"submission_final_{best_activation_name}_bs{best_batch_size}_epoch{final_best_epoch}.csv\"\n",
    "        create_submission(test_predictions, test_passenger_ids, output_file=submission_filename)\n",
    "    else:\n",
    "        print(\"Final training did not produce a best model state. Cannot generate submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8c3355-f59c-4228-913d-b04eb6b9d761",
   "metadata": {},
   "source": [
    "* best_hyperparameters.json íŒŒì¼ì„ ì½ì–´ ìµœì  ì¡°í•© ì„¤ì •ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "\n",
    "* ì „ì²´ ë°ì´í„°ë¥¼ ë‹¤ì‹œ ë¡œë“œí•˜ê³  ìµœì  ë°°ì¹˜ í¬ê¸°ë¡œ DataLoaderë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "* ìµœì  í™œì„±í™” í•¨ìˆ˜ë¡œ ëª¨ë¸ê³¼ ì˜µí‹°ë§ˆì´ì €ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "* training_loop_with_early_stopping í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ëª¨ë¸ì„ ì¬í•™ìŠµì‹œí‚¤ê³ , ì´ ê³¼ì •ì—ì„œ ê²€ì¦ ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ì•˜ë˜ ì‹œì (ì¡°ê¸° ì¢…ë£Œ ì‹œì )ì˜ ëª¨ë¸ ìƒíƒœì™€ ì—í¬í¬ ë²ˆí˜¸ë¥¼ ì–»ìŠµë‹ˆë‹¤.\n",
    "\n",
    "* ì–»ì–´ì§„ ìµœì  ì‹œì ì˜ ëª¨ë¸ ìƒíƒœë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "\n",
    "* test_model í•¨ìˆ˜ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "* create_submission í•¨ìˆ˜ë¡œ ìµœì¢… ì œì¶œ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤ (íŒŒì¼ ì´ë¦„ì— ìµœì  ì¡°í•© ë° ì—í¬í¬ í¬í•¨)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcff00d-75b7-4059-821b-12b1e508c14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ëª…ë ¹ì¤„ ì¸ì ì²˜ë¦¬ ë° ì‹¤í–‰ ---\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Retrain the best model with early stopping and generate submission.\")\n",
    "    parser.add_argument(\"--config\", default=\"best_hyperparameters.json\", help=\"Path to best hyperparameters JSON\")\n",
    "    # ì¬í•™ìŠµ ì—í¬í¬ ìˆ˜ë¥¼ ì¸ìë¡œ ë°›ì„ ìˆ˜ ìˆë„ë¡ ì¶”ê°€ (ì„ íƒì‚¬í•­)\n",
    "    parser.add_argument(\"-e\", \"--retrain_epochs\", type=int, default=None, help=\"Number of epochs for retraining (uses config value if not set)\")\n",
    "\n",
    "    if 'ipykernel' in sys.modules:\n",
    "        print(\"Running in interactive mode. Using default config path and epochs from config (or 1000).\")\n",
    "        args = parser.parse_args([]) # ê¸°ë³¸ê°’ ì‚¬ìš©\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    generate_submission_main(config_path=args.config, retrain_epochs=args.retrain_epochs) # ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33182661-0876-4ba8-aeb9-e88dd078e835",
   "metadata": {},
   "source": [
    "![ìºê¸€ ì œì¶œ ê²°](dl_titanic_leaderboard.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0434c03b-5203-4759-a11b-d3535aac822e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
