{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29fcf32b-7b92-4cd3-8eec-b3d9e65b013d",
   "metadata": {},
   "source": [
    "# [ìš”êµ¬ì‚¬í•­ 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74e4879d-b21c-47e2-a824-38e9b5cd5b9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T14:19:47.103074Z",
     "start_time": "2025-10-18T14:19:39.991546Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "import argparse\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "class TitanicDataset(Dataset):\n",
    "  def __init__(self, X, y):\n",
    "    self.X = torch.FloatTensor(X)\n",
    "    self.y = torch.LongTensor(y)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    feature = self.X[idx]\n",
    "    target = self.y[idx]\n",
    "    return {'input': feature, 'target': target}\n",
    "\n",
    "  def __str__(self):\n",
    "    str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "      len(self.X), self.X.shape, self.y.shape\n",
    "    )\n",
    "    return str\n",
    "\n",
    "\n",
    "class TitanicTestDataset(Dataset):\n",
    "  def __init__(self, X):\n",
    "    self.X = torch.FloatTensor(X)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    feature = self.X[idx]\n",
    "    return {'input': feature}\n",
    "\n",
    "  def __str__(self):\n",
    "    str = \"Data Size: {0}, Input Shape: {1}\".format(\n",
    "      len(self.X), self.X.shape\n",
    "    )\n",
    "    return str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c95fbb-7f7d-4e0b-bdca-4fcba5346df1",
   "metadata": {},
   "source": [
    "__init__: ì…ë ¥ ë°ì´í„°(íŠ¹ì§• X, íƒ€ê²Ÿ y)ë¥¼ íŒŒì´í† ì¹˜ê°€ ì‚¬ìš©í•˜ëŠ” ë°ì´í„° í˜•ì‹ì¸ í…ì„œ(Tensor)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì€ yë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ ëª©í‘œì´ë¯€ë¡œ Xë§Œ ê°€ì§‘ë‹ˆë‹¤.\n",
    "\n",
    "__len__: ë°ì´í„°ì…‹ì— ìˆëŠ” ì´ ìƒ˜í”Œ ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "__getitem__: ì¸ë±ìŠ¤(ì˜ˆ: dataset[10])ë¥¼ ì‚¬ìš©í•´ í•˜ë‚˜ì˜ ë°ì´í„° ìƒ˜í”Œ(í•™ìŠµìš©ì€ íŠ¹ì§•ê³¼ íƒ€ê²Ÿ, í…ŒìŠ¤íŠ¸ìš©ì€ íŠ¹ì§•ë§Œ)ì„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.\n",
    "\n",
    "__str__: print() í•¨ìˆ˜ë¡œ ì¶œë ¥ë  ë•Œ ë°ì´í„°ì…‹ í¬ê¸°ì™€ í˜•íƒœì— ëŒ€í•œ ê°„ë‹¨í•œ ë¬¸ìì—´ ì„¤ëª…ì„ ì œê³µí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65a86bed-144d-4055-a024-fe64d3067e4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T14:19:47.119696Z",
     "start_time": "2025-10-18T14:19:47.112382Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset():\n",
    "    # ìŠ¤í¬ë¦½íŠ¸ê°€ ì‹¤í–‰ë˜ëŠ” í˜„ì¬ íŒŒì¼ ê²½ë¡œë¥¼ ê¸°ì¤€ìœ¼ë¡œ CSV íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "    try:\n",
    "        CURRENT_FILE_PATH = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError:\n",
    "        # __file__ì´ ì •ì˜ë˜ì§€ ì•Šì€ í™˜ê²½(ì˜ˆ: Jupyter)ì—ì„œëŠ” í˜„ì¬ ì‘ì—… ë””ë ‰í„°ë¦¬ë¥¼ ì‚¬ìš©\n",
    "        CURRENT_FILE_PATH = os.getcwd()\n",
    "\n",
    "    train_data_path = os.path.join(CURRENT_FILE_PATH, \"train.csv\")\n",
    "    test_data_path = os.path.join(CURRENT_FILE_PATH, \"test.csv\")\n",
    "\n",
    "    train_df = pd.read_csv(train_data_path)\n",
    "    test_df = pd.read_csv(test_data_path)\n",
    "\n",
    "    all_df = pd.concat([train_df, test_df], sort=False)\n",
    "\n",
    "    all_df = get_preprocessed_dataset_1(all_df)\n",
    "    all_df = get_preprocessed_dataset_2(all_df)\n",
    "    all_df = get_preprocessed_dataset_3(all_df)\n",
    "    all_df = get_preprocessed_dataset_4(all_df)\n",
    "    all_df = get_preprocessed_dataset_5(all_df)\n",
    "    all_df = get_preprocessed_dataset_6(all_df)\n",
    "\n",
    "    print(\"--- Preprocessed DataFrame Columns ---\")\n",
    "    print(all_df.columns)\n",
    "    print(\"--- Preprocessed DataFrame Head ---\")\n",
    "    print(all_df.head(5))\n",
    "\n",
    "    train_X = all_df[~all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1).reset_index(drop=True)\n",
    "    train_y = train_df[\"Survived\"]\n",
    "\n",
    "    test_X = all_df[all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1).reset_index(drop=True)\n",
    "\n",
    "    print(f\"\\nInput Features ({len(train_X.columns)}): {train_X.columns.tolist()}\")\n",
    "\n",
    "    dataset = TitanicDataset(train_X.values, train_y.values)\n",
    "    print(\"--- Full Train Dataset ---\")\n",
    "    print(dataset)\n",
    "\n",
    "    train_dataset, validation_dataset = random_split(dataset, [0.8, 0.2])\n",
    "    test_dataset = TitanicTestDataset(test_X.values)\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2658c92-0b0c-4da8-94c5-91f54cec0855",
   "metadata": {},
   "source": [
    "CSV ì°¾ê¸° & ë¡œë”©: íŒë‹¤ìŠ¤ë¥¼ ì´ìš©í•´ train.csvì™€ test.csv íŒŒì¼ì„ ì°¾ì•„ ì½ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ê²°í•©: í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ì „ì²˜ë¦¬ ë‹¨ê³„(ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°, ë²”ì£¼ ì¸ì½”ë”© ë“±)ë¥¼ ì–‘ìª½ ë°ì´í„°ì— ì¼ê´€ë˜ê²Œ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì „ì²˜ë¦¬: ë³´ì¡° í•¨ìˆ˜ë“¤(_1ë¶€í„° _6ê¹Œì§€)ì„ ìˆœì„œëŒ€ë¡œ í˜¸ì¶œí•˜ì—¬ ë°ì´í„°ë¥¼ ì •ì œí•˜ê³  ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë¶„ë¦¬: ê²°í•©í–ˆë˜ ë°ì´í„°ë¥¼ ë‹¤ì‹œ í•™ìŠµìš© íŠ¹ì§•(train_X), í•™ìŠµìš© ë ˆì´ë¸”(train_y), í…ŒìŠ¤íŠ¸ìš© íŠ¹ì§•(test_X)ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.\n",
    "\n",
    "Dataset ìƒì„±: ì‚¬ìš©ì ì •ì˜ í´ë˜ìŠ¤(TitanicDataset, TitanicTestDataset)ë¥¼ ì‚¬ìš©í•´ ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ê°ìŒ‰ë‹ˆë‹¤.\n",
    "\n",
    "í•™ìŠµ/ê²€ì¦ ë¶„í• : í•™ìŠµ ë°ì´í„°ë¥¼ ëª¨ë¸ í›ˆë ¨ì— ì‚¬ìš©í•  ë” í° ì„¸íŠ¸ì™€ í›ˆë ¨ ì¤‘ ì„±ëŠ¥ ê²€ì¦ì— ì‚¬ìš©í•  ë” ì‘ì€ ì„¸íŠ¸ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤ (80/20 ë¹„ìœ¨).\n",
    "\n",
    "ë°˜í™˜: íŒŒì´í† ì¹˜ì˜ DataLoaderì—ì„œ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ìµœì¢… Dataset ê°ì²´ë“¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53fd7314-c2b5-4d11-a6c6-d11de2357e01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T14:19:47.135584Z",
     "start_time": "2025-10-18T14:19:47.125535Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset_1(all_df):\n",
    "    # Pclassë³„ Fare (ìš”ê¸ˆ) í‰ê· ê°’ì„ ì‚¬ìš©í•˜ì—¬ Fare ê²°ì¸¡ì¹˜ ë©”ìš°ê¸°\n",
    "    Fare_mean = all_df[[\"Pclass\", \"Fare\"]].groupby(\"Pclass\").mean().reset_index()\n",
    "    Fare_mean.columns = [\"Pclass\", \"Fare_mean\"]\n",
    "    all_df = pd.merge(all_df, Fare_mean, on=\"Pclass\", how=\"left\")\n",
    "    all_df.loc[(all_df[\"Fare\"].isnull()), \"Fare\"] = all_df[\"Fare_mean\"]\n",
    "    all_df = all_df.drop(columns=[\"Fare_mean\"])\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_2(all_df):\n",
    "    # nameì„ ì„¸ ê°œì˜ ì»¬ëŸ¼ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ë‹¤ì‹œ all_dfì— í•©ì¹¨\n",
    "    name_df = all_df[\"Name\"].str.split(\"[,.]\", n=2, expand=True)\n",
    "    name_df.columns = [\"family_name\", \"title\", \"name\"]\n",
    "    name_df[\"family_name\"] = name_df[\"family_name\"].str.strip()\n",
    "    name_df[\"title\"] = name_df[\"title\"].str.strip()\n",
    "    name_df[\"name\"] = name_df[\"name\"].str.strip()\n",
    "    all_df = pd.concat([all_df, name_df], axis=1)\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_3(all_df):\n",
    "    # titleë³„ Age í‰ê· ê°’ì„ ì‚¬ìš©í•˜ì—¬ Age ê²°ì¸¡ì¹˜ ë©”ìš°ê¸°\n",
    "    title_age_mean = all_df[[\"title\", \"Age\"]].groupby(\"title\").median().round().reset_index()\n",
    "    title_age_mean.columns = [\"title\", \"title_age_mean\", ]\n",
    "    all_df = pd.merge(all_df, title_age_mean, on=\"title\", how=\"left\")\n",
    "    all_df.loc[(all_df[\"Age\"].isnull()), \"Age\"] = all_df[\"title_age_mean\"]\n",
    "    all_df = all_df.drop([\"title_age_mean\"], axis=1)\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_4(all_df):\n",
    "    # ê°€ì¡±ìˆ˜(family_num) ì»¬ëŸ¼ ìƒˆë¡­ê²Œ ì¶”ê°€\n",
    "    all_df[\"family_num\"] = all_df[\"Parch\"] + all_df[\"SibSp\"]\n",
    "    # í˜¼ìíƒ‘ìŠ¹(alone) ì»¬ëŸ¼ ìƒˆë¡­ê²Œ ì¶”ê°€\n",
    "    all_df.loc[all_df[\"family_num\"] == 0, \"alone\"] = 1\n",
    "    all_df[\"alone\"].fillna(0, inplace=True)\n",
    "    # í•™ìŠµì— ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°\n",
    "    all_df = all_df.drop([\"PassengerId\", \"Name\", \"family_name\", \"name\", \"Ticket\", \"Cabin\"], axis=1)\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_5(all_df):\n",
    "    # title ê°’ ê°œìˆ˜ ì¤„ì´ê¸°\n",
    "    all_df.loc[\n",
    "    ~(\n",
    "            (all_df[\"title\"] == \"Mr\") |\n",
    "            (all_df[\"title\"] == \"Miss\") |\n",
    "            (all_df[\"title\"] == \"Mrs\") |\n",
    "            (all_df[\"title\"] == \"Master\")\n",
    "    ),\n",
    "    \"title\"\n",
    "    ] = \"other\"\n",
    "    all_df[\"Embarked\"].fillna(\"missing\", inplace=True)\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_6(all_df):\n",
    "    # ì¹´í…Œê³ ë¦¬ ë³€ìˆ˜ë¥¼ LabelEncoderë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ˜ì¹˜ê°’ìœ¼ë¡œ ë³€ê²½í•˜ê¸°\n",
    "    category_features = all_df.columns[all_df.dtypes == \"object\"]\n",
    "    for category_feature in category_features:\n",
    "        le = LabelEncoder()\n",
    "        if all_df[category_feature].dtypes == \"object\":\n",
    "          le = le.fit(all_df[category_feature])\n",
    "          all_df[category_feature] = le.transform(all_df[category_feature])\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fba863-8cde-4aec-a139-d0edd92f0f2f",
   "metadata": {},
   "source": [
    "_1: ëˆ„ë½ëœ ìš”ê¸ˆ ê°’ì„ ê° ìŠ¹ê° ë“±ê¸‰ì˜ í‰ê·  ìš”ê¸ˆìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤. \n",
    "\n",
    "_2: Name ì—´ì—ì„œ í˜¸ì¹­ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. \n",
    "\n",
    "_3: ëˆ„ë½ëœ ë‚˜ì´ê°’ì„ ì¶”ì¶œëœ titleê³¼ ì—°ê´€ëœ ë‚˜ì´ì˜ ì¤‘ì•™ê°’ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤. \n",
    "\n",
    "_4: ìƒˆë¡œìš´ íŠ¹ì§•ì¸ family_numê³¼ aloneì„ ë§Œë“­ë‹ˆë‹¤. ëª¨ë¸ë§ì— ë¶ˆí•„ìš”í•˜ë‹¤ê³  íŒë‹¨ë˜ëŠ” ì—´ì„ ì œê±°í•©ë‹ˆë‹¤.\n",
    "\n",
    "_5: ë“œë¬¸ í˜¸ì¹­ë“¤ì„ 'other'ë¡œ ê·¸ë£¹í™”í•˜ì—¬ title ì—´ì„ ë‹¨ìˆœí™”í•©ë‹ˆë‹¤. ëˆ„ë½ëœ íƒ‘ìŠ¹ í•­êµ¬ ê°’ì„ 'missing'ì´ë¼ëŠ” ì„ì‹œ ê°’ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤.\n",
    "\n",
    "_6: ë²”ì£¼í˜• ë¬¸ìì—´ ì—´ì„ Label Encodingì„ ì‚¬ìš©í•´ ìˆ«ì í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T14:19:47.150183Z",
     "start_time": "2025-10-18T14:19:47.143647Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "  # 1ë²ˆ ë¸”ë¡ì˜ ì „ì²˜ë¦¬ í•¨ìˆ˜ í˜¸ì¶œ (ì´ í•¨ìˆ˜ëŠ” ë‹¤ë¥¸ íŒŒì¼ì— ì •ì˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •)\n",
    "  train_dataset, validation_dataset, test_dataset = get_preprocessed_dataset()\n",
    "\n",
    "  print(f\"\\nTrain dataset size: {len(train_dataset)}\")\n",
    "  print(f\"Validation dataset size: {len(validation_dataset)}\")\n",
    "  print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "  # wandb.configì—ì„œ ë°°ì¹˜ í¬ê¸°ë¥¼ ê°€ì ¸ì™€ DataLoader ìƒì„±\n",
    "  train_data_loader = DataLoader(dataset=train_dataset, batch_size=wandb.config.batch_size, shuffle=True)\n",
    "  validation_data_loader = DataLoader(dataset=validation_dataset, batch_size=len(validation_dataset))\n",
    "  test_data_loader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset))\n",
    "\n",
    "  return train_data_loader, validation_data_loader, test_data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a17a40-a384-43be-8bbb-1e24bfc112d3",
   "metadata": {},
   "source": [
    "ë°ì´í„°ë“¤ì„ DataLoaderë¡œ ê°ì‹¸ì„œ ëª¨ë¸ í•™ìŠµ ì‹œ ë°ì´í„°ë¥¼ ë¯¸ë‹ˆë°°ì¹˜ ë‹¨ìœ„ë¡œ íš¨ìœ¨ì ìœ¼ë¡œ ê³µê¸‰í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d17d1c1c-9b50-4202-9183-476e204f2890",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T14:19:47.181103Z",
     "start_time": "2025-10-18T14:19:47.175588Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "  def __init__(self, n_input, n_output):\n",
    "    super().__init__()\n",
    "\n",
    "    self.model = nn.Sequential(\n",
    "      nn.Linear(n_input, wandb.config.n_hidden_unit_list[0]),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(wandb.config.n_hidden_unit_list[0], wandb.config.n_hidden_unit_list[1]),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(wandb.config.n_hidden_unit_list[1], n_output),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.model(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_model_and_optimizer():\n",
    "  # ì…ë ¥ í”¼ì²˜ 10ê°œ (Pclass, Sex, Age, SibSp, Parch, Fare, Embarked, title, family_num, alone)\n",
    "  # ì¶œë ¥ í´ë˜ìŠ¤ 2ê°œ (0: ì‚¬ë§, 1: ìƒì¡´)\n",
    "  my_model = MyModel(n_input=10, n_output=2)\n",
    "  optimizer = optim.SGD(my_model.parameters(), lr=wandb.config.learning_rate)\n",
    "\n",
    "  return my_model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157f3e50-c463-43a2-a0ac-af757451220e",
   "metadata": {},
   "source": [
    "__init__: ëª¨ë¸ì„ êµ¬ì„±í•˜ëŠ” ì¸µë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ì…ë ¥ì¸µ, 2ê°œì˜ ì€ë‹‰ì¸µ, ì¶œë ¥ì¸µìœ¼ë¡œ êµ¬ì„±ëœ ê°„ë‹¨í•œ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ ì…ë‹ˆë‹¤. ê° ì¸µì˜ ë‰´ëŸ° ìˆ˜ëŠ” wandb.configì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "\n",
    "forward: ì…ë ¥ ë°ì´í„°(x)ê°€ ëª¨ë¸ì˜ ì¸µë“¤ì„ ì–´ë–¤ ìˆœì„œë¡œ í†µê³¼í•˜ì—¬ ìµœì¢… ì¶œë ¥ì„ ë§Œë“¤ì–´ë‚´ëŠ”ì§€ ì •ì˜í•©ë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ca94715-daae-4c95-b640-0664499ea533",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T14:19:47.201571Z",
     "start_time": "2025-10-18T14:19:47.192071Z"
    }
   },
   "outputs": [],
   "source": [
    "def training_loop(model, optimizer, train_data_loader, validation_data_loader):\n",
    "  n_epochs = wandb.config.epochs\n",
    "  loss_fn = nn.CrossEntropyLoss()  # ë¶„ë¥˜ ë¬¸ì œì´ë¯€ë¡œ CrossEntropyLoss ì‚¬ìš©\n",
    "  next_print_epoch = 100\n",
    "\n",
    "  for epoch in range(1, n_epochs + 1):\n",
    "    loss_train = 0.0\n",
    "    num_trains = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    model.train() # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì •\n",
    "    for batch in train_data_loader:\n",
    "      # Datasetì´ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì´ë¯€ë¡œ í‚¤ë¡œ ì ‘ê·¼\n",
    "      input = batch['input']\n",
    "      target = batch['target']\n",
    "\n",
    "      output_train = model(input)\n",
    "      loss = loss_fn(output_train, target)\n",
    "      loss_train += loss.item()\n",
    "      num_trains += 1\n",
    "\n",
    "      # ì •í™•ë„ ê³„ì‚°\n",
    "      _, predicted = torch.max(output_train.data, 1)\n",
    "      total_train += target.size(0)\n",
    "      correct_train += (predicted == target).sum().item()\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    loss_validation = 0.0\n",
    "    num_validations = 0\n",
    "    correct_validation = 0\n",
    "    total_validation = 0\n",
    "\n",
    "    model.eval() # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •\n",
    "    with torch.no_grad():\n",
    "      for batch in validation_data_loader:\n",
    "        input = batch['input']\n",
    "        target = batch['target']\n",
    "\n",
    "        output_validation = model(input)\n",
    "        loss = loss_fn(output_validation, target)\n",
    "        loss_validation += loss.item()\n",
    "        num_validations += 1\n",
    "\n",
    "        # ì •í™•ë„ ê³„ì‚°\n",
    "        _, predicted = torch.max(output_validation.data, 1)\n",
    "        total_validation += target.size(0)\n",
    "        correct_validation += (predicted == target).sum().item()\n",
    "\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    validation_accuracy = 100 * correct_validation / total_validation\n",
    "\n",
    "    wandb.log({\n",
    "      \"Epoch\": epoch,\n",
    "      \"Training loss\": loss_train / num_trains,\n",
    "      \"Validation loss\": loss_validation / num_validations,\n",
    "      \"Training accuracy\": train_accuracy,\n",
    "      \"Validation accuracy\": validation_accuracy\n",
    "    })\n",
    "\n",
    "    if epoch % next_print_epoch == 0 or epoch == 1:\n",
    "      print(\n",
    "        f\"Epoch {epoch}, \"\n",
    "        f\"Training loss {loss_train / num_trains:.4f}, \"\n",
    "        f\"Validation loss {loss_validation / num_validations:.4f}, \"\n",
    "        f\"Training Acc {train_accuracy:.2f}%, \"\n",
    "        f\"Validation Acc {validation_accuracy:.2f}%\"\n",
    "      )\n",
    "      if epoch >= next_print_epoch:\n",
    "          next_print_epoch += 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38943a86-cdd2-48a4-924c-d406f05f6cba",
   "metadata": {},
   "source": [
    "* ì—í¬í¬ ë°˜ë³µ: ì •í•´ì§„ íšŸìˆ˜ë§Œí¼ ì „ì²´ ë°ì´í„°ì…‹ í•™ìŠµì„ ë°˜ë³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "* í•™ìŠµ ëª¨ë“œ: model.train()ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµ ìƒíƒœë¡œ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "* ë¯¸ë‹ˆë°°ì¹˜ í•™ìŠµ: train_data_loaderì—ì„œ ë°ì´í„°ë¥¼ ë¯¸ë‹ˆë°°ì¹˜ ë‹¨ìœ„ë¡œ ê°€ì ¸ì™€ ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "* ëª¨ë¸ ì˜ˆì¸¡ (model(input))\n",
    "\n",
    "* ì†ì‹¤ ê³„ì‚° (loss_fn)\n",
    "\n",
    "* ì—­ì „íŒŒ (loss.backward())\n",
    "\n",
    "* ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ (optimizer.step())\n",
    "\n",
    "* í•™ìŠµ ì†ì‹¤ê³¼ ì •í™•ë„ ëˆ„ì  ê³„ì‚°\n",
    "\n",
    "* í‰ê°€ ëª¨ë“œ: model.eval()ìœ¼ë¡œ ëª¨ë¸ì„ í‰ê°€ ìƒíƒœë¡œ ì„¤ì •í•©ë‹ˆë‹¤ (ë“œë¡­ì•„ì›ƒ ë“± ë¹„í™œì„±í™”).\n",
    "\n",
    "* ê²€ì¦: validation_data_loaderì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ëª¨ë¸ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³ , ê²€ì¦ ì†ì‹¤ê³¼ ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤ (ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ëŠ” ì•ˆ í•¨).\n",
    "\n",
    "* ë¡œê¹…: ê° ì—í¬í¬ì˜ í•™ìŠµ/ê²€ì¦ ì†ì‹¤ê³¼ ì •í™•ë„ë¥¼ wandbì— ê¸°ë¡í•©ë‹ˆë‹¤.\n",
    "\n",
    "* ì¶œë ¥: ì£¼ê¸°ì ìœ¼ë¡œ í•™ìŠµ ì§„í–‰ ìƒí™©(ì†ì‹¤, ì •í™•ë„)ì„ í™”ë©´ì— ì¶œë ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7edecf4-c9f1-4da6-9493-9dd64275b2bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T14:19:47.224529Z",
     "start_time": "2025-10-18T14:19:47.218484Z"
    }
   },
   "outputs": [],
   "source": [
    "def main(args):\n",
    "  current_time_str = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "  config = {\n",
    "    'epochs': args.epochs,\n",
    "    'batch_size': args.batch_size,\n",
    "    'learning_rate': 1e-3,\n",
    "    'n_hidden_unit_list': [20, 20], # ì€ë‹‰ì¸µ ì„¤ì •ì€ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "  }\n",
    "\n",
    "  wandb.init(\n",
    "    mode=\"online\" if args.wandb else \"disabled\",\n",
    "    project=\"titanic_survival_prediction\", # wandb í”„ë¡œì íŠ¸ëª… ë³€ê²½\n",
    "    notes=\"Titanic survival prediction with MLP\", # wandb ë…¸íŠ¸ ë³€ê²½\n",
    "    tags=[\"mlp\", \"titanic\"], # wandb íƒœê·¸ ë³€ê²½\n",
    "    name=current_time_str,\n",
    "    config=config\n",
    "  )\n",
    "  print(\"--- wandb arguments ---\")\n",
    "  print(args)\n",
    "  print(\"--- wandb config ---\")\n",
    "  print(wandb.config)\n",
    "\n",
    "  # test_data_loaderë„ ë°˜í™˜ë˜ì§€ë§Œ, training_loopì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ\n",
    "  train_data_loader, validation_data_loader, test_data_loader = get_data()\n",
    "\n",
    "  linear_model, optimizer = get_model_and_optimizer()\n",
    "\n",
    "  print(\"\\n\" + \"#\" * 50)\n",
    "  print(\"Start Training...\")\n",
    "  training_loop(\n",
    "    model=linear_model,\n",
    "    optimizer=optimizer,\n",
    "    train_data_loader=train_data_loader,\n",
    "    validation_data_loader=validation_data_loader\n",
    "  )\n",
    "  print(\"Training Finished.\")\n",
    "  print(\"#\" * 50 + \"\\n\")\n",
    "\n",
    "  wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07456eb6-9fe5-4bf7-b9f5-174b145b372d",
   "metadata": {},
   "source": [
    "ì„¤ì • ë¡œë“œ: wandb ì‹¤í—˜ ì„¤ì •(config)ì„ ì •ì˜í•©ë‹ˆë‹¤ (ì—í¬í¬ ìˆ˜, ë°°ì¹˜ í¬ê¸° ë“±).\n",
    "\n",
    "wandb ì´ˆê¸°í™”: ì‹¤í—˜ ì¶”ì ì„ ìœ„í•´ wandbë¥¼ ì„¤ì •í•˜ê³  ì‹œì‘í•©ë‹ˆë‹¤. í”„ë¡œì íŠ¸ ì´ë¦„, ë…¸íŠ¸, íƒœê·¸ ë“±ì„ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë°ì´í„° ë¡œë”©: get_data() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "\n",
    "ëª¨ë¸/ì˜µí‹°ë§ˆì´ì € ìƒì„±: get_model_and_optimizer() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ëª¨ë¸ê³¼ ì˜µí‹°ë§ˆì´ì €ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤.\n",
    "\n",
    "í•™ìŠµ ì‹œì‘: training_loop() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ëª¨ë¸ í•™ìŠµ ë° ê²€ì¦ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
    "\n",
    "wandb ì¢…ë£Œ: ì‹¤í—˜ ê¸°ë¡ì„ ë§ˆì¹˜ê³  wandbë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "68787fd8-7b3e-4f84-b61d-b86b6b3fbf07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T14:19:47.800684Z",
     "start_time": "2025-10-18T14:19:47.231131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Jupyter Notebook. Using default hardcoded args.\n",
      "--- wandb arguments ---\n",
      "Namespace(wandb=False, batch_size=16, epochs=1000)\n",
      "--- wandb config ---\n",
      "{'epochs': 1000, 'batch_size': 16, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cyun0\\AppData\\Local\\Temp\\ipykernel_21216\\2165486214.py:37: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_df[\"alone\"].fillna(0, inplace=True)\n",
      "C:\\Users\\cyun0\\AppData\\Local\\Temp\\ipykernel_21216\\2165486214.py:54: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_df[\"Embarked\"].fillna(\"missing\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preprocessed DataFrame Columns ---\n",
      "Index(['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare',\n",
      "       'Embarked', 'title', 'family_num', 'alone'],\n",
      "      dtype='object')\n",
      "--- Preprocessed DataFrame Head ---\n",
      "   Survived  Pclass  Sex   Age  SibSp  Parch     Fare  Embarked  title  \\\n",
      "0       0.0       3    1  22.0      1      0   7.2500         2      2   \n",
      "1       1.0       1    0  38.0      1      0  71.2833         0      3   \n",
      "2       1.0       3    0  26.0      0      0   7.9250         2      1   \n",
      "3       1.0       1    0  35.0      1      0  53.1000         2      3   \n",
      "4       0.0       3    1  35.0      0      0   8.0500         2      2   \n",
      "\n",
      "   family_num  alone  \n",
      "0           1    0.0  \n",
      "1           1    0.0  \n",
      "2           0    1.0  \n",
      "3           1    0.0  \n",
      "4           0    1.0  \n",
      "\n",
      "Input Features (10): ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'title', 'family_num', 'alone']\n",
      "--- Full Train Dataset ---\n",
      "Data Size: 891, Input Shape: torch.Size([891, 10]), Target Shape: torch.Size([891])\n",
      "\n",
      "Train dataset size: 713\n",
      "Validation dataset size: 178\n",
      "Test dataset size: 418\n",
      "\n",
      "##################################################\n",
      "Start Training...\n",
      "Epoch 1, Training loss 0.7133, Validation loss 0.6617, Training Acc 52.31%, Validation Acc 66.29%\n",
      "Epoch 100, Training loss 0.5820, Validation loss 0.6097, Training Acc 70.27%, Validation Acc 69.10%\n",
      "Epoch 200, Training loss 0.5598, Validation loss 0.5935, Training Acc 70.41%, Validation Acc 69.10%\n",
      "Epoch 300, Training loss 0.5269, Validation loss 0.5782, Training Acc 70.97%, Validation Acc 69.66%\n",
      "Epoch 400, Training loss 0.4863, Validation loss 0.5567, Training Acc 77.00%, Validation Acc 73.60%\n",
      "Epoch 500, Training loss 0.4648, Validation loss 0.5182, Training Acc 78.26%, Validation Acc 73.60%\n",
      "Epoch 600, Training loss 0.4417, Validation loss 0.5338, Training Acc 79.10%, Validation Acc 75.28%\n",
      "Epoch 700, Training loss 0.4212, Validation loss 0.5412, Training Acc 81.63%, Validation Acc 71.91%\n",
      "Epoch 800, Training loss 0.4321, Validation loss 0.5234, Training Acc 80.79%, Validation Acc 74.72%\n",
      "Epoch 900, Training loss 0.4340, Validation loss 0.5387, Training Acc 80.36%, Validation Acc 76.40%\n",
      "Epoch 1000, Training loss 0.4279, Validation loss 0.5709, Training Acc 81.35%, Validation Acc 73.60%\n",
      "Training Finished.\n",
      "##################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys # sys ëª¨ë“ˆ ì„í¬íŠ¸ ì¶”ê°€\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--wandb\", action=argparse.BooleanOptionalAction, default=False, help=\"True or False\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-b\", \"--batch_size\", type=int, default=16, help=\"Batch size (int, default: 16)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-e\", \"--epochs\", type=int, default=1000, help=\"Number of training epochs (int, default:1000)\" # 1_000 -> 1000\n",
    "    )\n",
    "\n",
    "    # --- ğŸ‘‡ ì—¬ê¸° ìˆ˜ì • ğŸ‘‡ ---\n",
    "    # Jupyter ë…¸íŠ¸ë¶ í™˜ê²½ì¸ì§€ í™•ì¸\n",
    "    if 'ipykernel_launcher' in sys.argv[0]:\n",
    "        print(\"Running in Jupyter Notebook. Using default hardcoded args.\")\n",
    "        # ë…¸íŠ¸ë¶ì—ì„œ í…ŒìŠ¤íŠ¸í•  ë•Œ ì›í•˜ëŠ” ê°’ì„ ì§ì ‘ ì„¤ì •\n",
    "        args = argparse.Namespace(wandb=False, batch_size=16, epochs=1000) # ì˜ˆì‹œ: wandb ë¹„í™œì„±í™”, batch 16, epoch 1000\n",
    "        # WandBë¥¼ ì‚¬ìš©í•˜ê³  ì‹¶ë‹¤ë©´ wandb=True ë¡œ ë³€ê²½\n",
    "        # ë‹¤ë¥¸ batch_sizeë‚˜ epochsë¡œ í…ŒìŠ¤íŠ¸í•˜ê³  ì‹¶ë‹¤ë©´ ì´ ê°’ì„ ë³€ê²½\n",
    "    else:\n",
    "        # í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰ë  ë•ŒëŠ” ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±\n",
    "        args = parser.parse_args()\n",
    "    # --- ğŸ‘† ì—¬ê¸°ê¹Œì§€ ìˆ˜ì • ğŸ‘† ---\n",
    "\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83765600-acf9-41c7-a241-8e4168faff7f",
   "metadata": {},
   "source": [
    "í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰í•  ë•Œ --epochs, --batch_size ê°™ì€ ì¸ìë¥¼ ë°›ì„ ìˆ˜ ìˆê²Œ ì„¤ì •í•©ë‹ˆë‹¤. ë°›ì€ ì¸ìë¥¼ main í•¨ìˆ˜ì— ì „ë‹¬í•˜ì—¬ ì‹¤í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b2bc30-c744-4282-be83-da1778bd80d4",
   "metadata": {},
   "source": [
    "![í•™ìŠµ1](https://github.com/O-E2/deep_learning/blob/ee3a2d4c8d369f2970393fc38fefdefbeedc8e62/dl_basic_wandb.png)\n",
    "\n",
    "Wandb URL\n",
    "https://wandb.ai/cyun0407-korea-university-of-technology-and-education/titanic_survival_prediction/runs/i4pqcafr  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8138a5-2878-47bd-9a44-2853765f5a3c",
   "metadata": {},
   "source": [
    "# [ìš”êµ¬ì‚¬í•­ 2]\n",
    "\n",
    "Wansb URL \n",
    "https://wandb.ai/cyun0407-korea-university-of-technology-and-education/titanic_hyperparameter_tuning?nw=nwusercyun0407\n",
    "\n",
    "ë” ë‚˜ì€ ì„±ëŠ¥ì„ ì‚°ì¶œí•˜ëŠ” Activation Function : ReLU\n",
    "ë” ë‚˜ì€ ì„±ëŠ¥ì„ ì‚°ì¶œí•˜ëŠ” Batch Size : 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "287d3b1d-cb29-48fc-b45e-6670319fe9e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T14:19:48.125984Z",
     "start_time": "2025-10-18T14:19:48.119443Z"
    }
   },
   "outputs": [],
   "source": [
    "# find_best_hyperparameters.py\n",
    "import os, json, sys, argparse, copy\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pathlib import Path\n",
    "\n",
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, X, y): self.X = torch.FloatTensor(X); self.y = torch.LongTensor(y)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx): return {'input': self.X[idx], 'target': self.y[idx]}\n",
    "    def __str__(self): return f\"Data Size: {len(self.X)}, Input Shape: {self.X.shape}, Target Shape: {self.y.shape}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d55eed-b462-412b-86bc-1cff947973dc",
   "metadata": {},
   "source": [
    "íŒŒì´í† ì¹˜ DataLoaderê°€ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë°ì´í„°ì…‹ì˜ êµ¬ì¡°(ë°ì´í„° ë¡œë”©, ê¸¸ì´ ë°˜í™˜, íŠ¹ì • í•­ëª© ì ‘ê·¼ ë°©ë²•)ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. TitanicDatasetì€ í•™ìŠµ ë° ê²€ì¦ ë°ì´í„°ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1814570a-a867-4058-b351-16f0fcd68e2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T14:19:48.471633Z",
     "start_time": "2025-10-18T14:19:48.460089Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_preprocessed_dataset_1(all_df): \n",
    "    Fare_mean = all_df[[\"Pclass\", \"Fare\"]].groupby(\"Pclass\").mean().reset_index(); Fare_mean.columns = [\"Pclass\", \"Fare_mean\"]\n",
    "    all_df = pd.merge(all_df, Fare_mean, on=\"Pclass\", how=\"left\"); all_df.loc[all_df[\"Fare\"].isnull(), \"Fare\"] = all_df[\"Fare_mean\"]\n",
    "    return all_df.drop(columns=[\"Fare_mean\"])\n",
    "def get_preprocessed_dataset_2(all_df): \n",
    "    name_df = all_df[\"Name\"].str.split(\"[,.]\", n=2, expand=True)\n",
    "    if name_df.shape[1] == 3:\n",
    "        name_df.columns = [\"family_name\", \"title\", \"name\"]; name_df[\"family_name\"] = name_df[\"family_name\"].str.strip(); name_df[\"title\"] = name_df[\"title\"].str.strip(); name_df[\"name\"] = name_df[\"name\"].str.strip()\n",
    "        all_df = pd.concat([all_df, name_df], axis=1)\n",
    "    else: all_df['title'] = 'unknown' # ì´ë¦„ ë¶„ë¦¬ ì‹¤íŒ¨ ì‹œ 'unknown' title ì¶”ê°€\n",
    "    return all_df\n",
    "def get_preprocessed_dataset_3(all_df): \n",
    "    if 'title' not in all_df.columns:\n",
    "        if all_df['Age'].isnull().any(): all_df['Age'] = all_df['Age'].fillna(all_df['Age'].median())\n",
    "        return all_df\n",
    "    title_age_median = all_df[[\"title\", \"Age\"]].groupby(\"title\").median().round().reset_index(); title_age_median.columns = [\"title\", \"title_age_median\"]\n",
    "    all_df = pd.merge(all_df, title_age_median, on=\"title\", how=\"left\"); all_df.loc[all_df[\"Age\"].isnull(), \"Age\"] = all_df[\"title_age_median\"]\n",
    "    if \"title_age_median\" in all_df.columns: all_df = all_df.drop([\"title_age_median\"], axis=1)\n",
    "    return all_df\n",
    "def get_preprocessed_dataset_4(all_df): \n",
    "    all_df[\"family_num\"] = all_df[\"Parch\"] + all_df[\"SibSp\"]; all_df.loc[all_df[\"family_num\"] == 0, \"alone\"] = 1; all_df[\"alone\"] = all_df[\"alone\"].fillna(0).astype(float)\n",
    "    cols_to_drop = [\"PassengerId\", \"Name\", \"family_name\", \"name\", \"Ticket\", \"Cabin\"]; existing_cols_to_drop = [col for col in cols_to_drop if col in all_df.columns]\n",
    "    return all_df.drop(existing_cols_to_drop, axis=1)\n",
    "def get_preprocessed_dataset_5(all_df):\n",
    "    if 'title' in all_df.columns: all_df.loc[~((all_df[\"title\"] == \"Mr\") | (all_df[\"title\"] == \"Miss\") | (all_df[\"title\"] == \"Mrs\") | (all_df[\"title\"] == \"Master\")), \"title\"] = \"other\"\n",
    "    all_df[\"Embarked\"] = all_df[\"Embarked\"].fillna(\"missing\")\n",
    "    return all_df\n",
    "def get_preprocessed_dataset_6(all_df): \n",
    "    category_features = all_df.select_dtypes(include=['object']).columns\n",
    "    for cat_feat in category_features:\n",
    "        le = LabelEncoder(); valid_indices = all_df[cat_feat].notna()\n",
    "        if valid_indices.any():\n",
    "            all_df.loc[valid_indices, cat_feat] = le.fit_transform(all_df.loc[valid_indices, cat_feat])\n",
    "            try: all_df[cat_feat] = pd.to_numeric(all_df[cat_feat])\n",
    "            except ValueError: pass\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bdd747-11c6-4b6d-a3f3-11deb5c95ab7",
   "metadata": {},
   "source": [
    "íƒ€ì´íƒ€ë‹‰ ì›ë³¸ ë°ì´í„°ë¥¼ ë‹¨ê³„ë³„ë¡œ ê°€ê³µí•˜ëŠ” í•¨ìˆ˜ë“¤ì…ë‹ˆë‹¤. ê²°ì¸¡ì¹˜ ì²˜ë¦¬, íŠ¹ì§• ìƒì„± ë° ì¶”ì¶œ, ë²”ì£¼í˜• ë°ì´í„°ì˜ ìˆ˜ì¹˜í™” ë“±ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83ed857c-d889-4049-be19-e4349cee6b5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T14:19:48.803606Z",
     "start_time": "2025-10-18T14:19:48.796147Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_preprocessed_data_raw(): \n",
    "    \"\"\"CSV ë¡œë“œ, ì „ì²˜ë¦¬, í•™ìŠµ/ê²€ì¦ìš© ë°ì´í„° ë°°ì—´ ë°˜í™˜\"\"\"\n",
    "    try: CURRENT_FILE_PATH = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError: CURRENT_FILE_PATH = os.getcwd()\n",
    "    train_data_path = os.path.join(CURRENT_FILE_PATH, \"train.csv\")\n",
    "    test_data_path = os.path.join(CURRENT_FILE_PATH, \"test.csv\") # testëŠ” ë¡œë“œë§Œ í•˜ê³  ì‚¬ìš© ì•ˆí•¨\n",
    "    try:\n",
    "        train_df = pd.read_csv(train_data_path)\n",
    "        test_df = pd.read_csv(test_data_path)\n",
    "    except FileNotFoundError as e: print(f\"Error: {e}\"); sys.exit(1)\n",
    "\n",
    "    all_df = pd.concat([train_df, test_df], sort=False) # ì „ì²˜ë¦¬ ì¼ê´€ì„± ìœ„í•´ í•©ì¹¨\n",
    "    all_df = get_preprocessed_dataset_1(all_df); all_df = get_preprocessed_dataset_2(all_df)\n",
    "    all_df = get_preprocessed_dataset_3(all_df); all_df = get_preprocessed_dataset_4(all_df)\n",
    "    all_df = get_preprocessed_dataset_5(all_df); all_df = get_preprocessed_dataset_6(all_df)\n",
    "    train_X_df = all_df[~all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1)\n",
    "    train_y = train_df[\"Survived\"]\n",
    "\n",
    "    # í•™ìŠµ ë°ì´í„°ì— ëŒ€í•´ì„œë§Œ ìµœì¢… NaN ë° íƒ€ì… í™•ì¸\n",
    "    print(\"\\n--- Final Data Check & Fill NaNs (Train Data Only) ---\")\n",
    "    columns_to_check = train_X_df.columns\n",
    "    df = train_X_df\n",
    "    for col in columns_to_check:\n",
    "        if df[col].dtype == 'object':\n",
    "            try: df[col] = pd.to_numeric(df[col])\n",
    "            except ValueError: df[col] = 0 # ë³€í™˜ ë¶ˆê°€ì‹œ 0ìœ¼ë¡œ ì±„ì›€\n",
    "        if df[col].isnull().any(): df[col] = df[col].fillna(0) # NaN 0ìœ¼ë¡œ ì±„ì›€\n",
    "    train_X = train_X_df.reset_index(drop=True)\n",
    "\n",
    "    print(\"Train data preprocessing finished.\")\n",
    "    return train_X.values, train_y.values # í•™ìŠµ ë°ì´í„°(X, y)ë§Œ ë°˜í™˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc9033a-40eb-4db0-94e3-abb9f1127b09",
   "metadata": {},
   "source": [
    "CSV íŒŒì¼ì„ ë¡œë“œí•˜ê³  ìœ„ì˜ ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤ì„ ìˆœì„œëŒ€ë¡œ í˜¸ì¶œí•©ë‹ˆë‹¤. ìµœì¢…ì ìœ¼ë¡œ í•™ìŠµ ë°ì´í„°ë§Œ ë„˜íŒŒì´ ë°°ì—´ í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0bef9a48-c6d2-4eda-b441-0f4f570a9e10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T14:19:49.511837Z",
     "start_time": "2025-10-18T14:19:49.506287Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataloaders(train_X, train_y, batch_size_config): \n",
    "    \"\"\"í•™ìŠµ/ê²€ì¦ DataLoader ìƒì„±\"\"\"\n",
    "    dataset = TitanicDataset(train_X, train_y)\n",
    "    generator = torch.Generator().manual_seed(42) # ë°ì´í„° ë¶„í•  ì¬í˜„ì„± ìœ„í•œ ì‹œë“œ ê³ ì •\n",
    "    train_dataset, validation_dataset = random_split(dataset, [0.8, 0.2], generator=generator)\n",
    "    print(f\"\\nTrain size: {len(train_dataset)}, Validation size: {len(validation_dataset)}\")\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size_config, shuffle=True)\n",
    "    validation_loader = DataLoader(dataset=validation_dataset, batch_size=len(validation_dataset)) # ê²€ì¦ì€ ì „ì²´ ë°ì´í„° ì‚¬ìš©\n",
    "    return train_loader, validation_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3552c139-f80c-4061-8b6f-c789e8713075",
   "metadata": {},
   "source": [
    "ì „ì²˜ë¦¬ëœ í•™ìŠµ ë°ì´í„°(X, y)ì™€ ë°°ì¹˜ í¬ê¸°ë¥¼ ë°›ì•„, ë°ì´í„°ë¥¼ í•™ìŠµìš©ê³¼ ê²€ì¦ìš©ìœ¼ë¡œ 8:2 ë¹„ìœ¨ë¡œ ë‚˜ëˆˆ ë’¤ ê°ê°ì— ëŒ€í•œ DataLoader ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "07a9fe2a-cfb2-4ad1-a35d-8a4ed5686014",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T14:19:49.610115Z",
     "start_time": "2025-10-18T14:19:49.604580Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyModel(nn.Module): \n",
    "    \"\"\"í™œì„±í™” í•¨ìˆ˜ì™€ ì€ë‹‰ì¸µ í¬ê¸°ë¥¼ ì¸ìë¡œ ë°›ëŠ” MLP ëª¨ë¸\"\"\"\n",
    "    def __init__(self, n_input, n_output, activation_fn_class, n_hidden1=20, n_hidden2=20):\n",
    "        super().__init__(); activation = activation_fn_class()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_input, n_hidden1), activation,\n",
    "            nn.Linear(n_hidden1, n_hidden2), activation,\n",
    "            nn.Linear(n_hidden2, n_output)\n",
    "        )\n",
    "    def forward(self, x): return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21456c8-02d3-4a63-a32c-af3be398f8f6",
   "metadata": {},
   "source": [
    "íŒŒì´í† ì¹˜ nn.Moduleì„ ìƒì†ë°›ì•„ ê°„ë‹¨í•œ ëª¨ë¸ êµ¬ì¡°ë¥¼ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c6302d8b-8c56-495f-90d5-579c118a788b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T14:19:49.655483Z",
     "start_time": "2025-10-18T14:19:49.650398Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model_and_optimizer(activation_fn_class, learning_rate, n_hidden1=20, n_hidden2=20): # ... (ì´ì „ê³¼ ë™ì¼) ...\n",
    "    \"\"\"ëª¨ë¸ ê°ì²´ì™€ SGD ì˜µí‹°ë§ˆì´ì € ìƒì„±\"\"\"\n",
    "    model = MyModel(10, 2, activation_fn_class, n_hidden1, n_hidden2) # ì…ë ¥ 10ê°œ, ì¶œë ¥ 2ê°œ ê³ ì •\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303ad0e1-8893-42d9-8d1a-c90b0851a04c",
   "metadata": {},
   "source": [
    "MyModel í´ë˜ìŠ¤ë¥¼ ì´ìš©í•´ ëª¨ë¸ ê°ì²´ë¥¼ ìƒì„±í•˜ê³ , í•™ìŠµì— ì‚¬ìš©í•  SGD ì˜µí‹°ë§ˆì´ì €ë¥¼ ì„¤ì •í•˜ì—¬ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f7e520c-37a3-49ac-ad4a-084895580032",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T14:19:49.734636Z",
     "start_time": "2025-10-18T14:19:49.725370Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- í•™ìŠµ ë£¨í”„: ìµœì¢… ì •í™•ë„ë§Œ ë°˜í™˜ ---\n",
    "def simplified_training_loop(model, optimizer, train_data_loader, validation_data_loader, n_epochs, loss_fn):\n",
    "    \"\"\"ëª¨ë¸ í•™ìŠµ/ê²€ì¦ ìˆ˜í–‰ í›„ ìµœì¢… ê²€ì¦ ì •í™•ë„ ë°˜í™˜ (ìµœê³  ê¸°ë¡ ì—†ìŒ)\"\"\"\n",
    "    print(f\"\\nStart Training for {n_epochs} epochs...\")\n",
    "    final_validation_accuracy = 0.0 # ë§ˆì§€ë§‰ ì—í¬í¬ ì •í™•ë„ ì €ì¥ìš©\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # --- í•™ìŠµ \n",
    "        model.train(); loss_train_epoch = 0.0; correct_train_epoch = 0; total_train_samples = 0\n",
    "        for batch in train_data_loader: # ... (í•™ìŠµ ë¡œì§ ë™ì¼) ...\n",
    "            input_data = batch['input']; target = batch['target']; optimizer.zero_grad(); output_train = model(input_data); loss = loss_fn(output_train, target); loss.backward(); optimizer.step()\n",
    "            loss_train_epoch += loss.item() * input_data.size(0); _, predicted = torch.max(output_train.data, 1); total_train_samples += target.size(0); correct_train_epoch += (predicted == target).sum().item()\n",
    "        avg_loss_train = loss_train_epoch / total_train_samples if total_train_samples else 0\n",
    "        train_accuracy = 100 * correct_train_epoch / total_train_samples if total_train_samples else 0\n",
    "\n",
    "        # --- ê²€ì¦ \n",
    "        model.eval(); loss_val_epoch = 0.0; correct_val_epoch = 0; total_val_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in validation_data_loader: # ... (ê²€ì¦ ë¡œì§ ë™ì¼) ...\n",
    "                input_data = batch['input']; target = batch['target']; output_validation = model(input_data); loss = loss_fn(output_validation, target); loss_val_epoch += loss.item() * input_data.size(0)\n",
    "                _, predicted = torch.max(output_validation.data, 1); total_val_samples += target.size(0); correct_val_epoch += (predicted == target).sum().item()\n",
    "        avg_loss_validation = loss_val_epoch / total_val_samples if total_val_samples else 0\n",
    "        validation_accuracy = 100 * correct_val_epoch / total_val_samples if total_val_samples else 0\n",
    "        final_validation_accuracy = validation_accuracy # ë§ˆì§€ë§‰ ê°’ ê°±ì‹ \n",
    "\n",
    "        # ë¡œê¹… ë° ì¶œë ¥ \n",
    "        if wandb.run: wandb.log({\"Epoch\": epoch, \"Training loss\": avg_loss_train, \"Validation loss\": avg_loss_validation, \"Training accuracy\": train_accuracy, \"Validation accuracy\": validation_accuracy})\n",
    "        if epoch % 100 == 0 or epoch == 1: print(f\"Epoch {epoch:>{len(str(n_epochs))}}/{n_epochs}, Train Loss: {avg_loss_train:.4f}, Acc: {train_accuracy:.2f}% | Val Loss: {avg_loss_validation:.4f}, Acc: {validation_accuracy:.2f}%\")\n",
    "\n",
    "    print(f\"\\n--- Run Finished --- Final Val Acc: {final_validation_accuracy:.2f}%\")\n",
    "    return final_validation_accuracy # ìµœì¢… ê²€ì¦ ì •í™•ë„ë§Œ ë°˜í™˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd276ad-6f18-44b4-9e7b-f2729f69d2b9",
   "metadata": {},
   "source": [
    "ëª¨ë¸ í•™ìŠµê³¼ ê²€ì¦ì„ ì§€ì •ëœ ì—í¬í¬ë§Œí¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c21936d0-7ce6-4ab7-bc56-095143a45526",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T14:19:49.807241Z",
     "start_time": "2025-10-18T14:19:49.795626Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ë° ìµœì  ì¡°í•© ì €ì¥ ---\n",
    "def find_hyperparameters(args):\n",
    "    \"\"\"ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° 'ì¡°í•©'ë§Œ ì°¾ì•„ json íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "    current_time_str = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    # ì‹¤í—˜ ì„¤ì •\n",
    "    batch_sizes_to_test = [16, 32, 64, 128]\n",
    "    activation_functions_to_test = {\"Sigmoid\": nn.Sigmoid, \"ReLU\": nn.ReLU, \"ELU\": nn.ELU, \"LeakyReLU\": nn.LeakyReLU}\n",
    "    learning_rate = 1e-3; n_hidden_units = [20, 20]; loss_fn = nn.CrossEntropyLoss()\n",
    "    # ë°ì´í„° ë¡œë”©\n",
    "    print(\"Preprocessing data for tuning...\"); train_X, train_y = get_preprocessed_data_raw()[:2] # í•™ìŠµ ë°ì´í„°ë§Œ í•„ìš”\n",
    "    # ìµœê³  ì„±ëŠ¥ ì¶”ì \n",
    "    overall_best_accuracy = -1.0; overall_best_config = {}\n",
    "\n",
    "    # ì‹¤í—˜ ë£¨í”„\n",
    "    for batch_size in batch_sizes_to_test:\n",
    "        print(f\"\\n{'='*25} Testing Batch Size: {batch_size} {'='*25}\")\n",
    "        train_loader, validation_loader = get_dataloaders(train_X, train_y, batch_size)\n",
    "        for activation_name, activation_fn_class in activation_functions_to_test.items():\n",
    "            print(f\"\\n--- Testing Activation: {activation_name} ---\")\n",
    "            config = {'epochs': args.epochs, 'batch_size': batch_size, 'learning_rate': learning_rate, 'n_hidden_unit_list': n_hidden_units, 'activation_function': activation_name}\n",
    "            run_name = f\"Tune_{activation_name}_BS{batch_size}_{current_time_str}\"\n",
    "            run = wandb.init(mode=\"online\" if args.wandb else \"disabled\", project=\"titanic_hyperparameter_tuning\", name=run_name, config=config, reinit=True)\n",
    "            print(\"--- Run Config ---\"); print(config)\n",
    "            model, optimizer = get_model_and_optimizer(activation_fn_class, config['learning_rate'], n_hidden1=config['n_hidden_unit_list'][0], n_hidden2=config['n_hidden_unit_list'][1])\n",
    "\n",
    "            # ê°„ì†Œí™”ëœ í•™ìŠµ ë£¨í”„ ì‹¤í–‰, ìµœì¢… ì •í™•ë„ ë°›ê¸°\n",
    "            final_accuracy_run = simplified_training_loop(model, optimizer, train_loader, validation_loader, config['epochs'], loss_fn)\n",
    "\n",
    "            # ìµœê³  ì¡°í•© ê°±ì‹  í™•ì¸ (ìµœì¢… ì •í™•ë„ ê¸°ì¤€)\n",
    "            if final_accuracy_run > overall_best_accuracy:\n",
    "                overall_best_accuracy = final_accuracy_run\n",
    "                overall_best_config = config # configë§Œ ì €ì¥\n",
    "                print(f\" New Best Config Found! Final Val Acc: {overall_best_accuracy:.2f}%, Config: {activation_name}/BS={batch_size}\")\n",
    "            run.finish()\n",
    "\n",
    "    # ìµœì¢… ê²°ê³¼ ìš”ì•½ ë° ì €ì¥ (configë§Œ)\n",
    "    print(\"\\n\" + \"=\" * 60); print(\" Hyperparameter Tuning Finished (Config Only) \"); print(\"=\" * 60)\n",
    "    if overall_best_config:\n",
    "        print(f\"Overall Best Final Val Acc: {overall_best_accuracy:.2f}%\")\n",
    "        print(f\"Best Config Found: {overall_best_config}\")\n",
    "        best_config_path = \"best_hyperparameters.json\"\n",
    "        try:\n",
    "            with open(best_config_path, 'w') as f: json.dump(overall_best_config, f, indent=4)\n",
    "            print(f\"Best hyperparameters saved to {best_config_path}\")\n",
    "        except Exception as e: print(f\"Error saving best config: {e}\")\n",
    "    else: print(\"No successful training run recorded.\")\n",
    "    print(\"=\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db456e86-f7e7-4c8a-9dd2-0f785f5c4b33",
   "metadata": {},
   "source": [
    "ì§€ì •ëœ ë°°ì¹˜ í¬ê¸°ì™€ í™œì„±í™” í•¨ìˆ˜ë“¤ì˜ ëª¨ë“  ì¡°í•©ì— ëŒ€í•´ ë£¨í”„ë¥¼ ë•ë‹ˆë‹¤. ê° ì¡°í•©ë§ˆë‹¤ simplified_training_loopë¥¼ ì‹¤í–‰í•˜ì—¬ ìµœì¢… ê²€ì¦ ì •í™•ë„ë¥¼ ì–»ê³ , ì´ ì •í™•ë„ê°€ ì§€ê¸ˆê¹Œì§€ ê¸°ë¡ëœ ìµœê³  ì •í™•ë„ë³´ë‹¤ ë†’ìœ¼ë©´ í•´ë‹¹ ì¡°í•©ì˜ ì„¤ì •ì„ overall_best_configì— ì €ì¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3a3f2cbc-90c7-4fed-a961-230562e2a4e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T14:22:11.744161Z",
     "start_time": "2025-10-18T14:19:49.851601Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in interactive mode. Using default args: epochs=1000, wandb=False\n",
      "Preprocessing data for tuning...\n",
      "\n",
      "--- Final Data Check & Fill NaNs (Train Data Only) ---\n",
      "Train data preprocessing finished.\n",
      "\n",
      "========================= Testing Batch Size: 16 =========================\n",
      "\n",
      "Train size: 713, Validation size: 178\n",
      "\n",
      "--- Testing Activation: Sigmoid ---\n",
      "--- Run Config ---\n",
      "{'epochs': 1000, 'batch_size': 16, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'Sigmoid'}\n",
      "\n",
      "Start Training for 1000 epochs...\n",
      "Epoch    1/1000, Train Loss: 0.6792, Acc: 59.33% | Val Loss: 0.6332, Acc: 70.79%\n",
      "Epoch  100/1000, Train Loss: 0.6665, Acc: 59.33% | Val Loss: 0.6286, Acc: 70.79%\n",
      "Epoch  200/1000, Train Loss: 0.6532, Acc: 59.33% | Val Loss: 0.6230, Acc: 70.79%\n",
      "Epoch  300/1000, Train Loss: 0.6373, Acc: 59.47% | Val Loss: 0.6167, Acc: 70.22%\n",
      "Epoch  400/1000, Train Loss: 0.6191, Acc: 69.57% | Val Loss: 0.6109, Acc: 69.66%\n",
      "Epoch  500/1000, Train Loss: 0.6072, Acc: 69.57% | Val Loss: 0.6115, Acc: 66.85%\n",
      "Epoch  600/1000, Train Loss: 0.6006, Acc: 69.28% | Val Loss: 0.6128, Acc: 65.73%\n",
      "Epoch  700/1000, Train Loss: 0.5969, Acc: 68.86% | Val Loss: 0.6126, Acc: 66.29%\n",
      "Epoch  800/1000, Train Loss: 0.5942, Acc: 68.86% | Val Loss: 0.6087, Acc: 66.29%\n",
      "Epoch  900/1000, Train Loss: 0.5920, Acc: 68.86% | Val Loss: 0.6061, Acc: 66.29%\n",
      "Epoch 1000/1000, Train Loss: 0.5902, Acc: 68.86% | Val Loss: 0.6045, Acc: 66.29%\n",
      "\n",
      "--- Run Finished --- Final Val Acc: 66.29%\n",
      " New Best Config Found! Final Val Acc: 66.29%, Config: Sigmoid/BS=16\n",
      "\n",
      "--- Testing Activation: ReLU ---\n",
      "--- Run Config ---\n",
      "{'epochs': 1000, 'batch_size': 16, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'ReLU'}\n",
      "\n",
      "Start Training for 1000 epochs...\n",
      "Epoch    1/1000, Train Loss: 0.6891, Acc: 61.29% | Val Loss: 0.6717, Acc: 61.24%\n",
      "Epoch  100/1000, Train Loss: 0.5936, Acc: 68.86% | Val Loss: 0.5912, Acc: 67.42%\n",
      "Epoch  200/1000, Train Loss: 0.5826, Acc: 70.55% | Val Loss: 0.5751, Acc: 67.98%\n",
      "Epoch  300/1000, Train Loss: 0.5718, Acc: 71.39% | Val Loss: 0.5528, Acc: 73.03%\n",
      "Epoch  400/1000, Train Loss: 0.5536, Acc: 72.79% | Val Loss: 0.5328, Acc: 74.16%\n",
      "Epoch  500/1000, Train Loss: 0.5355, Acc: 73.77% | Val Loss: 0.5027, Acc: 75.28%\n",
      "Epoch  600/1000, Train Loss: 0.5046, Acc: 77.70% | Val Loss: 0.4913, Acc: 75.28%\n",
      "Epoch  700/1000, Train Loss: 0.4791, Acc: 77.70% | Val Loss: 0.4656, Acc: 79.78%\n",
      "Epoch  800/1000, Train Loss: 0.4740, Acc: 78.96% | Val Loss: 0.4347, Acc: 82.58%\n",
      "Epoch  900/1000, Train Loss: 0.4841, Acc: 77.70% | Val Loss: 0.4229, Acc: 83.15%\n",
      "Epoch 1000/1000, Train Loss: 0.4564, Acc: 79.80% | Val Loss: 0.4300, Acc: 83.71%\n",
      "\n",
      "--- Run Finished --- Final Val Acc: 83.71%\n",
      " New Best Config Found! Final Val Acc: 83.71%, Config: ReLU/BS=16\n",
      "\n",
      "--- Testing Activation: ELU ---\n",
      "--- Run Config ---\n",
      "{'epochs': 1000, 'batch_size': 16, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'ELU'}\n",
      "\n",
      "Start Training for 1000 epochs...\n",
      "Epoch    1/1000, Train Loss: 0.6360, Acc: 64.94% | Val Loss: 0.6145, Acc: 65.73%\n",
      "Epoch  100/1000, Train Loss: 0.5795, Acc: 69.71% | Val Loss: 0.5846, Acc: 67.42%\n",
      "Epoch  200/1000, Train Loss: 0.5388, Acc: 72.93% | Val Loss: 0.5798, Acc: 70.79%\n",
      "Epoch  300/1000, Train Loss: 0.4956, Acc: 76.30% | Val Loss: 0.4909, Acc: 79.21%\n",
      "Epoch  400/1000, Train Loss: 0.4712, Acc: 78.40% | Val Loss: 0.4450, Acc: 81.46%\n",
      "Epoch  500/1000, Train Loss: 0.4752, Acc: 79.24% | Val Loss: 0.4594, Acc: 81.46%\n",
      "Epoch  600/1000, Train Loss: 0.4577, Acc: 78.96% | Val Loss: 0.4378, Acc: 82.02%\n",
      "Epoch  700/1000, Train Loss: 0.4517, Acc: 78.68% | Val Loss: 0.4222, Acc: 82.02%\n",
      "Epoch  800/1000, Train Loss: 0.4495, Acc: 78.40% | Val Loss: 0.4220, Acc: 81.46%\n",
      "Epoch  900/1000, Train Loss: 0.4438, Acc: 80.08% | Val Loss: 0.4399, Acc: 83.15%\n",
      "Epoch 1000/1000, Train Loss: 0.4379, Acc: 80.50% | Val Loss: 0.4949, Acc: 80.90%\n",
      "\n",
      "--- Run Finished --- Final Val Acc: 80.90%\n",
      "\n",
      "--- Testing Activation: LeakyReLU ---\n",
      "--- Run Config ---\n",
      "{'epochs': 1000, 'batch_size': 16, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'LeakyReLU'}\n",
      "\n",
      "Start Training for 1000 epochs...\n",
      "Epoch    1/1000, Train Loss: 0.6913, Acc: 53.30% | Val Loss: 0.6628, Acc: 64.61%\n",
      "Epoch  100/1000, Train Loss: 0.5810, Acc: 69.99% | Val Loss: 0.5878, Acc: 65.73%\n",
      "Epoch  200/1000, Train Loss: 0.5673, Acc: 72.65% | Val Loss: 0.5632, Acc: 71.91%\n",
      "Epoch  300/1000, Train Loss: 0.5340, Acc: 74.05% | Val Loss: 0.5311, Acc: 71.91%\n",
      "Epoch  400/1000, Train Loss: 0.4985, Acc: 76.44% | Val Loss: 0.5072, Acc: 77.53%\n",
      "Epoch  500/1000, Train Loss: 0.4771, Acc: 78.26% | Val Loss: 0.4145, Acc: 84.27%\n",
      "Epoch  600/1000, Train Loss: 0.4804, Acc: 77.84% | Val Loss: 0.4131, Acc: 83.15%\n",
      "Epoch  700/1000, Train Loss: 0.4680, Acc: 79.52% | Val Loss: 0.4021, Acc: 85.39%\n",
      "Epoch  800/1000, Train Loss: 0.4460, Acc: 80.93% | Val Loss: 0.4163, Acc: 85.96%\n",
      "Epoch  900/1000, Train Loss: 0.4632, Acc: 80.08% | Val Loss: 0.4255, Acc: 85.96%\n",
      "Epoch 1000/1000, Train Loss: 0.4579, Acc: 80.08% | Val Loss: 0.4196, Acc: 82.58%\n",
      "\n",
      "--- Run Finished --- Final Val Acc: 82.58%\n",
      "\n",
      "========================= Testing Batch Size: 32 =========================\n",
      "\n",
      "Train size: 713, Validation size: 178\n",
      "\n",
      "--- Testing Activation: Sigmoid ---\n",
      "--- Run Config ---\n",
      "{'epochs': 1000, 'batch_size': 32, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'Sigmoid'}\n",
      "\n",
      "Start Training for 1000 epochs...\n",
      "Epoch    1/1000, Train Loss: 0.6702, Acc: 59.33% | Val Loss: 0.6215, Acc: 70.79%\n",
      "Epoch  100/1000, Train Loss: 0.6571, Acc: 59.33% | Val Loss: 0.6204, Acc: 70.79%\n",
      "Epoch  200/1000, Train Loss: 0.6461, Acc: 59.33% | Val Loss: 0.6160, Acc: 70.79%\n",
      "Epoch  300/1000, Train Loss: 0.6355, Acc: 59.33% | Val Loss: 0.6127, Acc: 70.79%\n",
      "Epoch  400/1000, Train Loss: 0.6259, Acc: 68.16% | Val Loss: 0.6105, Acc: 67.42%\n",
      "Epoch  500/1000, Train Loss: 0.6179, Acc: 68.72% | Val Loss: 0.6095, Acc: 67.98%\n",
      "Epoch  600/1000, Train Loss: 0.6116, Acc: 69.14% | Val Loss: 0.6107, Acc: 66.85%\n",
      "Epoch  700/1000, Train Loss: 0.6070, Acc: 69.28% | Val Loss: 0.6127, Acc: 66.85%\n",
      "Epoch  800/1000, Train Loss: 0.6039, Acc: 69.00% | Val Loss: 0.6149, Acc: 66.29%\n",
      "Epoch  900/1000, Train Loss: 0.6018, Acc: 68.86% | Val Loss: 0.6156, Acc: 66.29%\n",
      "Epoch 1000/1000, Train Loss: 0.6003, Acc: 68.86% | Val Loss: 0.6155, Acc: 66.29%\n",
      "\n",
      "--- Run Finished --- Final Val Acc: 66.29%\n",
      "\n",
      "--- Testing Activation: ReLU ---\n",
      "--- Run Config ---\n",
      "{'epochs': 1000, 'batch_size': 32, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'ReLU'}\n",
      "\n",
      "Start Training for 1000 epochs...\n",
      "Epoch    1/1000, Train Loss: 0.8389, Acc: 62.69% | Val Loss: 0.6606, Acc: 69.66%\n",
      "Epoch  100/1000, Train Loss: 0.5963, Acc: 68.30% | Val Loss: 0.6125, Acc: 66.85%\n",
      "Epoch  200/1000, Train Loss: 0.5846, Acc: 69.85% | Val Loss: 0.5764, Acc: 71.91%\n",
      "Epoch  300/1000, Train Loss: 0.5750, Acc: 70.13% | Val Loss: 0.5765, Acc: 71.35%\n",
      "Epoch  400/1000, Train Loss: 0.5624, Acc: 71.11% | Val Loss: 0.5561, Acc: 71.91%\n",
      "Epoch  500/1000, Train Loss: 0.5548, Acc: 71.39% | Val Loss: 0.5438, Acc: 73.03%\n",
      "Epoch  600/1000, Train Loss: 0.5425, Acc: 71.81% | Val Loss: 0.5037, Acc: 77.53%\n",
      "Epoch  700/1000, Train Loss: 0.5258, Acc: 72.37% | Val Loss: 0.4951, Acc: 74.16%\n",
      "Epoch  800/1000, Train Loss: 0.5344, Acc: 72.65% | Val Loss: 0.6841, Acc: 72.47%\n",
      "Epoch  900/1000, Train Loss: 0.4895, Acc: 76.30% | Val Loss: 0.4453, Acc: 79.21%\n",
      "Epoch 1000/1000, Train Loss: 0.4813, Acc: 77.56% | Val Loss: 0.5268, Acc: 76.97%\n",
      "\n",
      "--- Run Finished --- Final Val Acc: 76.97%\n",
      "\n",
      "--- Testing Activation: ELU ---\n",
      "--- Run Config ---\n",
      "{'epochs': 1000, 'batch_size': 32, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'ELU'}\n",
      "\n",
      "Start Training for 1000 epochs...\n",
      "Epoch    1/1000, Train Loss: 0.7839, Acc: 50.49% | Val Loss: 0.6481, Acc: 69.66%\n",
      "Epoch  100/1000, Train Loss: 0.5661, Acc: 74.33% | Val Loss: 0.5788, Acc: 71.35%\n",
      "Epoch  200/1000, Train Loss: 0.5487, Acc: 73.77% | Val Loss: 0.5451, Acc: 75.28%\n",
      "Epoch  300/1000, Train Loss: 0.5320, Acc: 73.35% | Val Loss: 0.5467, Acc: 71.91%\n",
      "Epoch  400/1000, Train Loss: 0.5108, Acc: 73.77% | Val Loss: 0.6092, Acc: 70.79%\n",
      "Epoch  500/1000, Train Loss: 0.5061, Acc: 74.61% | Val Loss: 0.5957, Acc: 76.40%\n",
      "Epoch  600/1000, Train Loss: 0.4784, Acc: 77.84% | Val Loss: 0.5019, Acc: 80.34%\n",
      "Epoch  700/1000, Train Loss: 0.4786, Acc: 79.10% | Val Loss: 0.4330, Acc: 78.65%\n",
      "Epoch  800/1000, Train Loss: 0.4626, Acc: 79.94% | Val Loss: 0.5948, Acc: 75.28%\n",
      "Epoch  900/1000, Train Loss: 0.4561, Acc: 79.10% | Val Loss: 0.4307, Acc: 79.21%\n",
      "Epoch 1000/1000, Train Loss: 0.4568, Acc: 79.80% | Val Loss: 0.4402, Acc: 79.21%\n",
      "\n",
      "--- Run Finished --- Final Val Acc: 79.21%\n",
      "\n",
      "--- Testing Activation: LeakyReLU ---\n",
      "--- Run Config ---\n",
      "{'epochs': 1000, 'batch_size': 32, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'LeakyReLU'}\n",
      "\n",
      "Start Training for 1000 epochs...\n",
      "Epoch    1/1000, Train Loss: 1.0302, Acc: 34.64% | Val Loss: 0.9873, Acc: 28.09%\n",
      "Epoch  100/1000, Train Loss: 0.5787, Acc: 71.53% | Val Loss: 0.5802, Acc: 73.60%\n",
      "Epoch  200/1000, Train Loss: 0.5673, Acc: 71.81% | Val Loss: 0.5702, Acc: 71.91%\n",
      "Epoch  300/1000, Train Loss: 0.5508, Acc: 72.23% | Val Loss: 0.5877, Acc: 73.60%\n",
      "Epoch  400/1000, Train Loss: 0.5372, Acc: 72.79% | Val Loss: 0.5546, Acc: 71.91%\n",
      "Epoch  500/1000, Train Loss: 0.5224, Acc: 73.21% | Val Loss: 0.5551, Acc: 72.47%\n",
      "Epoch  600/1000, Train Loss: 0.5070, Acc: 73.35% | Val Loss: 0.5302, Acc: 74.16%\n",
      "Epoch  700/1000, Train Loss: 0.4796, Acc: 77.56% | Val Loss: 0.5382, Acc: 73.60%\n",
      "Epoch  800/1000, Train Loss: 0.4693, Acc: 77.70% | Val Loss: 0.4648, Acc: 77.53%\n",
      "Epoch  900/1000, Train Loss: 0.4517, Acc: 78.26% | Val Loss: 0.4511, Acc: 80.90%\n",
      "Epoch 1000/1000, Train Loss: 0.4621, Acc: 78.82% | Val Loss: 0.5774, Acc: 76.97%\n",
      "\n",
      "--- Run Finished --- Final Val Acc: 76.97%\n",
      "\n",
      "========================= Testing Batch Size: 64 =========================\n",
      "\n",
      "Train size: 713, Validation size: 178\n",
      "\n",
      "--- Testing Activation: Sigmoid ---\n",
      "--- Run Config ---\n",
      "{'epochs': 1000, 'batch_size': 64, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'Sigmoid'}\n",
      "\n",
      "Start Training for 1000 epochs...\n",
      "Epoch    1/1000, Train Loss: 0.6736, Acc: 59.33% | Val Loss: 0.6318, Acc: 70.79%\n",
      "Epoch  100/1000, Train Loss: 0.6677, Acc: 59.33% | Val Loss: 0.6300, Acc: 70.79%\n",
      "Epoch  200/1000, Train Loss: 0.6628, Acc: 59.33% | Val Loss: 0.6271, Acc: 70.79%\n",
      "Epoch  300/1000, Train Loss: 0.6578, Acc: 59.33% | Val Loss: 0.6238, Acc: 70.79%\n",
      "Epoch  400/1000, Train Loss: 0.6530, Acc: 59.33% | Val Loss: 0.6220, Acc: 70.79%\n",
      "Epoch  500/1000, Train Loss: 0.6479, Acc: 59.33% | Val Loss: 0.6200, Acc: 70.79%\n",
      "Epoch  600/1000, Train Loss: 0.6421, Acc: 59.33% | Val Loss: 0.6192, Acc: 70.79%\n",
      "Epoch  700/1000, Train Loss: 0.6366, Acc: 64.38% | Val Loss: 0.6167, Acc: 71.35%\n",
      "Epoch  800/1000, Train Loss: 0.6319, Acc: 65.78% | Val Loss: 0.6134, Acc: 69.10%\n",
      "Epoch  900/1000, Train Loss: 0.6269, Acc: 67.60% | Val Loss: 0.6124, Acc: 68.54%\n",
      "Epoch 1000/1000, Train Loss: 0.6219, Acc: 68.86% | Val Loss: 0.6117, Acc: 69.66%\n",
      "\n",
      "--- Run Finished --- Final Val Acc: 69.66%\n",
      "\n",
      "--- Testing Activation: ReLU ---\n",
      "--- Run Config ---\n",
      "{'epochs': 1000, 'batch_size': 64, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'ReLU'}\n",
      "\n",
      "Start Training for 1000 epochs...\n",
      "Epoch    1/1000, Train Loss: 0.7264, Acc: 38.99% | Val Loss: 0.7226, Acc: 35.96%\n",
      "Epoch  100/1000, Train Loss: 0.6095, Acc: 69.28% | Val Loss: 0.5938, Acc: 71.91%\n",
      "Epoch  200/1000, Train Loss: 0.6016, Acc: 70.69% | Val Loss: 0.5950, Acc: 70.22%\n",
      "Epoch  300/1000, Train Loss: 0.5977, Acc: 70.83% | Val Loss: 0.6009, Acc: 70.79%\n",
      "Epoch  400/1000, Train Loss: 0.5936, Acc: 70.69% | Val Loss: 0.5888, Acc: 70.79%\n",
      "Epoch  500/1000, Train Loss: 0.5908, Acc: 70.27% | Val Loss: 0.5867, Acc: 70.79%\n",
      "Epoch  600/1000, Train Loss: 0.5869, Acc: 70.83% | Val Loss: 0.5579, Acc: 71.35%\n",
      "Epoch  700/1000, Train Loss: 0.5833, Acc: 71.11% | Val Loss: 0.5754, Acc: 71.35%\n",
      "Epoch  800/1000, Train Loss: 0.5796, Acc: 70.83% | Val Loss: 0.5731, Acc: 71.35%\n",
      "Epoch  900/1000, Train Loss: 0.5769, Acc: 71.67% | Val Loss: 0.5810, Acc: 70.22%\n",
      "Epoch 1000/1000, Train Loss: 0.5727, Acc: 71.11% | Val Loss: 0.5629, Acc: 71.35%\n",
      "\n",
      "--- Run Finished --- Final Val Acc: 71.35%\n",
      "\n",
      "--- Testing Activation: ELU ---\n",
      "--- Run Config ---\n",
      "{'epochs': 1000, 'batch_size': 64, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'ELU'}\n",
      "\n",
      "Start Training for 1000 epochs...\n",
      "Epoch    1/1000, Train Loss: 1.0840, Acc: 36.33% | Val Loss: 0.8298, Acc: 29.78%\n",
      "Epoch  100/1000, Train Loss: 0.5953, Acc: 68.72% | Val Loss: 0.5960, Acc: 69.10%\n",
      "Epoch  200/1000, Train Loss: 0.5861, Acc: 70.55% | Val Loss: 0.5937, Acc: 68.54%\n",
      "Epoch  300/1000, Train Loss: 0.5770, Acc: 72.65% | Val Loss: 0.5614, Acc: 74.72%\n",
      "Epoch  400/1000, Train Loss: 0.5719, Acc: 73.21% | Val Loss: 0.5637, Acc: 73.60%\n",
      "Epoch  500/1000, Train Loss: 0.5663, Acc: 73.21% | Val Loss: 0.5569, Acc: 73.60%\n",
      "Epoch  600/1000, Train Loss: 0.5562, Acc: 73.35% | Val Loss: 0.5430, Acc: 74.72%\n",
      "Epoch  700/1000, Train Loss: 0.5529, Acc: 73.77% | Val Loss: 0.5739, Acc: 70.22%\n",
      "Epoch  800/1000, Train Loss: 0.5397, Acc: 75.04% | Val Loss: 0.5464, Acc: 72.47%\n",
      "Epoch  900/1000, Train Loss: 0.5310, Acc: 73.91% | Val Loss: 0.5050, Acc: 76.97%\n",
      "Epoch 1000/1000, Train Loss: 0.5193, Acc: 74.33% | Val Loss: 0.4839, Acc: 77.53%\n",
      "\n",
      "--- Run Finished --- Final Val Acc: 77.53%\n",
      "\n",
      "--- Testing Activation: LeakyReLU ---\n",
      "--- Run Config ---\n",
      "{'epochs': 1000, 'batch_size': 64, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'LeakyReLU'}\n",
      "\n",
      "Start Training for 1000 epochs...\n",
      "Epoch    1/1000, Train Loss: 0.8592, Acc: 41.09% | Val Loss: 0.8829, Acc: 28.09%\n",
      "Epoch  100/1000, Train Loss: 0.5991, Acc: 68.02% | Val Loss: 0.6108, Acc: 66.85%\n",
      "Epoch  200/1000, Train Loss: 0.5956, Acc: 67.88% | Val Loss: 0.5952, Acc: 70.22%\n",
      "Epoch  300/1000, Train Loss: 0.5919, Acc: 69.00% | Val Loss: 0.6001, Acc: 69.10%\n",
      "Epoch  400/1000, Train Loss: 0.5874, Acc: 69.57% | Val Loss: 0.5970, Acc: 69.66%\n",
      "Epoch  500/1000, Train Loss: 0.5842, Acc: 69.42% | Val Loss: 0.5794, Acc: 71.91%\n",
      "Epoch  600/1000, Train Loss: 0.5808, Acc: 70.13% | Val Loss: 0.5857, Acc: 71.35%\n",
      "Epoch  700/1000, Train Loss: 0.5769, Acc: 70.13% | Val Loss: 0.5738, Acc: 71.91%\n",
      "Epoch  800/1000, Train Loss: 0.5729, Acc: 69.85% | Val Loss: 0.5719, Acc: 71.91%\n",
      "Epoch  900/1000, Train Loss: 0.5706, Acc: 69.14% | Val Loss: 0.5838, Acc: 71.35%\n",
      "Epoch 1000/1000, Train Loss: 0.5661, Acc: 69.00% | Val Loss: 0.5709, Acc: 71.35%\n",
      "\n",
      "--- Run Finished --- Final Val Acc: 71.35%\n",
      "\n",
      "========================= Testing Batch Size: 128 =========================\n",
      "\n",
      "Train size: 713, Validation size: 178\n",
      "\n",
      "--- Testing Activation: Sigmoid ---\n",
      "--- Run Config ---\n",
      "{'epochs': 1000, 'batch_size': 128, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'Sigmoid'}\n",
      "\n",
      "Start Training for 1000 epochs...\n",
      "Epoch    1/1000, Train Loss: 0.8306, Acc: 40.67% | Val Loss: 0.9114, Acc: 29.21%\n",
      "Epoch  100/1000, Train Loss: 0.6670, Acc: 59.33% | Val Loss: 0.6379, Acc: 70.79%\n",
      "Epoch  200/1000, Train Loss: 0.6635, Acc: 59.33% | Val Loss: 0.6240, Acc: 70.79%\n",
      "Epoch  300/1000, Train Loss: 0.6614, Acc: 59.33% | Val Loss: 0.6221, Acc: 70.79%\n",
      "Epoch  400/1000, Train Loss: 0.6593, Acc: 59.33% | Val Loss: 0.6208, Acc: 70.79%\n",
      "Epoch  500/1000, Train Loss: 0.6570, Acc: 59.33% | Val Loss: 0.6201, Acc: 70.79%\n",
      "Epoch  600/1000, Train Loss: 0.6547, Acc: 59.33% | Val Loss: 0.6191, Acc: 70.79%\n",
      "Epoch  700/1000, Train Loss: 0.6523, Acc: 59.33% | Val Loss: 0.6182, Acc: 70.79%\n",
      "Epoch  800/1000, Train Loss: 0.6499, Acc: 59.33% | Val Loss: 0.6171, Acc: 70.79%\n",
      "Epoch  900/1000, Train Loss: 0.6475, Acc: 59.33% | Val Loss: 0.6158, Acc: 70.79%\n",
      "Epoch 1000/1000, Train Loss: 0.6451, Acc: 59.33% | Val Loss: 0.6150, Acc: 70.79%\n",
      "\n",
      "--- Run Finished --- Final Val Acc: 70.79%\n",
      "\n",
      "--- Testing Activation: ReLU ---\n",
      "--- Run Config ---\n",
      "{'epochs': 1000, 'batch_size': 128, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'ReLU'}\n",
      "\n",
      "Start Training for 1000 epochs...\n",
      "Epoch    1/1000, Train Loss: 0.6790, Acc: 60.73% | Val Loss: 0.6030, Acc: 70.79%\n",
      "Epoch  100/1000, Train Loss: 0.6034, Acc: 68.30% | Val Loss: 0.5948, Acc: 68.54%\n",
      "Epoch  200/1000, Train Loss: 0.5987, Acc: 68.72% | Val Loss: 0.5999, Acc: 66.85%\n",
      "Epoch  300/1000, Train Loss: 0.5963, Acc: 69.14% | Val Loss: 0.5871, Acc: 70.79%\n",
      "Epoch  400/1000, Train Loss: 0.5932, Acc: 69.42% | Val Loss: 0.5815, Acc: 71.35%\n",
      "Epoch  500/1000, Train Loss: 0.5901, Acc: 69.71% | Val Loss: 0.5901, Acc: 68.54%\n",
      "Epoch  600/1000, Train Loss: 0.5873, Acc: 69.99% | Val Loss: 0.5883, Acc: 69.10%\n",
      "Epoch  700/1000, Train Loss: 0.5858, Acc: 69.57% | Val Loss: 0.5862, Acc: 68.54%\n",
      "Epoch  800/1000, Train Loss: 0.5821, Acc: 70.13% | Val Loss: 0.5810, Acc: 69.10%\n",
      "Epoch  900/1000, Train Loss: 0.5816, Acc: 69.71% | Val Loss: 0.5769, Acc: 69.10%\n",
      "Epoch 1000/1000, Train Loss: 0.5766, Acc: 70.69% | Val Loss: 0.5608, Acc: 70.79%\n",
      "\n",
      "--- Run Finished --- Final Val Acc: 70.79%\n",
      "\n",
      "--- Testing Activation: ELU ---\n",
      "--- Run Config ---\n",
      "{'epochs': 1000, 'batch_size': 128, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'ELU'}\n",
      "\n",
      "Start Training for 1000 epochs...\n",
      "Epoch    1/1000, Train Loss: 0.7889, Acc: 37.73% | Val Loss: 0.7697, Acc: 29.78%\n",
      "Epoch  100/1000, Train Loss: 0.5878, Acc: 70.97% | Val Loss: 0.5935, Acc: 70.22%\n",
      "Epoch  200/1000, Train Loss: 0.5808, Acc: 73.35% | Val Loss: 0.5812, Acc: 71.91%\n",
      "Epoch  300/1000, Train Loss: 0.5762, Acc: 73.77% | Val Loss: 0.5748, Acc: 71.35%\n",
      "Epoch  400/1000, Train Loss: 0.5717, Acc: 73.63% | Val Loss: 0.5705, Acc: 71.35%\n",
      "Epoch  500/1000, Train Loss: 0.5683, Acc: 73.77% | Val Loss: 0.5673, Acc: 71.91%\n",
      "Epoch  600/1000, Train Loss: 0.5639, Acc: 73.91% | Val Loss: 0.5600, Acc: 71.91%\n",
      "Epoch  700/1000, Train Loss: 0.5599, Acc: 73.35% | Val Loss: 0.5591, Acc: 71.91%\n",
      "Epoch  800/1000, Train Loss: 0.5558, Acc: 73.91% | Val Loss: 0.5552, Acc: 71.91%\n",
      "Epoch  900/1000, Train Loss: 0.5520, Acc: 73.77% | Val Loss: 0.5458, Acc: 71.91%\n",
      "Epoch 1000/1000, Train Loss: 0.5480, Acc: 74.19% | Val Loss: 0.5414, Acc: 72.47%\n",
      "\n",
      "--- Run Finished --- Final Val Acc: 72.47%\n",
      "\n",
      "--- Testing Activation: LeakyReLU ---\n",
      "--- Run Config ---\n",
      "{'epochs': 1000, 'batch_size': 128, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'LeakyReLU'}\n",
      "\n",
      "Start Training for 1000 epochs...\n",
      "Epoch    1/1000, Train Loss: 1.6794, Acc: 59.33% | Val Loss: 0.7952, Acc: 70.79%\n",
      "Epoch  100/1000, Train Loss: 0.5979, Acc: 71.95% | Val Loss: 0.6040, Acc: 69.10%\n",
      "Epoch  200/1000, Train Loss: 0.5929, Acc: 71.95% | Val Loss: 0.5968, Acc: 72.47%\n",
      "Epoch  300/1000, Train Loss: 0.5892, Acc: 71.81% | Val Loss: 0.5942, Acc: 71.35%\n",
      "Epoch  400/1000, Train Loss: 0.5869, Acc: 71.53% | Val Loss: 0.5924, Acc: 71.35%\n",
      "Epoch  500/1000, Train Loss: 0.5837, Acc: 71.67% | Val Loss: 0.5881, Acc: 71.35%\n",
      "Epoch  600/1000, Train Loss: 0.5813, Acc: 71.95% | Val Loss: 0.5849, Acc: 71.91%\n",
      "Epoch  700/1000, Train Loss: 0.5789, Acc: 72.23% | Val Loss: 0.5816, Acc: 71.91%\n",
      "Epoch  800/1000, Train Loss: 0.5764, Acc: 72.23% | Val Loss: 0.5748, Acc: 71.91%\n",
      "Epoch  900/1000, Train Loss: 0.5747, Acc: 71.95% | Val Loss: 0.5770, Acc: 71.35%\n",
      "Epoch 1000/1000, Train Loss: 0.5725, Acc: 71.81% | Val Loss: 0.5709, Acc: 71.91%\n",
      "\n",
      "--- Run Finished --- Final Val Acc: 71.91%\n",
      "\n",
      "============================================================\n",
      " Hyperparameter Tuning Finished (Config Only) \n",
      "============================================================\n",
      "Overall Best Final Val Acc: 83.71%\n",
      "Best Config Found: {'epochs': 1000, 'batch_size': 16, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'ReLU'}\n",
      "Best hyperparameters saved to best_hyperparameters.json\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- ëª…ë ¹ì¤„ ì¸ì ì²˜ë¦¬ ë° ì‹¤í–‰ ---\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Find best hyperparameters (config only) for Titanic.\")\n",
    "    parser.add_argument(\"--wandb\", action=argparse.BooleanOptionalAction, default=False, help=\"Enable WandB logging\")\n",
    "    parser.add_argument(\"-e\", \"--epochs\", type=int, default=1000, help=\"Epochs per combination (default: 1000)\")\n",
    "\n",
    "    # Jupyter í™˜ê²½ ê°ì§€ ë° ì²˜ë¦¬\n",
    "    if 'ipykernel' in sys.modules:\n",
    "        print(\"Running in interactive mode. Using default args: epochs=1000, wandb=False\")\n",
    "        args = argparse.Namespace(wandb=False, epochs=1000) # ê¸°ë³¸ê°’ ì„¤ì •\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    find_hyperparameters(args) # ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48261013-ce45-4903-ab77-0003a7869364",
   "metadata": {},
   "source": [
    "![í•™ìŠµ2](https://github.com/O-E2/deep_learning/blob/ee3a2d4c8d369f2970393fc38fefdefbeedc8e62/dl_parameters_wandb.png)\n",
    "\n",
    "Wandb URL\n",
    "https://wandb.ai/cyun0407-korea-university-of-technology-and-education/titanic_hyperparameter_tuning/workspace?nw=nwusercyun0407"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca0c956-a814-452d-ae3f-a1ca7d93608e",
   "metadata": {},
   "source": [
    "# [ìš”êµ¬ì‚¬í•­ 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "05c8417e-85dd-4501-9fdc-5c81e21f23bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, sys, argparse, copy\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim # optimì€ ì¬í•™ìŠµ ì‹œ í•„ìš”\n",
    "from torch.utils.data import Dataset, DataLoader, random_split # random_splitì€ ì¬í•™ìŠµ ì‹œ í•„ìš”\n",
    "from sklearn.preprocessing import LabelEncoder # ì „ì²˜ë¦¬ í•¨ìˆ˜ì— í•„ìš”\n",
    "from pathlib import Path\n",
    "\n",
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, X, y): self.X = torch.FloatTensor(X); self.y = torch.LongTensor(y)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx): return {'input': self.X[idx], 'target': self.y[idx]}\n",
    "    def __str__(self): return f\"Data Size: {len(self.X)}, Input Shape: {self.X.shape}, Target Shape: {self.y.shape}\"\n",
    "\n",
    "class TitanicTestDataset(Dataset):\n",
    "    def __init__(self, X): self.X = torch.FloatTensor(X)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx): return {'input': self.X[idx]}\n",
    "    def __str__(self): return f\"Data Size: {len(self.X)}, Input Shape: {self.X.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "363e62f7-413d-4a46-8158-c3717471bee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... ë°ì´í„° ì „ì²˜ë¦¬ ë³´ì¡° í•¨ìˆ˜ get_preprocessed_dataset_1 ~ _6 ì •ì˜\n",
    "def get_preprocessed_dataset_1(all_df): # ... (ë™ì¼) ...\n",
    "    Fare_mean = all_df[[\"Pclass\", \"Fare\"]].groupby(\"Pclass\").mean().reset_index(); Fare_mean.columns = [\"Pclass\", \"Fare_mean\"]\n",
    "    all_df = pd.merge(all_df, Fare_mean, on=\"Pclass\", how=\"left\"); all_df.loc[all_df[\"Fare\"].isnull(), \"Fare\"] = all_df[\"Fare_mean\"]\n",
    "    return all_df.drop(columns=[\"Fare_mean\"])\n",
    "# get_preprocessed_dataset_2 ~ _6 ì •ì˜\n",
    "def get_preprocessed_dataset_2(all_df):\n",
    "    name_df = all_df[\"Name\"].str.split(\"[,.]\", n=2, expand=True)\n",
    "    if name_df.shape[1] == 3:\n",
    "        name_df.columns = [\"family_name\", \"title\", \"name\"]; name_df[\"family_name\"] = name_df[\"family_name\"].str.strip(); name_df[\"title\"] = name_df[\"title\"].str.strip(); name_df[\"name\"] = name_df[\"name\"].str.strip()\n",
    "        all_df = pd.concat([all_df, name_df], axis=1)\n",
    "    else: all_df['title'] = 'unknown'\n",
    "    return all_df\n",
    "def get_preprocessed_dataset_3(all_df):\n",
    "    if 'title' not in all_df.columns:\n",
    "        if all_df['Age'].isnull().any(): all_df['Age'] = all_df['Age'].fillna(all_df['Age'].median())\n",
    "        return all_df\n",
    "    title_age_median = all_df[[\"title\", \"Age\"]].groupby(\"title\").median().round().reset_index(); title_age_median.columns = [\"title\", \"title_age_median\"]\n",
    "    all_df = pd.merge(all_df, title_age_median, on=\"title\", how=\"left\"); all_df.loc[all_df[\"Age\"].isnull(), \"Age\"] = all_df[\"title_age_median\"]\n",
    "    if \"title_age_median\" in all_df.columns: all_df = all_df.drop([\"title_age_median\"], axis=1)\n",
    "    return all_df\n",
    "def get_preprocessed_dataset_4(all_df):\n",
    "    all_df[\"family_num\"] = all_df[\"Parch\"] + all_df[\"SibSp\"]; all_df.loc[all_df[\"family_num\"] == 0, \"alone\"] = 1; all_df[\"alone\"] = all_df[\"alone\"].fillna(0).astype(float)\n",
    "    cols_to_drop = [\"PassengerId\", \"Name\", \"family_name\", \"name\", \"Ticket\", \"Cabin\"]; existing_cols_to_drop = [col for col in cols_to_drop if col in all_df.columns]\n",
    "    return all_df.drop(existing_cols_to_drop, axis=1)\n",
    "def get_preprocessed_dataset_5(all_df):\n",
    "    if 'title' in all_df.columns: all_df.loc[~((all_df[\"title\"] == \"Mr\") | (all_df[\"title\"] == \"Miss\") | (all_df[\"title\"] == \"Mrs\") | (all_df[\"title\"] == \"Master\")), \"title\"] = \"other\"\n",
    "    all_df[\"Embarked\"] = all_df[\"Embarked\"].fillna(\"missing\")\n",
    "    return all_df\n",
    "def get_preprocessed_dataset_6(all_df):\n",
    "    category_features = all_df.select_dtypes(include=['object']).columns\n",
    "    for cat_feat in category_features:\n",
    "        le = LabelEncoder(); valid_indices = all_df[cat_feat].notna()\n",
    "        if valid_indices.any():\n",
    "            all_df.loc[valid_indices, cat_feat] = le.fit_transform(all_df.loc[valid_indices, cat_feat])\n",
    "            try: all_df[cat_feat] = pd.to_numeric(all_df[cat_feat])\n",
    "            except ValueError: pass\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "48a73b2f-02d4-4696-9f51-517520e7981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_data_raw():\n",
    "    \"\"\"CSV ë¡œë“œ, ì „ì²˜ë¦¬, ìµœì¢… ë°ì´í„° ë°°ì—´ ë°˜í™˜ (í…ŒìŠ¤íŠ¸ ID í¬í•¨)\"\"\"\n",
    "    try: CURRENT_FILE_PATH = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError: CURRENT_FILE_PATH = os.getcwd()\n",
    "    train_data_path = os.path.join(CURRENT_FILE_PATH, \"train.csv\")\n",
    "    test_data_path = os.path.join(CURRENT_FILE_PATH, \"test.csv\")\n",
    "    try:\n",
    "        train_df = pd.read_csv(train_data_path)\n",
    "        test_df = pd.read_csv(test_data_path)\n",
    "    except FileNotFoundError as e: print(f\"Error: {e}\"); sys.exit(1)\n",
    "\n",
    "    test_passenger_ids = test_df['PassengerId'] # í…ŒìŠ¤íŠ¸ ìŠ¹ê° ID ì €ì¥\n",
    "    all_df = pd.concat([train_df, test_df], sort=False)\n",
    "\n",
    "    # ì „ì²˜ë¦¬ ë‹¨ê³„ ì ìš©\n",
    "    all_df = get_preprocessed_dataset_1(all_df); all_df = get_preprocessed_dataset_2(all_df)\n",
    "    all_df = get_preprocessed_dataset_3(all_df); all_df = get_preprocessed_dataset_4(all_df)\n",
    "    all_df = get_preprocessed_dataset_5(all_df); all_df = get_preprocessed_dataset_6(all_df)\n",
    "\n",
    "    # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„ë¦¬\n",
    "    train_X_df = all_df[~all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1)\n",
    "    train_y = train_df[\"Survived\"]\n",
    "    test_X_df = all_df[all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1)\n",
    "\n",
    "    # ìµœì¢… ë°ì´í„° í™•ì¸ ë° NaN ì²˜ë¦¬ (ë¶„ë¦¬ëœ ë°ì´í„°í”„ë ˆì„ì— ê°ê° ì ìš©)\n",
    "    print(\"\\n--- Final Data Check & Fill NaNs ---\")\n",
    "    columns_to_check = train_X_df.columns\n",
    "    for df in [train_X_df, test_X_df]:\n",
    "        df_name = \"Train\" if df is train_X_df else \"Test\"\n",
    "        for col in columns_to_check:\n",
    "            if df[col].dtype == 'object':\n",
    "                try: df[col] = pd.to_numeric(df[col])\n",
    "                except ValueError: df[col] = 0 # ë³€í™˜ ë¶ˆê°€ì‹œ 0ìœ¼ë¡œ ì±„ì›€\n",
    "            if df[col].isnull().any(): df[col] = df[col].fillna(0) # NaN 0ìœ¼ë¡œ ì±„ì›€\n",
    "\n",
    "    # ìµœì¢… ì¸ë±ìŠ¤ ë¦¬ì…‹ ë° Numpy ë°°ì—´ ë³€í™˜\n",
    "    train_X = train_X_df.reset_index(drop=True).values\n",
    "    test_X = test_X_df.reset_index(drop=True).values\n",
    "    train_y = train_y.values # Numpy ë°°ì—´ë¡œ ë³€í™˜\n",
    "\n",
    "    print(\"Data preprocessing finished for submission generation.\")\n",
    "    return train_X, train_y, test_X, test_passenger_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6c0a517c-3979-4a38-8208-ecd01771701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DataLoader ìƒì„± í•¨ìˆ˜ (get_dataloaders) ---\n",
    "def get_dataloaders(train_X, train_y, test_X, batch_size_config):\n",
    "    \"\"\"í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ DataLoader ëª¨ë‘ ìƒì„±\"\"\"\n",
    "    dataset = TitanicDataset(train_X, train_y)\n",
    "    generator = torch.Generator().manual_seed(42) # ì‹œë“œ ê³ ì •\n",
    "    train_dataset, validation_dataset = random_split(dataset, [0.8, 0.2], generator=generator)\n",
    "    test_dataset = TitanicTestDataset(test_X)\n",
    "\n",
    "    print(f\"\\nTrain size: {len(train_dataset)}, Validation size: {len(validation_dataset)}, Test size: {len(test_dataset)}\")\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size_config, shuffle=True)\n",
    "    validation_loader = DataLoader(dataset=validation_dataset, batch_size=len(validation_dataset))\n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë” ìƒì„± (ë¹„ì–´ìˆì§€ ì•Šì„ ë•Œë§Œ)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset)) if len(test_dataset) > 0 else None\n",
    "\n",
    "    return train_loader, validation_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7e98f35b-459d-429c-8c94-e9811465ab99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ì‹ ê²½ë§ ëª¨ë¸ ì •ì˜ (MyModel) ---\n",
    "class MyModel(nn.Module):\n",
    "    \"\"\"í™œì„±í™” í•¨ìˆ˜ì™€ ì€ë‹‰ì¸µ í¬ê¸°ë¥¼ ì¸ìë¡œ ë°›ëŠ” MLP ëª¨ë¸\"\"\"\n",
    "    def __init__(self, n_input, n_output, activation_fn_class, n_hidden1=20, n_hidden2=20):\n",
    "        super().__init__(); activation = activation_fn_class()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_input, n_hidden1), activation,\n",
    "            nn.Linear(n_hidden1, n_hidden2), activation,\n",
    "            nn.Linear(n_hidden2, n_output)\n",
    "        )\n",
    "    def forward(self, x): return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "af291b7c-94cf-4f16-bde0-ad33f7af4fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ëª¨ë¸ ë° ì˜µí‹°ë§ˆì´ì € ìƒì„± í•¨ìˆ˜ (get_model_and_optimizer) ---\n",
    "def get_model_and_optimizer(activation_fn_class, learning_rate, n_hidden1=20, n_hidden2=20):\n",
    "    \"\"\"ëª¨ë¸ ê°ì²´ì™€ SGD ì˜µí‹°ë§ˆì´ì € ìƒì„±\"\"\"\n",
    "    model = MyModel(10, 2, activation_fn_class, n_hidden1, n_hidden2) # ì…ë ¥ 10ê°œ, ì¶œë ¥ 2ê°œ ê³ ì •\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b0ae0167-8ade-44c5-ba7b-16519d5882bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ëª¨ë¸ í•™ìŠµ ë° ê²€ì¦ ë£¨í”„ ---\n",
    "def training_loop_with_early_stopping(model, optimizer, train_data_loader, validation_data_loader, n_epochs, loss_fn):\n",
    "    \"\"\"ëª¨ë¸ í•™ìŠµ/ê²€ì¦ ìˆ˜í–‰ ë° 'ìµœê³  ì„±ëŠ¥' ëª¨ë¸ ìƒíƒœ ë°˜í™˜ (ì¡°ê¸° ì¢…ë£Œ ë¡œì§)\"\"\"\n",
    "    best_validation_accuracy = -1.0\n",
    "    best_model_state = None\n",
    "    best_epoch = 0\n",
    "\n",
    "    print(f\"\\nStart Final Training for {n_epochs} epochs (with early stopping logic)...\")\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # --- í•™ìŠµ ---\n",
    "        model.train(); loss_train_epoch = 0.0; correct_train_epoch = 0; total_train_samples = 0\n",
    "        for batch in train_data_loader:\n",
    "            input_data = batch['input']; target = batch['target']; optimizer.zero_grad(); output_train = model(input_data); loss = loss_fn(output_train, target); loss.backward(); optimizer.step()\n",
    "            loss_train_epoch += loss.item() * input_data.size(0); _, predicted = torch.max(output_train.data, 1); total_train_samples += target.size(0); correct_train_epoch += (predicted == target).sum().item()\n",
    "        avg_loss_train = loss_train_epoch / total_train_samples if total_train_samples else 0\n",
    "        train_accuracy = 100 * correct_train_epoch / total_train_samples if total_train_samples else 0\n",
    "\n",
    "        # --- ê²€ì¦ ---\n",
    "        model.eval(); loss_val_epoch = 0.0; correct_val_epoch = 0; total_val_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in validation_data_loader:\n",
    "                input_data = batch['input']; target = batch['target']; output_validation = model(input_data); loss = loss_fn(output_validation, target); loss_val_epoch += loss.item() * input_data.size(0)\n",
    "                _, predicted = torch.max(output_validation.data, 1); total_val_samples += target.size(0); correct_val_epoch += (predicted == target).sum().item()\n",
    "        avg_loss_validation = loss_val_epoch / total_val_samples if total_val_samples else 0\n",
    "        validation_accuracy = 100 * correct_val_epoch / total_val_samples if total_val_samples else 0\n",
    "\n",
    "        # ìµœê³  ì„±ëŠ¥ ê°±ì‹  í™•ì¸,ì¡°ê¸° ì¢…ë£Œ ë¡œì§\n",
    "        if validation_accuracy > best_validation_accuracy:\n",
    "            best_validation_accuracy = validation_accuracy\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = epoch\n",
    "            print(f\" New best validation accuracy: {best_validation_accuracy:.2f}% at epoch {epoch}\")\n",
    "\n",
    "        # ì£¼ê¸°ì  ì¶œë ¥\n",
    "        if epoch % 100 == 0 or epoch == 1:\n",
    "            print(f\"Epoch {epoch:>{len(str(n_epochs))}}/{n_epochs}, Train Loss: {avg_loss_train:.4f}, Acc: {train_accuracy:.2f}% | Val Loss: {avg_loss_validation:.4f}, Acc: {validation_accuracy:.2f}%{' *Best*' if epoch == best_epoch else ''}\")\n",
    "\n",
    "    print(f\"\\n--- Final Training Summary --- Best Val Acc: {best_validation_accuracy:.2f}% achieved at epoch {best_epoch}\")\n",
    "    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ìƒíƒœì™€ ì—í¬í¬ ë°˜í™˜\n",
    "    return best_validation_accuracy, best_model_state, best_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1110e837-5d69-443a-bff3-02b10c7aee6b",
   "metadata": {},
   "source": [
    "ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ì„ ì¬í•™ìŠµì‹œí‚¤ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. í•™ìŠµ ì¤‘ ë§¤ ì—í¬í¬ë§ˆë‹¤ ê²€ì¦ ë°ì´í„° ì„±ëŠ¥ì„ í™•ì¸í•˜ê³ , ìµœê³  ì„±ëŠ¥ì„ ë³´ì¸ ì‹œì ì˜ ëª¨ë¸ ìƒíƒœì™€ ì—í¬í¬ ë²ˆí˜¸ë¥¼ ê¸°ë¡í•˜ì—¬ ìµœì¢…ì ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4dfaedad-038d-4fc9-ba30-2330f43071a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ (test_model) ---\n",
    "# (find_best_hyperparameters.py ì™€ ë™ì¼í•˜ê²Œ ì •ì˜)\n",
    "def test_model(model, test_data_loader):\n",
    "    \"\"\"í•™ìŠµëœ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ìˆ˜í–‰\"\"\"\n",
    "    model.eval(); predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_data_loader: input_data = batch['input']; output = model(input_data); _, predicted = torch.max(output.data, 1); predictions.extend(predicted.cpu().numpy())\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6b07bae1-78c8-411d-8943-36a641af838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Submission íŒŒì¼ ìƒì„± í•¨ìˆ˜ (create_submission) ---\n",
    "# (find_best_hyperparameters.py ì™€ ë™ì¼í•˜ê²Œ ì •ì˜)\n",
    "def create_submission(predictions, passenger_ids, output_file=\"submission.csv\"):\n",
    "    \"\"\"ì˜ˆì¸¡ ê²°ê³¼ì™€ ìŠ¹ê° IDë¡œ submission CSV íŒŒì¼ ìƒì„±\"\"\"\n",
    "    if len(predictions) != len(passenger_ids): print(f\"Error: Prediction count ({len(predictions)}) != Passenger ID count ({len(passenger_ids)})\"); return\n",
    "    submission_df = pd.DataFrame({\"PassengerId\": passenger_ids, \"Survived\": predictions})\n",
    "    try: submission_df.to_csv(output_file, index=False); print(f\"Submission file saved: {output_file}\")\n",
    "    except Exception as e: print(f\"Error saving submission file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bfc7e5e6-c574-4536-84d3-9adde74e523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ìµœì  ëª¨ë¸ ë¡œë“œ ë° ì˜ˆì¸¡) ---\n",
    "def generate_submission_main(config_path=\"best_hyperparameters.json\", retrain_epochs=None):\n",
    "    \"\"\"ìµœì  ì„¤ì • ë¡œë“œ, í•´ë‹¹ ì„¤ì •ìœ¼ë¡œ ì¬í•™ìŠµ(ì¡°ê¸°ì¢…ë£Œ ì ìš©), ì˜ˆì¸¡, submission íŒŒì¼ ìƒì„±\"\"\"\n",
    "    # ìµœì  ì„¤ì • ë¡œë“œ\n",
    "    try:\n",
    "        with open(config_path, 'r') as f: best_config = json.load(f)\n",
    "    except FileNotFoundError: print(f\"Error: Config file '{config_path}' not found.\"); sys.exit(1)\n",
    "    except json.JSONDecodeError: print(f\"Error: Could not decode JSON from '{config_path}'.\"); sys.exit(1)\n",
    "\n",
    "    print(\"--- Loaded Best Config ---\"); print(best_config)\n",
    "\n",
    "    # ì„¤ì •ê°’ ì¶”ì¶œ\n",
    "    best_activation_name = best_config.get('activation_function')\n",
    "    best_batch_size = best_config.get('batch_size')\n",
    "    learning_rate = best_config.get('learning_rate', 1e-3)\n",
    "    n_hidden1 = best_config.get('n_hidden_unit_list', [20, 20])[0]\n",
    "    n_hidden2 = best_config.get('n_hidden_unit_list', [20, 20])[1]\n",
    "    # ì¬í•™ìŠµ ì—í¬í¬ ìˆ˜ ê²°ì • (ì¸ì ìš°ì„  > config ê°’ > ê¸°ë³¸ê°’ 1000)\n",
    "    epochs_to_train = retrain_epochs if retrain_epochs is not None else best_config.get('epochs', 1000)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # í™œì„±í™” í•¨ìˆ˜ í´ë˜ìŠ¤ ê°€ì ¸ì˜¤ê¸°\n",
    "    activation_functions_map = {\"Sigmoid\": nn.Sigmoid, \"ReLU\": nn.ReLU, \"ELU\": nn.ELU, \"LeakyReLU\": nn.LeakyReLU}\n",
    "    if not best_activation_name or best_activation_name not in activation_functions_map:\n",
    "        print(f\"Error: Invalid activation function '{best_activation_name}' in config.\"); sys.exit(1)\n",
    "    best_activation_class = activation_functions_map[best_activation_name]\n",
    "\n",
    "    print(\"\\nPreprocessing data for final training and submission...\")\n",
    "    train_X, train_y, test_X, test_passenger_ids = get_preprocessed_data_raw()\n",
    "\n",
    "    if not best_batch_size: print(\"Error: 'batch_size' not found in config.\"); sys.exit(1)\n",
    "    train_loader, validation_loader, test_loader = get_dataloaders(train_X, train_y, test_X, best_batch_size)\n",
    "    if test_loader is None: print(\"Error: Test data is empty, cannot generate submission.\"); sys.exit(1)\n",
    "\n",
    "    model, optimizer = get_model_and_optimizer(best_activation_class, learning_rate, n_hidden1, n_hidden2)\n",
    "\n",
    "    # === ìµœì¢… ëª¨ë¸ ì¬í•™ìŠµ (ì¡°ê¸° ì¢…ë£Œ ë¡œì§ ì ìš©) ===\n",
    "    final_best_accuracy, final_best_model_state, final_best_epoch = training_loop_with_early_stopping(\n",
    "        model, optimizer, train_loader, validation_loader, epochs_to_train, loss_fn\n",
    "    )\n",
    "\n",
    "    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ìƒíƒœ ë¡œë“œ\n",
    "    if final_best_model_state:\n",
    "        model.load_state_dict(final_best_model_state)\n",
    "        print(f\"\\nLoaded model state from epoch {final_best_epoch} with validation accuracy {final_best_accuracy:.2f}%\")\n",
    "\n",
    "        # ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "        print(\"Generating predictions with the best epoch model...\")\n",
    "        test_predictions = test_model(model, test_loader)\n",
    "\n",
    "        # Submission íŒŒì¼ ìƒì„±\n",
    "        submission_filename = f\"submission_final_{best_activation_name}_bs{best_batch_size}_epoch{final_best_epoch}.csv\"\n",
    "        create_submission(test_predictions, test_passenger_ids, output_file=submission_filename)\n",
    "    else:\n",
    "        print(\"Final training did not produce a best model state. Cannot generate submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8c3355-f59c-4228-913d-b04eb6b9d761",
   "metadata": {},
   "source": [
    "* best_hyperparameters.json íŒŒì¼ì„ ì½ì–´ ìµœì  ì¡°í•© ì„¤ì •ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "\n",
    "* ì „ì²´ ë°ì´í„°ë¥¼ ë‹¤ì‹œ ë¡œë“œí•˜ê³  ìµœì  ë°°ì¹˜ í¬ê¸°ë¡œ DataLoaderë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "* ìµœì  í™œì„±í™” í•¨ìˆ˜ë¡œ ëª¨ë¸ê³¼ ì˜µí‹°ë§ˆì´ì €ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "* training_loop_with_early_stopping í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ëª¨ë¸ì„ ì¬í•™ìŠµì‹œí‚¤ê³ , ì´ ê³¼ì •ì—ì„œ ê²€ì¦ ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ì•˜ë˜ ì‹œì (ì¡°ê¸° ì¢…ë£Œ ì‹œì )ì˜ ëª¨ë¸ ìƒíƒœì™€ ì—í¬í¬ ë²ˆí˜¸ë¥¼ ì–»ìŠµë‹ˆë‹¤.\n",
    "\n",
    "* ì–»ì–´ì§„ ìµœì  ì‹œì ì˜ ëª¨ë¸ ìƒíƒœë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "\n",
    "* test_model í•¨ìˆ˜ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "* create_submission í•¨ìˆ˜ë¡œ ìµœì¢… ì œì¶œ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤ (íŒŒì¼ ì´ë¦„ì— ìµœì  ì¡°í•© ë° ì—í¬í¬ í¬í•¨)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fdcff00d-75b7-4059-821b-12b1e508c14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in interactive mode. Using default config path and epochs from config (or 1000).\n",
      "--- Loaded Best Config ---\n",
      "{'epochs': 1000, 'batch_size': 16, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'ReLU'}\n",
      "\n",
      "Preprocessing data for final training and submission...\n",
      "\n",
      "--- Final Data Check & Fill NaNs ---\n",
      "Data preprocessing finished for submission generation.\n",
      "\n",
      "Train size: 713, Validation size: 178, Test size: 418\n",
      "\n",
      "Start Final Training for 1000 epochs (with early stopping logic)...\n",
      " New best validation accuracy: 73.03% at epoch 1\n",
      "Epoch    1/1000, Train Loss: 0.6796, Acc: 61.71% | Val Loss: 0.5822, Acc: 73.03% *Best*\n",
      " New best validation accuracy: 73.60% at epoch 2\n",
      "Epoch  100/1000, Train Loss: 0.5821, Acc: 70.83% | Val Loss: 0.5581, Acc: 72.47%\n",
      " New best validation accuracy: 74.16% at epoch 198\n",
      "Epoch  200/1000, Train Loss: 0.5664, Acc: 71.67% | Val Loss: 0.5488, Acc: 71.35%\n",
      " New best validation accuracy: 75.28% at epoch 238\n",
      " New best validation accuracy: 76.40% at epoch 295\n",
      " New best validation accuracy: 76.97% at epoch 297\n",
      "Epoch  300/1000, Train Loss: 0.5509, Acc: 71.39% | Val Loss: 0.5355, Acc: 71.91%\n",
      " New best validation accuracy: 78.09% at epoch 359\n",
      " New best validation accuracy: 78.65% at epoch 375\n",
      "Epoch  400/1000, Train Loss: 0.5164, Acc: 74.89% | Val Loss: 0.5220, Acc: 74.72%\n",
      " New best validation accuracy: 79.21% at epoch 408\n",
      " New best validation accuracy: 79.78% at epoch 423\n",
      " New best validation accuracy: 80.34% at epoch 471\n",
      " New best validation accuracy: 80.90% at epoch 479\n",
      " New best validation accuracy: 82.02% at epoch 485\n",
      " New best validation accuracy: 84.27% at epoch 491\n",
      "Epoch  500/1000, Train Loss: 0.4872, Acc: 79.66% | Val Loss: 0.4902, Acc: 79.21%\n",
      " New best validation accuracy: 85.96% at epoch 513\n",
      " New best validation accuracy: 86.52% at epoch 539\n",
      "Epoch  600/1000, Train Loss: 0.4755, Acc: 78.68% | Val Loss: 0.5683, Acc: 75.84%\n",
      " New best validation accuracy: 87.08% at epoch 610\n",
      "Epoch  700/1000, Train Loss: 0.4557, Acc: 80.93% | Val Loss: 0.4164, Acc: 82.02%\n",
      "Epoch  800/1000, Train Loss: 0.4503, Acc: 79.80% | Val Loss: 0.4245, Acc: 83.15%\n",
      " New best validation accuracy: 87.64% at epoch 808\n",
      "Epoch  900/1000, Train Loss: 0.4456, Acc: 80.22% | Val Loss: 0.4110, Acc: 83.71%\n",
      "Epoch 1000/1000, Train Loss: 0.4528, Acc: 79.52% | Val Loss: 0.3969, Acc: 87.08%\n",
      "\n",
      "--- Final Training Summary --- Best Val Acc: 87.64% achieved at epoch 808\n",
      "\n",
      "Loaded model state from epoch 808 with validation accuracy 87.64%\n",
      "Generating predictions with the best epoch model...\n",
      "Submission file saved: submission_final_ReLU_bs16_epoch808.csv\n"
     ]
    }
   ],
   "source": [
    "# --- ëª…ë ¹ì¤„ ì¸ì ì²˜ë¦¬ ë° ì‹¤í–‰ ---\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Retrain the best model with early stopping and generate submission.\")\n",
    "    parser.add_argument(\"--config\", default=\"best_hyperparameters.json\", help=\"Path to best hyperparameters JSON\")\n",
    "    # ì¬í•™ìŠµ ì—í¬í¬ ìˆ˜ë¥¼ ì¸ìë¡œ ë°›ì„ ìˆ˜ ìˆë„ë¡ ì¶”ê°€ (ì„ íƒì‚¬í•­)\n",
    "    parser.add_argument(\"-e\", \"--retrain_epochs\", type=int, default=None, help=\"Number of epochs for retraining (uses config value if not set)\")\n",
    "\n",
    "    if 'ipykernel' in sys.modules:\n",
    "        print(\"Running in interactive mode. Using default config path and epochs from config (or 1000).\")\n",
    "        args = parser.parse_args([]) # ê¸°ë³¸ê°’ ì‚¬ìš©\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    generate_submission_main(config_path=args.config, retrain_epochs=args.retrain_epochs) # ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33182661-0876-4ba8-aeb9-e88dd078e835",
   "metadata": {},
   "source": [
    "![ìºê¸€ ì œì¶œ ê²°ê³¼](https://github.com/O-E2/deep_learning/blob/0666538d13c2d34481a02ffd9c976f8979391f3e/dl_titanic_leaderboard.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2a1186-65f9-4e65-9e2c-42ddddaf6112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
