{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "29fcf32b-7b92-4cd3-8eec-b3d9e65b013d",
      "metadata": {
        "id": "29fcf32b-7b92-4cd3-8eec-b3d9e65b013d"
      },
      "source": [
        "# [요구사항 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74e4879d-b21c-47e2-a824-38e9b5cd5b9f",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-18T14:19:47.103074Z",
          "start_time": "2025-10-18T14:19:39.991546Z"
        },
        "id": "74e4879d-b21c-47e2-a824-38e9b5cd5b9f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from datetime import datetime\n",
        "import wandb\n",
        "import argparse\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from pathlib import Path\n",
        "\n",
        "pd.set_option(\"display.width\", None)\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "\n",
        "class TitanicDataset(Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    self.X = torch.FloatTensor(X)\n",
        "    self.y = torch.LongTensor(y)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    feature = self.X[idx]\n",
        "    target = self.y[idx]\n",
        "    return {'input': feature, 'target': target}\n",
        "\n",
        "  def __str__(self):\n",
        "    str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
        "      len(self.X), self.X.shape, self.y.shape\n",
        "    )\n",
        "    return str\n",
        "\n",
        "\n",
        "class TitanicTestDataset(Dataset):\n",
        "  def __init__(self, X):\n",
        "    self.X = torch.FloatTensor(X)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    feature = self.X[idx]\n",
        "    return {'input': feature}\n",
        "\n",
        "  def __str__(self):\n",
        "    str = \"Data Size: {0}, Input Shape: {1}\".format(\n",
        "      len(self.X), self.X.shape\n",
        "    )\n",
        "    return str"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0c95fbb-7f7d-4e0b-bdca-4fcba5346df1",
      "metadata": {
        "id": "d0c95fbb-7f7d-4e0b-bdca-4fcba5346df1"
      },
      "source": [
        "__init__: 입력 데이터(특징 X, 타겟 y)를 파이토치가 사용하는 데이터 형식인 텐서(Tensor)로 변환합니다. 테스트 데이터셋은 y를 예측하는 것이 목표이므로 X만 가집니다.\n",
        "\n",
        "__len__: 데이터셋에 있는 총 샘플 수를 반환합니다.\n",
        "\n",
        "__getitem__: 인덱스(예: dataset[10])를 사용해 하나의 데이터 샘플(학습용은 특징과 타겟, 테스트용은 특징만)을 가져올 수 있게 합니다.\n",
        "\n",
        "__str__: print() 함수로 출력될 때 데이터셋 크기와 형태에 대한 간단한 문자열 설명을 제공합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65a86bed-144d-4055-a024-fe64d3067e4c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-18T14:19:47.119696Z",
          "start_time": "2025-10-18T14:19:47.112382Z"
        },
        "id": "65a86bed-144d-4055-a024-fe64d3067e4c"
      },
      "outputs": [],
      "source": [
        "def get_preprocessed_dataset():\n",
        "    # 스크립트가 실행되는 현재 파일 경로를 기준으로 CSV 파일 경로 설정\n",
        "    try:\n",
        "        CURRENT_FILE_PATH = os.path.dirname(os.path.abspath(__file__))\n",
        "    except NameError:\n",
        "        # __file__이 정의되지 않은 환경(예: Jupyter)에서는 현재 작업 디렉터리를 사용\n",
        "        CURRENT_FILE_PATH = os.getcwd()\n",
        "\n",
        "    train_data_path = os.path.join(CURRENT_FILE_PATH, \"train.csv\")\n",
        "    test_data_path = os.path.join(CURRENT_FILE_PATH, \"test.csv\")\n",
        "\n",
        "    train_df = pd.read_csv(train_data_path)\n",
        "    test_df = pd.read_csv(test_data_path)\n",
        "\n",
        "    all_df = pd.concat([train_df, test_df], sort=False)\n",
        "\n",
        "    all_df = get_preprocessed_dataset_1(all_df)\n",
        "    all_df = get_preprocessed_dataset_2(all_df)\n",
        "    all_df = get_preprocessed_dataset_3(all_df)\n",
        "    all_df = get_preprocessed_dataset_4(all_df)\n",
        "    all_df = get_preprocessed_dataset_5(all_df)\n",
        "    all_df = get_preprocessed_dataset_6(all_df)\n",
        "\n",
        "    print(\"--- Preprocessed DataFrame Columns ---\")\n",
        "    print(all_df.columns)\n",
        "    print(\"--- Preprocessed DataFrame Head ---\")\n",
        "    print(all_df.head(5))\n",
        "\n",
        "    train_X = all_df[~all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1).reset_index(drop=True)\n",
        "    train_y = train_df[\"Survived\"]\n",
        "\n",
        "    test_X = all_df[all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1).reset_index(drop=True)\n",
        "\n",
        "    print(f\"\\nInput Features ({len(train_X.columns)}): {train_X.columns.tolist()}\")\n",
        "\n",
        "    dataset = TitanicDataset(train_X.values, train_y.values)\n",
        "    print(\"--- Full Train Dataset ---\")\n",
        "    print(dataset)\n",
        "\n",
        "    train_dataset, validation_dataset = random_split(dataset, [0.8, 0.2])\n",
        "    test_dataset = TitanicTestDataset(test_X.values)\n",
        "\n",
        "    return train_dataset, validation_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2658c92-0b0c-4da8-94c5-91f54cec0855",
      "metadata": {
        "id": "b2658c92-0b0c-4da8-94c5-91f54cec0855"
      },
      "source": [
        "CSV 찾기 & 로딩: 판다스를 이용해 train.csv와 test.csv 파일을 찾아 읽습니다.\n",
        "\n",
        "결합: 학습 데이터와 테스트 데이터를 하나로 합칩니다. 이렇게 하면 전처리 단계(결측치 채우기, 범주 인코딩 등)를 양쪽 데이터에 일관되게 적용할 수 있습니다.\n",
        "\n",
        "전처리: 보조 함수들(_1부터 _6까지)을 순서대로 호출하여 데이터를 정제하고 변환합니다.\n",
        "\n",
        "분리: 결합했던 데이터를 다시 학습용 특징(train_X), 학습용 레이블(train_y), 테스트용 특징(test_X)으로 나눕니다.\n",
        "\n",
        "Dataset 생성: 사용자 정의 클래스(TitanicDataset, TitanicTestDataset)를 사용해 처리된 데이터를 감쌉니다.\n",
        "\n",
        "학습/검증 분할: 학습 데이터를 모델 훈련에 사용할 더 큰 세트와 훈련 중 성능 검증에 사용할 더 작은 세트로 나눕니다 (80/20 비율).\n",
        "\n",
        "반환: 파이토치의 DataLoader에서 바로 사용할 수 있는 최종 Dataset 객체들을 반환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53fd7314-c2b5-4d11-a6c6-d11de2357e01",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-18T14:19:47.135584Z",
          "start_time": "2025-10-18T14:19:47.125535Z"
        },
        "id": "53fd7314-c2b5-4d11-a6c6-d11de2357e01"
      },
      "outputs": [],
      "source": [
        "def get_preprocessed_dataset_1(all_df):\n",
        "    # Pclass별 Fare (요금) 평균값을 사용하여 Fare 결측치 메우기\n",
        "    Fare_mean = all_df[[\"Pclass\", \"Fare\"]].groupby(\"Pclass\").mean().reset_index()\n",
        "    Fare_mean.columns = [\"Pclass\", \"Fare_mean\"]\n",
        "    all_df = pd.merge(all_df, Fare_mean, on=\"Pclass\", how=\"left\")\n",
        "    all_df.loc[(all_df[\"Fare\"].isnull()), \"Fare\"] = all_df[\"Fare_mean\"]\n",
        "    all_df = all_df.drop(columns=[\"Fare_mean\"])\n",
        "    return all_df\n",
        "\n",
        "\n",
        "def get_preprocessed_dataset_2(all_df):\n",
        "    # name을 세 개의 컬럼으로 분리하여 다시 all_df에 합침\n",
        "    name_df = all_df[\"Name\"].str.split(\"[,.]\", n=2, expand=True)\n",
        "    name_df.columns = [\"family_name\", \"title\", \"name\"]\n",
        "    name_df[\"family_name\"] = name_df[\"family_name\"].str.strip()\n",
        "    name_df[\"title\"] = name_df[\"title\"].str.strip()\n",
        "    name_df[\"name\"] = name_df[\"name\"].str.strip()\n",
        "    all_df = pd.concat([all_df, name_df], axis=1)\n",
        "    return all_df\n",
        "\n",
        "\n",
        "def get_preprocessed_dataset_3(all_df):\n",
        "    # title별 Age 평균값을 사용하여 Age 결측치 메우기\n",
        "    title_age_mean = all_df[[\"title\", \"Age\"]].groupby(\"title\").median().round().reset_index()\n",
        "    title_age_mean.columns = [\"title\", \"title_age_mean\", ]\n",
        "    all_df = pd.merge(all_df, title_age_mean, on=\"title\", how=\"left\")\n",
        "    all_df.loc[(all_df[\"Age\"].isnull()), \"Age\"] = all_df[\"title_age_mean\"]\n",
        "    all_df = all_df.drop([\"title_age_mean\"], axis=1)\n",
        "    return all_df\n",
        "\n",
        "\n",
        "def get_preprocessed_dataset_4(all_df):\n",
        "    # 가족수(family_num) 컬럼 새롭게 추가\n",
        "    all_df[\"family_num\"] = all_df[\"Parch\"] + all_df[\"SibSp\"]\n",
        "    # 혼자탑승(alone) 컬럼 새롭게 추가\n",
        "    all_df.loc[all_df[\"family_num\"] == 0, \"alone\"] = 1\n",
        "    all_df[\"alone\"].fillna(0, inplace=True)\n",
        "    # 학습에 불필요한 컬럼 제거\n",
        "    all_df = all_df.drop([\"PassengerId\", \"Name\", \"family_name\", \"name\", \"Ticket\", \"Cabin\"], axis=1)\n",
        "    return all_df\n",
        "\n",
        "\n",
        "def get_preprocessed_dataset_5(all_df):\n",
        "    # title 값 개수 줄이기\n",
        "    all_df.loc[\n",
        "    ~(\n",
        "            (all_df[\"title\"] == \"Mr\") |\n",
        "            (all_df[\"title\"] == \"Miss\") |\n",
        "            (all_df[\"title\"] == \"Mrs\") |\n",
        "            (all_df[\"title\"] == \"Master\")\n",
        "    ),\n",
        "    \"title\"\n",
        "    ] = \"other\"\n",
        "    all_df[\"Embarked\"].fillna(\"missing\", inplace=True)\n",
        "    return all_df\n",
        "\n",
        "\n",
        "def get_preprocessed_dataset_6(all_df):\n",
        "    # 카테고리 변수를 LabelEncoder를 사용하여 수치값으로 변경하기\n",
        "    category_features = all_df.columns[all_df.dtypes == \"object\"]\n",
        "    for category_feature in category_features:\n",
        "        le = LabelEncoder()\n",
        "        if all_df[category_feature].dtypes == \"object\":\n",
        "          le = le.fit(all_df[category_feature])\n",
        "          all_df[category_feature] = le.transform(all_df[category_feature])\n",
        "    return all_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96fba863-8cde-4aec-a139-d0edd92f0f2f",
      "metadata": {
        "id": "96fba863-8cde-4aec-a139-d0edd92f0f2f"
      },
      "source": [
        "_1: 누락된 요금 값을 각 승객 등급의 평균 요금으로 채웁니다.\n",
        "\n",
        "_2: Name 열에서 호칭을 추출합니다.\n",
        "\n",
        "_3: 누락된 나이값을 추출된 title과 연관된 나이의 중앙값으로 채웁니다.\n",
        "\n",
        "_4: 새로운 특징인 family_num과 alone을 만듭니다. 모델링에 불필요하다고 판단되는 열을 제거합니다.\n",
        "\n",
        "_5: 드문 호칭들을 'other'로 그룹화하여 title 열을 단순화합니다. 누락된 탑승 항구 값을 'missing'이라는 임시 값으로 채웁니다.\n",
        "\n",
        "_6: 범주형 문자열 열을 Label Encoding을 사용해 숫자 표현으로 변환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-18T14:19:47.150183Z",
          "start_time": "2025-10-18T14:19:47.143647Z"
        },
        "id": "initial_id"
      },
      "outputs": [],
      "source": [
        "def get_data():\n",
        "  # 1번 블록의 전처리 함수 호출 (이 함수는 다른 파일에 정의되어 있다고 가정)\n",
        "  train_dataset, validation_dataset, test_dataset = get_preprocessed_dataset()\n",
        "\n",
        "  print(f\"\\nTrain dataset size: {len(train_dataset)}\")\n",
        "  print(f\"Validation dataset size: {len(validation_dataset)}\")\n",
        "  print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "\n",
        "  # wandb.config에서 배치 크기를 가져와 DataLoader 생성\n",
        "  train_data_loader = DataLoader(dataset=train_dataset, batch_size=wandb.config.batch_size, shuffle=True)\n",
        "  validation_data_loader = DataLoader(dataset=validation_dataset, batch_size=len(validation_dataset))\n",
        "  test_data_loader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset))\n",
        "\n",
        "  return train_data_loader, validation_data_loader, test_data_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38a17a40-a384-43be-8bbb-1e24bfc112d3",
      "metadata": {
        "id": "38a17a40-a384-43be-8bbb-1e24bfc112d3"
      },
      "source": [
        "데이터들을 DataLoader로 감싸서 모델 학습 시 데이터를 미니배치 단위로 효율적으로 공급할 수 있도록 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d17d1c1c-9b50-4202-9183-476e204f2890",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-18T14:19:47.181103Z",
          "start_time": "2025-10-18T14:19:47.175588Z"
        },
        "id": "d17d1c1c-9b50-4202-9183-476e204f2890"
      },
      "outputs": [],
      "source": [
        "class MyModel(nn.Module):\n",
        "  def __init__(self, n_input, n_output):\n",
        "    super().__init__()\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "      nn.Linear(n_input, wandb.config.n_hidden_unit_list[0]),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(wandb.config.n_hidden_unit_list[0], wandb.config.n_hidden_unit_list[1]),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(wandb.config.n_hidden_unit_list[1], n_output),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.model(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def get_model_and_optimizer():\n",
        "  # 입력 피처 10개 (Pclass, Sex, Age, SibSp, Parch, Fare, Embarked, title, family_num, alone)\n",
        "  # 출력 클래스 2개 (0: 사망, 1: 생존)\n",
        "  my_model = MyModel(n_input=10, n_output=2)\n",
        "  optimizer = optim.SGD(my_model.parameters(), lr=wandb.config.learning_rate)\n",
        "\n",
        "  return my_model, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "157f3e50-c463-43a2-a0ac-af757451220e",
      "metadata": {
        "id": "157f3e50-c463-43a2-a0ac-af757451220e"
      },
      "source": [
        "__init__: 모델을 구성하는 층들을 정의합니다. 여기서는 입력층, 2개의 은닉층, 출력층으로 구성된 간단한 다층 퍼셉트론입니다. 각 층의 뉴런 수는 wandb.config에서 가져옵니다.\n",
        "\n",
        "forward: 입력 데이터(x)가 모델의 층들을 어떤 순서로 통과하여 최종 출력을 만들어내는지 정의합니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ca94715-daae-4c95-b640-0664499ea533",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-18T14:19:47.201571Z",
          "start_time": "2025-10-18T14:19:47.192071Z"
        },
        "id": "3ca94715-daae-4c95-b640-0664499ea533"
      },
      "outputs": [],
      "source": [
        "def training_loop(model, optimizer, train_data_loader, validation_data_loader):\n",
        "  n_epochs = wandb.config.epochs\n",
        "  loss_fn = nn.CrossEntropyLoss()  # 분류 문제이므로 CrossEntropyLoss 사용\n",
        "  next_print_epoch = 100\n",
        "\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    loss_train = 0.0\n",
        "    num_trains = 0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    model.train() # 모델을 학습 모드로 설정\n",
        "    for batch in train_data_loader:\n",
        "      # Dataset이 딕셔너리 형태이므로 키로 접근\n",
        "      input = batch['input']\n",
        "      target = batch['target']\n",
        "\n",
        "      output_train = model(input)\n",
        "      loss = loss_fn(output_train, target)\n",
        "      loss_train += loss.item()\n",
        "      num_trains += 1\n",
        "\n",
        "      # 정확도 계산\n",
        "      _, predicted = torch.max(output_train.data, 1)\n",
        "      total_train += target.size(0)\n",
        "      correct_train += (predicted == target).sum().item()\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    loss_validation = 0.0\n",
        "    num_validations = 0\n",
        "    correct_validation = 0\n",
        "    total_validation = 0\n",
        "\n",
        "    model.eval() # 모델을 평가 모드로 설정\n",
        "    with torch.no_grad():\n",
        "      for batch in validation_data_loader:\n",
        "        input = batch['input']\n",
        "        target = batch['target']\n",
        "\n",
        "        output_validation = model(input)\n",
        "        loss = loss_fn(output_validation, target)\n",
        "        loss_validation += loss.item()\n",
        "        num_validations += 1\n",
        "\n",
        "        # 정확도 계산\n",
        "        _, predicted = torch.max(output_validation.data, 1)\n",
        "        total_validation += target.size(0)\n",
        "        correct_validation += (predicted == target).sum().item()\n",
        "\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "    validation_accuracy = 100 * correct_validation / total_validation\n",
        "\n",
        "    wandb.log({\n",
        "      \"Epoch\": epoch,\n",
        "      \"Training loss\": loss_train / num_trains,\n",
        "      \"Validation loss\": loss_validation / num_validations,\n",
        "      \"Training accuracy\": train_accuracy,\n",
        "      \"Validation accuracy\": validation_accuracy\n",
        "    })\n",
        "\n",
        "    if epoch % next_print_epoch == 0 or epoch == 1:\n",
        "      print(\n",
        "        f\"Epoch {epoch}, \"\n",
        "        f\"Training loss {loss_train / num_trains:.4f}, \"\n",
        "        f\"Validation loss {loss_validation / num_validations:.4f}, \"\n",
        "        f\"Training Acc {train_accuracy:.2f}%, \"\n",
        "        f\"Validation Acc {validation_accuracy:.2f}%\"\n",
        "      )\n",
        "      if epoch >= next_print_epoch:\n",
        "          next_print_epoch += 100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38943a86-cdd2-48a4-924c-d406f05f6cba",
      "metadata": {
        "id": "38943a86-cdd2-48a4-924c-d406f05f6cba"
      },
      "source": [
        "* 에포크 반복: 정해진 횟수만큼 전체 데이터셋 학습을 반복합니다.\n",
        "\n",
        "* 학습 모드: model.train()으로 모델을 학습 상태로 설정합니다.\n",
        "\n",
        "* 미니배치 학습: train_data_loader에서 데이터를 미니배치 단위로 가져와 다음을 수행합니다.\n",
        "\n",
        "* 모델 예측 (model(input))\n",
        "\n",
        "* 손실 계산 (loss_fn)\n",
        "\n",
        "* 역전파 (loss.backward())\n",
        "\n",
        "* 가중치 업데이트 (optimizer.step())\n",
        "\n",
        "* 학습 손실과 정확도 누적 계산\n",
        "\n",
        "* 평가 모드: model.eval()으로 모델을 평가 상태로 설정합니다 (드롭아웃 등 비활성화).\n",
        "\n",
        "* 검증: validation_data_loader에서 데이터를 가져와 모델 예측을 수행하고, 검증 손실과 정확도를 계산합니다 (가중치 업데이트는 안 함).\n",
        "\n",
        "* 로깅: 각 에포크의 학습/검증 손실과 정확도를 wandb에 기록합니다.\n",
        "\n",
        "* 출력: 주기적으로 학습 진행 상황(손실, 정확도)을 화면에 출력합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7edecf4-c9f1-4da6-9493-9dd64275b2bf",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-18T14:19:47.224529Z",
          "start_time": "2025-10-18T14:19:47.218484Z"
        },
        "id": "e7edecf4-c9f1-4da6-9493-9dd64275b2bf"
      },
      "outputs": [],
      "source": [
        "def main(args):\n",
        "  current_time_str = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "\n",
        "  config = {\n",
        "    'epochs': args.epochs,\n",
        "    'batch_size': args.batch_size,\n",
        "    'learning_rate': 1e-3,\n",
        "    'n_hidden_unit_list': [20, 20], # 은닉층 설정은 그대로 사용\n",
        "  }\n",
        "\n",
        "  wandb.init(\n",
        "    mode=\"online\" if args.wandb else \"disabled\",\n",
        "    project=\"titanic_survival_prediction\", # wandb 프로젝트명 변경\n",
        "    notes=\"Titanic survival prediction with MLP\", # wandb 노트 변경\n",
        "    tags=[\"mlp\", \"titanic\"], # wandb 태그 변경\n",
        "    name=current_time_str,\n",
        "    config=config\n",
        "  )\n",
        "  print(\"--- wandb arguments ---\")\n",
        "  print(args)\n",
        "  print(\"--- wandb config ---\")\n",
        "  print(wandb.config)\n",
        "\n",
        "  # test_data_loader도 반환되지만, training_loop에서는 사용하지 않음\n",
        "  train_data_loader, validation_data_loader, test_data_loader = get_data()\n",
        "\n",
        "  linear_model, optimizer = get_model_and_optimizer()\n",
        "\n",
        "  print(\"\\n\" + \"#\" * 50)\n",
        "  print(\"Start Training...\")\n",
        "  training_loop(\n",
        "    model=linear_model,\n",
        "    optimizer=optimizer,\n",
        "    train_data_loader=train_data_loader,\n",
        "    validation_data_loader=validation_data_loader\n",
        "  )\n",
        "  print(\"Training Finished.\")\n",
        "  print(\"#\" * 50 + \"\\n\")\n",
        "\n",
        "  wandb.finish()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07456eb6-9fe5-4bf7-b9f5-174b145b372d",
      "metadata": {
        "id": "07456eb6-9fe5-4bf7-b9f5-174b145b372d"
      },
      "source": [
        "설정 로드: wandb 실험 설정(config)을 정의합니다 (에포크 수, 배치 크기 등).\n",
        "\n",
        "wandb 초기화: 실험 추적을 위해 wandb를 설정하고 시작합니다. 프로젝트 이름, 노트, 태그 등을 지정합니다.\n",
        "\n",
        "데이터 로딩: get_data() 함수를 호출하여 학습/검증/테스트 데이터 로더를 가져옵니다.\n",
        "\n",
        "모델/옵티마이저 생성: get_model_and_optimizer() 함수를 호출하여 모델과 옵티마이저를 준비합니다.\n",
        "\n",
        "학습 시작: training_loop() 함수를 호출하여 모델 학습 및 검증을 시작합니다.\n",
        "\n",
        "wandb 종료: 실험 기록을 마치고 wandb를 종료합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68787fd8-7b3e-4f84-b61d-b86b6b3fbf07",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-18T14:19:47.800684Z",
          "start_time": "2025-10-18T14:19:47.231131Z"
        },
        "id": "68787fd8-7b3e-4f84-b61d-b86b6b3fbf07",
        "outputId": "dca572f8-adda-4b3f-b706-491243799cc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running in Jupyter Notebook. Using default hardcoded args.\n",
            "--- wandb arguments ---\n",
            "Namespace(wandb=False, batch_size=16, epochs=1000)\n",
            "--- wandb config ---\n",
            "{'epochs': 1000, 'batch_size': 16, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20]}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\cyun0\\AppData\\Local\\Temp\\ipykernel_21216\\2165486214.py:37: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  all_df[\"alone\"].fillna(0, inplace=True)\n",
            "C:\\Users\\cyun0\\AppData\\Local\\Temp\\ipykernel_21216\\2165486214.py:54: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  all_df[\"Embarked\"].fillna(\"missing\", inplace=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Preprocessed DataFrame Columns ---\n",
            "Index(['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare',\n",
            "       'Embarked', 'title', 'family_num', 'alone'],\n",
            "      dtype='object')\n",
            "--- Preprocessed DataFrame Head ---\n",
            "   Survived  Pclass  Sex   Age  SibSp  Parch     Fare  Embarked  title  \\\n",
            "0       0.0       3    1  22.0      1      0   7.2500         2      2   \n",
            "1       1.0       1    0  38.0      1      0  71.2833         0      3   \n",
            "2       1.0       3    0  26.0      0      0   7.9250         2      1   \n",
            "3       1.0       1    0  35.0      1      0  53.1000         2      3   \n",
            "4       0.0       3    1  35.0      0      0   8.0500         2      2   \n",
            "\n",
            "   family_num  alone  \n",
            "0           1    0.0  \n",
            "1           1    0.0  \n",
            "2           0    1.0  \n",
            "3           1    0.0  \n",
            "4           0    1.0  \n",
            "\n",
            "Input Features (10): ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'title', 'family_num', 'alone']\n",
            "--- Full Train Dataset ---\n",
            "Data Size: 891, Input Shape: torch.Size([891, 10]), Target Shape: torch.Size([891])\n",
            "\n",
            "Train dataset size: 713\n",
            "Validation dataset size: 178\n",
            "Test dataset size: 418\n",
            "\n",
            "##################################################\n",
            "Start Training...\n",
            "Epoch 1, Training loss 0.7133, Validation loss 0.6617, Training Acc 52.31%, Validation Acc 66.29%\n",
            "Epoch 100, Training loss 0.5820, Validation loss 0.6097, Training Acc 70.27%, Validation Acc 69.10%\n",
            "Epoch 200, Training loss 0.5598, Validation loss 0.5935, Training Acc 70.41%, Validation Acc 69.10%\n",
            "Epoch 300, Training loss 0.5269, Validation loss 0.5782, Training Acc 70.97%, Validation Acc 69.66%\n",
            "Epoch 400, Training loss 0.4863, Validation loss 0.5567, Training Acc 77.00%, Validation Acc 73.60%\n",
            "Epoch 500, Training loss 0.4648, Validation loss 0.5182, Training Acc 78.26%, Validation Acc 73.60%\n",
            "Epoch 600, Training loss 0.4417, Validation loss 0.5338, Training Acc 79.10%, Validation Acc 75.28%\n",
            "Epoch 700, Training loss 0.4212, Validation loss 0.5412, Training Acc 81.63%, Validation Acc 71.91%\n",
            "Epoch 800, Training loss 0.4321, Validation loss 0.5234, Training Acc 80.79%, Validation Acc 74.72%\n",
            "Epoch 900, Training loss 0.4340, Validation loss 0.5387, Training Acc 80.36%, Validation Acc 76.40%\n",
            "Epoch 1000, Training loss 0.4279, Validation loss 0.5709, Training Acc 81.35%, Validation Acc 73.60%\n",
            "Training Finished.\n",
            "##################################################\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import sys # sys 모듈 임포트 추가\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--wandb\", action=argparse.BooleanOptionalAction, default=False, help=\"True or False\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"-b\", \"--batch_size\", type=int, default=16, help=\"Batch size (int, default: 16)\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"-e\", \"--epochs\", type=int, default=1000, help=\"Number of training epochs (int, default:1000)\" # 1_000 -> 1000\n",
        "    )\n",
        "\n",
        "\n",
        "    # Jupyter 노트북 환경인지 확인\n",
        "    if 'ipykernel_launcher' in sys.argv[0]:\n",
        "        print(\"Running in Jupyter Notebook. Using default hardcoded args.\")\n",
        "        # 노트북에서 테스트할 때 원하는 값을 직접 설정\n",
        "        args = argparse.Namespace(wandb=False, batch_size=16, epochs=1000) # 예시: wandb 비활성화, batch 16, epoch 1000\n",
        "        # WandB를 사용하고 싶다면 wandb=True 로 변경\n",
        "        # 다른 batch_size나 epochs로 테스트하고 싶다면 이 값을 변경\n",
        "    else:\n",
        "        # 터미널에서 실행될 때는 명령줄 인자 파싱\n",
        "        args = parser.parse_args()\n",
        "\n",
        "\n",
        "    main(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83765600-acf9-41c7-a241-8e4168faff7f",
      "metadata": {
        "id": "83765600-acf9-41c7-a241-8e4168faff7f"
      },
      "source": [
        "터미널에서 실행할 때 --epochs, --batch_size 같은 인자를 받을 수 있게 설정합니다. 받은 인자를 main 함수에 전달하여 실행합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01b2bc30-c744-4282-be83-da1778bd80d4",
      "metadata": {
        "id": "01b2bc30-c744-4282-be83-da1778bd80d4"
      },
      "source": [
        "![학습1](https://github.com/O-E2/deep_learning/blob/ee3a2d4c8d369f2970393fc38fefdefbeedc8e62/dl_basic_wandb.png)\n",
        "\n",
        "Wandb URL\n",
        "https://wandb.ai/cyun0407-korea-university-of-technology-and-education/titanic_survival_prediction/runs/i4pqcafr  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a8138a5-2878-47bd-9a44-2853765f5a3c",
      "metadata": {
        "id": "8a8138a5-2878-47bd-9a44-2853765f5a3c"
      },
      "source": [
        "# [요구사항 2]\n",
        "\n",
        "Wansb URL\n",
        "https://wandb.ai/cyun0407-korea-university-of-technology-and-education/titanic_hyperparameter_tuning?nw=nwusercyun0407\n",
        "\n",
        "더 나은 성능을 산출하는 Activation Function : ReLU\n",
        "더 나은 성능을 산출하는 Batch Size : 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "287d3b1d-cb29-48fc-b45e-6670319fe9e9",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-18T14:19:48.125984Z",
          "start_time": "2025-10-18T14:19:48.119443Z"
        },
        "id": "287d3b1d-cb29-48fc-b45e-6670319fe9e9"
      },
      "outputs": [],
      "source": [
        "# find_best_hyperparameters.py\n",
        "import os, json, sys, argparse, copy\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from datetime import datetime\n",
        "import wandb\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from pathlib import Path\n",
        "\n",
        "class TitanicDataset(Dataset):\n",
        "    def __init__(self, X, y): self.X = torch.FloatTensor(X); self.y = torch.LongTensor(y)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return {'input': self.X[idx], 'target': self.y[idx]}\n",
        "    def __str__(self): return f\"Data Size: {len(self.X)}, Input Shape: {self.X.shape}, Target Shape: {self.y.shape}\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56d55eed-b462-412b-86bc-1cff947973dc",
      "metadata": {
        "id": "56d55eed-b462-412b-86bc-1cff947973dc"
      },
      "source": [
        "파이토치 DataLoader가 사용할 수 있도록 데이터셋의 구조(데이터 로딩, 길이 반환, 특정 항목 접근 방법)를 정의합니다. TitanicDataset은 학습 및 검증 데이터를 다룹니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1814570a-a867-4058-b351-16f0fcd68e2d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-18T14:19:48.471633Z",
          "start_time": "2025-10-18T14:19:48.460089Z"
        },
        "id": "1814570a-a867-4058-b351-16f0fcd68e2d"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_preprocessed_dataset_1(all_df):\n",
        "    Fare_mean = all_df[[\"Pclass\", \"Fare\"]].groupby(\"Pclass\").mean().reset_index(); Fare_mean.columns = [\"Pclass\", \"Fare_mean\"]\n",
        "    all_df = pd.merge(all_df, Fare_mean, on=\"Pclass\", how=\"left\"); all_df.loc[all_df[\"Fare\"].isnull(), \"Fare\"] = all_df[\"Fare_mean\"]\n",
        "    return all_df.drop(columns=[\"Fare_mean\"])\n",
        "def get_preprocessed_dataset_2(all_df):\n",
        "    name_df = all_df[\"Name\"].str.split(\"[,.]\", n=2, expand=True)\n",
        "    if name_df.shape[1] == 3:\n",
        "        name_df.columns = [\"family_name\", \"title\", \"name\"]; name_df[\"family_name\"] = name_df[\"family_name\"].str.strip(); name_df[\"title\"] = name_df[\"title\"].str.strip(); name_df[\"name\"] = name_df[\"name\"].str.strip()\n",
        "        all_df = pd.concat([all_df, name_df], axis=1)\n",
        "    else: all_df['title'] = 'unknown' # 이름 분리 실패 시 'unknown' title 추가\n",
        "    return all_df\n",
        "def get_preprocessed_dataset_3(all_df):\n",
        "    if 'title' not in all_df.columns:\n",
        "        if all_df['Age'].isnull().any(): all_df['Age'] = all_df['Age'].fillna(all_df['Age'].median())\n",
        "        return all_df\n",
        "    title_age_median = all_df[[\"title\", \"Age\"]].groupby(\"title\").median().round().reset_index(); title_age_median.columns = [\"title\", \"title_age_median\"]\n",
        "    all_df = pd.merge(all_df, title_age_median, on=\"title\", how=\"left\"); all_df.loc[all_df[\"Age\"].isnull(), \"Age\"] = all_df[\"title_age_median\"]\n",
        "    if \"title_age_median\" in all_df.columns: all_df = all_df.drop([\"title_age_median\"], axis=1)\n",
        "    return all_df\n",
        "def get_preprocessed_dataset_4(all_df):\n",
        "    all_df[\"family_num\"] = all_df[\"Parch\"] + all_df[\"SibSp\"]; all_df.loc[all_df[\"family_num\"] == 0, \"alone\"] = 1; all_df[\"alone\"] = all_df[\"alone\"].fillna(0).astype(float)\n",
        "    cols_to_drop = [\"PassengerId\", \"Name\", \"family_name\", \"name\", \"Ticket\", \"Cabin\"]; existing_cols_to_drop = [col for col in cols_to_drop if col in all_df.columns]\n",
        "    return all_df.drop(existing_cols_to_drop, axis=1)\n",
        "def get_preprocessed_dataset_5(all_df):\n",
        "    if 'title' in all_df.columns: all_df.loc[~((all_df[\"title\"] == \"Mr\") | (all_df[\"title\"] == \"Miss\") | (all_df[\"title\"] == \"Mrs\") | (all_df[\"title\"] == \"Master\")), \"title\"] = \"other\"\n",
        "    all_df[\"Embarked\"] = all_df[\"Embarked\"].fillna(\"missing\")\n",
        "    return all_df\n",
        "def get_preprocessed_dataset_6(all_df):\n",
        "    category_features = all_df.select_dtypes(include=['object']).columns\n",
        "    for cat_feat in category_features:\n",
        "        le = LabelEncoder(); valid_indices = all_df[cat_feat].notna()\n",
        "        if valid_indices.any():\n",
        "            all_df.loc[valid_indices, cat_feat] = le.fit_transform(all_df.loc[valid_indices, cat_feat])\n",
        "            try: all_df[cat_feat] = pd.to_numeric(all_df[cat_feat])\n",
        "            except ValueError: pass\n",
        "    return all_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91bdd747-11c6-4b6d-a3f3-11deb5c95ab7",
      "metadata": {
        "id": "91bdd747-11c6-4b6d-a3f3-11deb5c95ab7"
      },
      "source": [
        "타이타닉 원본 데이터를 단계별로 가공하는 함수들입니다. 결측치 처리, 특징 생성 및 추출, 범주형 데이터의 수치화 등을 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83ed857c-d889-4049-be19-e4349cee6b5d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-18T14:19:48.803606Z",
          "start_time": "2025-10-18T14:19:48.796147Z"
        },
        "id": "83ed857c-d889-4049-be19-e4349cee6b5d"
      },
      "outputs": [],
      "source": [
        "def get_preprocessed_data_raw():\n",
        "    \"\"\"CSV 로드, 전처리, 학습/검증용 데이터 배열 반환\"\"\"\n",
        "    try: CURRENT_FILE_PATH = os.path.dirname(os.path.abspath(__file__))\n",
        "    except NameError: CURRENT_FILE_PATH = os.getcwd()\n",
        "    train_data_path = os.path.join(CURRENT_FILE_PATH, \"train.csv\")\n",
        "    test_data_path = os.path.join(CURRENT_FILE_PATH, \"test.csv\") # test는 로드만 하고 사용 안함\n",
        "    try:\n",
        "        train_df = pd.read_csv(train_data_path)\n",
        "        test_df = pd.read_csv(test_data_path)\n",
        "    except FileNotFoundError as e: print(f\"Error: {e}\"); sys.exit(1)\n",
        "\n",
        "    all_df = pd.concat([train_df, test_df], sort=False) # 전처리 일관성 위해 합침\n",
        "    all_df = get_preprocessed_dataset_1(all_df); all_df = get_preprocessed_dataset_2(all_df)\n",
        "    all_df = get_preprocessed_dataset_3(all_df); all_df = get_preprocessed_dataset_4(all_df)\n",
        "    all_df = get_preprocessed_dataset_5(all_df); all_df = get_preprocessed_dataset_6(all_df)\n",
        "    train_X_df = all_df[~all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1)\n",
        "    train_y = train_df[\"Survived\"]\n",
        "\n",
        "    # 학습 데이터에 대해서만 최종 NaN 및 타입 확인\n",
        "    print(\"\\n--- Final Data Check & Fill NaNs (Train Data Only) ---\")\n",
        "    columns_to_check = train_X_df.columns\n",
        "    df = train_X_df\n",
        "    for col in columns_to_check:\n",
        "        if df[col].dtype == 'object':\n",
        "            try: df[col] = pd.to_numeric(df[col])\n",
        "            except ValueError: df[col] = 0 # 변환 불가시 0으로 채움\n",
        "        if df[col].isnull().any(): df[col] = df[col].fillna(0) # NaN 0으로 채움\n",
        "    train_X = train_X_df.reset_index(drop=True)\n",
        "\n",
        "    print(\"Train data preprocessing finished.\")\n",
        "    return train_X.values, train_y.values # 학습 데이터(X, y)만 반환"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bc9033a-40eb-4db0-94e3-abb9f1127b09",
      "metadata": {
        "id": "8bc9033a-40eb-4db0-94e3-abb9f1127b09"
      },
      "source": [
        "CSV 파일을 로드하고 위의 전처리 함수들을 순서대로 호출합니다. 최종적으로 학습 데이터만 넘파이 배열 형태로 반환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bef9a48-c6d2-4eda-b441-0f4f570a9e10",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-18T14:19:49.511837Z",
          "start_time": "2025-10-18T14:19:49.506287Z"
        },
        "id": "0bef9a48-c6d2-4eda-b441-0f4f570a9e10"
      },
      "outputs": [],
      "source": [
        "def get_dataloaders(train_X, train_y, batch_size_config):\n",
        "    \"\"\"학습/검증 DataLoader 생성\"\"\"\n",
        "    dataset = TitanicDataset(train_X, train_y)\n",
        "    generator = torch.Generator().manual_seed(42) # 데이터 분할 재현성 위한 시드 고정\n",
        "    train_dataset, validation_dataset = random_split(dataset, [0.8, 0.2], generator=generator)\n",
        "    print(f\"\\nTrain size: {len(train_dataset)}, Validation size: {len(validation_dataset)}\")\n",
        "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size_config, shuffle=True)\n",
        "    validation_loader = DataLoader(dataset=validation_dataset, batch_size=len(validation_dataset)) # 검증은 전체 데이터 사용\n",
        "    return train_loader, validation_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3552c139-f80c-4061-8b6f-c789e8713075",
      "metadata": {
        "id": "3552c139-f80c-4061-8b6f-c789e8713075"
      },
      "source": [
        "전처리된 학습 데이터(X, y)와 배치 크기를 받아, 데이터를 학습용과 검증용으로 8:2 비율로 나눈 뒤 각각에 대한 DataLoader 객체를 생성하여 반환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07a9fe2a-cfb2-4ad1-a35d-8a4ed5686014",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-18T14:19:49.610115Z",
          "start_time": "2025-10-18T14:19:49.604580Z"
        },
        "id": "07a9fe2a-cfb2-4ad1-a35d-8a4ed5686014"
      },
      "outputs": [],
      "source": [
        "class MyModel(nn.Module):\n",
        "    \"\"\"활성화 함수와 은닉층 크기를 인자로 받는 MLP 모델\"\"\"\n",
        "    def __init__(self, n_input, n_output, activation_fn_class, n_hidden1=20, n_hidden2=20):\n",
        "        super().__init__(); activation = activation_fn_class()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(n_input, n_hidden1), activation,\n",
        "            nn.Linear(n_hidden1, n_hidden2), activation,\n",
        "            nn.Linear(n_hidden2, n_output)\n",
        "        )\n",
        "    def forward(self, x): return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e21456c8-02d3-4a63-a32c-af3be398f8f6",
      "metadata": {
        "id": "e21456c8-02d3-4a63-a32c-af3be398f8f6"
      },
      "source": [
        "파이토치 nn.Module을 상속받아 간단한 모델 구조를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6302d8b-8c56-495f-90d5-579c118a788b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-18T14:19:49.655483Z",
          "start_time": "2025-10-18T14:19:49.650398Z"
        },
        "id": "c6302d8b-8c56-495f-90d5-579c118a788b"
      },
      "outputs": [],
      "source": [
        "def get_model_and_optimizer(activation_fn_class, learning_rate, n_hidden1=20, n_hidden2=20): # ... (이전과 동일) ...\n",
        "    \"\"\"모델 객체와 SGD 옵티마이저 생성\"\"\"\n",
        "    model = MyModel(10, 2, activation_fn_class, n_hidden1, n_hidden2) # 입력 10개, 출력 2개 고정\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "    return model, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "303ad0e1-8893-42d9-8d1a-c90b0851a04c",
      "metadata": {
        "id": "303ad0e1-8893-42d9-8d1a-c90b0851a04c"
      },
      "source": [
        "MyModel 클래스를 이용해 모델 객체를 생성하고, 학습에 사용할 SGD 옵티마이저를 설정하여 함께 반환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f7e520c-37a3-49ac-ad4a-084895580032",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-18T14:19:49.734636Z",
          "start_time": "2025-10-18T14:19:49.725370Z"
        },
        "id": "4f7e520c-37a3-49ac-ad4a-084895580032"
      },
      "outputs": [],
      "source": [
        "# --- 학습 루프: 최종 정확도만 반환 ---\n",
        "def simplified_training_loop(model, optimizer, train_data_loader, validation_data_loader, n_epochs, loss_fn):\n",
        "    \"\"\"모델 학습/검증 수행 후 최종 검증 정확도 반환 (최고 기록 없음)\"\"\"\n",
        "    print(f\"\\nStart Training for {n_epochs} epochs...\")\n",
        "    final_validation_accuracy = 0.0 # 마지막 에포크 정확도 저장용\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        # --- 학습\n",
        "        model.train(); loss_train_epoch = 0.0; correct_train_epoch = 0; total_train_samples = 0\n",
        "        for batch in train_data_loader: # ... (학습 로직 동일) ...\n",
        "            input_data = batch['input']; target = batch['target']; optimizer.zero_grad(); output_train = model(input_data); loss = loss_fn(output_train, target); loss.backward(); optimizer.step()\n",
        "            loss_train_epoch += loss.item() * input_data.size(0); _, predicted = torch.max(output_train.data, 1); total_train_samples += target.size(0); correct_train_epoch += (predicted == target).sum().item()\n",
        "        avg_loss_train = loss_train_epoch / total_train_samples if total_train_samples else 0\n",
        "        train_accuracy = 100 * correct_train_epoch / total_train_samples if total_train_samples else 0\n",
        "\n",
        "        # --- 검증\n",
        "        model.eval(); loss_val_epoch = 0.0; correct_val_epoch = 0; total_val_samples = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in validation_data_loader: # ... (검증 로직 동일) ...\n",
        "                input_data = batch['input']; target = batch['target']; output_validation = model(input_data); loss = loss_fn(output_validation, target); loss_val_epoch += loss.item() * input_data.size(0)\n",
        "                _, predicted = torch.max(output_validation.data, 1); total_val_samples += target.size(0); correct_val_epoch += (predicted == target).sum().item()\n",
        "        avg_loss_validation = loss_val_epoch / total_val_samples if total_val_samples else 0\n",
        "        validation_accuracy = 100 * correct_val_epoch / total_val_samples if total_val_samples else 0\n",
        "        final_validation_accuracy = validation_accuracy # 마지막 값 갱신\n",
        "\n",
        "        # 로깅 및 출력\n",
        "        if wandb.run: wandb.log({\"Epoch\": epoch, \"Training loss\": avg_loss_train, \"Validation loss\": avg_loss_validation, \"Training accuracy\": train_accuracy, \"Validation accuracy\": validation_accuracy})\n",
        "        if epoch % 100 == 0 or epoch == 1: print(f\"Epoch {epoch:>{len(str(n_epochs))}}/{n_epochs}, Train Loss: {avg_loss_train:.4f}, Acc: {train_accuracy:.2f}% | Val Loss: {avg_loss_validation:.4f}, Acc: {validation_accuracy:.2f}%\")\n",
        "\n",
        "    print(f\"\\n--- Run Finished --- Final Val Acc: {final_validation_accuracy:.2f}%\")\n",
        "    return final_validation_accuracy # 최종 검증 정확도만 반환"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bd276ad-6f18-44b4-9e7b-f2729f69d2b9",
      "metadata": {
        "id": "7bd276ad-6f18-44b4-9e7b-f2729f69d2b9"
      },
      "source": [
        "모델 학습과 검증을 지정된 에포크만큼 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c21936d0-7ce6-4ab7-bc56-095143a45526",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-18T14:19:49.807241Z",
          "start_time": "2025-10-18T14:19:49.795626Z"
        },
        "id": "c21936d0-7ce6-4ab7-bc56-095143a45526"
      },
      "outputs": [],
      "source": [
        "# --- 메인 실행 함수 하이퍼파라미터 탐색 및 최적 조합 저장 ---\n",
        "def find_hyperparameters(args):\n",
        "    \"\"\"최적 하이퍼파라미터 '조합'만 찾아 json 파일로 저장\"\"\"\n",
        "    current_time_str = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "    # 실험 설정\n",
        "    batch_sizes_to_test = [16, 32, 64, 128]\n",
        "    activation_functions_to_test = {\"Sigmoid\": nn.Sigmoid, \"ReLU\": nn.ReLU, \"ELU\": nn.ELU, \"LeakyReLU\": nn.LeakyReLU}\n",
        "    learning_rate = 1e-3; n_hidden_units = [20, 20]; loss_fn = nn.CrossEntropyLoss()\n",
        "    # 데이터 로딩\n",
        "    print(\"Preprocessing data for tuning...\"); train_X, train_y = get_preprocessed_data_raw()[:2] # 학습 데이터만 필요\n",
        "    # 최고 성능 추적\n",
        "    overall_best_accuracy = -1.0; overall_best_config = {}\n",
        "\n",
        "    # 실험 루프\n",
        "    for batch_size in batch_sizes_to_test:\n",
        "        print(f\"\\n{'='*25} Testing Batch Size: {batch_size} {'='*25}\")\n",
        "        train_loader, validation_loader = get_dataloaders(train_X, train_y, batch_size)\n",
        "        for activation_name, activation_fn_class in activation_functions_to_test.items():\n",
        "            print(f\"\\n--- Testing Activation: {activation_name} ---\")\n",
        "            config = {'epochs': args.epochs, 'batch_size': batch_size, 'learning_rate': learning_rate, 'n_hidden_unit_list': n_hidden_units, 'activation_function': activation_name}\n",
        "            run_name = f\"Tune_{activation_name}_BS{batch_size}_{current_time_str}\"\n",
        "            run = wandb.init(mode=\"online\" if args.wandb else \"disabled\", project=\"titanic_hyperparameter_tuning\", name=run_name, config=config, reinit=True)\n",
        "            print(\"--- Run Config ---\"); print(config)\n",
        "            model, optimizer = get_model_and_optimizer(activation_fn_class, config['learning_rate'], n_hidden1=config['n_hidden_unit_list'][0], n_hidden2=config['n_hidden_unit_list'][1])\n",
        "\n",
        "            # 간소화된 학습 루프 실행, 최종 정확도 받기\n",
        "            final_accuracy_run = simplified_training_loop(model, optimizer, train_loader, validation_loader, config['epochs'], loss_fn)\n",
        "\n",
        "            # 최고 조합 갱신 확인 (최종 정확도 기준)\n",
        "            if final_accuracy_run > overall_best_accuracy:\n",
        "                overall_best_accuracy = final_accuracy_run\n",
        "                overall_best_config = config # config만 저장\n",
        "                print(f\" New Best Config Found! Final Val Acc: {overall_best_accuracy:.2f}%, Config: {activation_name}/BS={batch_size}\")\n",
        "            run.finish()\n",
        "\n",
        "    # 최종 결과 요약 및 저장 (config만)\n",
        "    print(\"\\n\" + \"=\" * 60); print(\" Hyperparameter Tuning Finished (Config Only) \"); print(\"=\" * 60)\n",
        "    if overall_best_config:\n",
        "        print(f\"Overall Best Final Val Acc: {overall_best_accuracy:.2f}%\")\n",
        "        print(f\"Best Config Found: {overall_best_config}\")\n",
        "        best_config_path = \"best_hyperparameters.json\"\n",
        "        try:\n",
        "            with open(best_config_path, 'w') as f: json.dump(overall_best_config, f, indent=4)\n",
        "            print(f\"Best hyperparameters saved to {best_config_path}\")\n",
        "        except Exception as e: print(f\"Error saving best config: {e}\")\n",
        "    else: print(\"No successful training run recorded.\")\n",
        "    print(\"=\" * 60 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db456e86-f7e7-4c8a-9dd2-0f785f5c4b33",
      "metadata": {
        "id": "db456e86-f7e7-4c8a-9dd2-0f785f5c4b33"
      },
      "source": [
        "지정된 배치 크기와 활성화 함수들의 모든 조합에 대해 루프를 돕니다. 각 조합마다 simplified_training_loop를 실행하여 최종 검증 정확도를 얻고, 이 정확도가 지금까지 기록된 최고 정확도보다 높으면 해당 조합의 설정을 overall_best_config에 저장합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a3f2cbc-90c7-4fed-a961-230562e2a4e2",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-18T14:22:11.744161Z",
          "start_time": "2025-10-18T14:19:49.851601Z"
        },
        "id": "3a3f2cbc-90c7-4fed-a961-230562e2a4e2",
        "outputId": "73c17971-2f5a-4d48-fe56-6edad7d3d451"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running in interactive mode. Using default args: epochs=1000, wandb=False\n",
            "Preprocessing data for tuning...\n",
            "\n",
            "--- Final Data Check & Fill NaNs (Train Data Only) ---\n",
            "Train data preprocessing finished.\n",
            "\n",
            "========================= Testing Batch Size: 16 =========================\n",
            "\n",
            "Train size: 713, Validation size: 178\n",
            "\n",
            "--- Testing Activation: Sigmoid ---\n",
            "--- Run Config ---\n",
            "{'epochs': 1000, 'batch_size': 16, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'Sigmoid'}\n",
            "\n",
            "Start Training for 1000 epochs...\n",
            "Epoch    1/1000, Train Loss: 0.6792, Acc: 59.33% | Val Loss: 0.6332, Acc: 70.79%\n",
            "Epoch  100/1000, Train Loss: 0.6665, Acc: 59.33% | Val Loss: 0.6286, Acc: 70.79%\n",
            "Epoch  200/1000, Train Loss: 0.6532, Acc: 59.33% | Val Loss: 0.6230, Acc: 70.79%\n",
            "Epoch  300/1000, Train Loss: 0.6373, Acc: 59.47% | Val Loss: 0.6167, Acc: 70.22%\n",
            "Epoch  400/1000, Train Loss: 0.6191, Acc: 69.57% | Val Loss: 0.6109, Acc: 69.66%\n",
            "Epoch  500/1000, Train Loss: 0.6072, Acc: 69.57% | Val Loss: 0.6115, Acc: 66.85%\n",
            "Epoch  600/1000, Train Loss: 0.6006, Acc: 69.28% | Val Loss: 0.6128, Acc: 65.73%\n",
            "Epoch  700/1000, Train Loss: 0.5969, Acc: 68.86% | Val Loss: 0.6126, Acc: 66.29%\n",
            "Epoch  800/1000, Train Loss: 0.5942, Acc: 68.86% | Val Loss: 0.6087, Acc: 66.29%\n",
            "Epoch  900/1000, Train Loss: 0.5920, Acc: 68.86% | Val Loss: 0.6061, Acc: 66.29%\n",
            "Epoch 1000/1000, Train Loss: 0.5902, Acc: 68.86% | Val Loss: 0.6045, Acc: 66.29%\n",
            "\n",
            "--- Run Finished --- Final Val Acc: 66.29%\n",
            " New Best Config Found! Final Val Acc: 66.29%, Config: Sigmoid/BS=16\n",
            "\n",
            "--- Testing Activation: ReLU ---\n",
            "--- Run Config ---\n",
            "{'epochs': 1000, 'batch_size': 16, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'ReLU'}\n",
            "\n",
            "Start Training for 1000 epochs...\n",
            "Epoch    1/1000, Train Loss: 0.6891, Acc: 61.29% | Val Loss: 0.6717, Acc: 61.24%\n",
            "Epoch  100/1000, Train Loss: 0.5936, Acc: 68.86% | Val Loss: 0.5912, Acc: 67.42%\n",
            "Epoch  200/1000, Train Loss: 0.5826, Acc: 70.55% | Val Loss: 0.5751, Acc: 67.98%\n",
            "Epoch  300/1000, Train Loss: 0.5718, Acc: 71.39% | Val Loss: 0.5528, Acc: 73.03%\n",
            "Epoch  400/1000, Train Loss: 0.5536, Acc: 72.79% | Val Loss: 0.5328, Acc: 74.16%\n",
            "Epoch  500/1000, Train Loss: 0.5355, Acc: 73.77% | Val Loss: 0.5027, Acc: 75.28%\n",
            "Epoch  600/1000, Train Loss: 0.5046, Acc: 77.70% | Val Loss: 0.4913, Acc: 75.28%\n",
            "Epoch  700/1000, Train Loss: 0.4791, Acc: 77.70% | Val Loss: 0.4656, Acc: 79.78%\n",
            "Epoch  800/1000, Train Loss: 0.4740, Acc: 78.96% | Val Loss: 0.4347, Acc: 82.58%\n",
            "Epoch  900/1000, Train Loss: 0.4841, Acc: 77.70% | Val Loss: 0.4229, Acc: 83.15%\n",
            "Epoch 1000/1000, Train Loss: 0.4564, Acc: 79.80% | Val Loss: 0.4300, Acc: 83.71%\n",
            "\n",
            "--- Run Finished --- Final Val Acc: 83.71%\n",
            " New Best Config Found! Final Val Acc: 83.71%, Config: ReLU/BS=16\n",
            "\n",
            "--- Testing Activation: ELU ---\n",
            "--- Run Config ---\n",
            "{'epochs': 1000, 'batch_size': 16, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'ELU'}\n",
            "\n",
            "Start Training for 1000 epochs...\n",
            "Epoch    1/1000, Train Loss: 0.6360, Acc: 64.94% | Val Loss: 0.6145, Acc: 65.73%\n",
            "Epoch  100/1000, Train Loss: 0.5795, Acc: 69.71% | Val Loss: 0.5846, Acc: 67.42%\n",
            "Epoch  200/1000, Train Loss: 0.5388, Acc: 72.93% | Val Loss: 0.5798, Acc: 70.79%\n",
            "Epoch  300/1000, Train Loss: 0.4956, Acc: 76.30% | Val Loss: 0.4909, Acc: 79.21%\n",
            "Epoch  400/1000, Train Loss: 0.4712, Acc: 78.40% | Val Loss: 0.4450, Acc: 81.46%\n",
            "Epoch  500/1000, Train Loss: 0.4752, Acc: 79.24% | Val Loss: 0.4594, Acc: 81.46%\n",
            "Epoch  600/1000, Train Loss: 0.4577, Acc: 78.96% | Val Loss: 0.4378, Acc: 82.02%\n",
            "Epoch  700/1000, Train Loss: 0.4517, Acc: 78.68% | Val Loss: 0.4222, Acc: 82.02%\n",
            "Epoch  800/1000, Train Loss: 0.4495, Acc: 78.40% | Val Loss: 0.4220, Acc: 81.46%\n",
            "Epoch  900/1000, Train Loss: 0.4438, Acc: 80.08% | Val Loss: 0.4399, Acc: 83.15%\n",
            "Epoch 1000/1000, Train Loss: 0.4379, Acc: 80.50% | Val Loss: 0.4949, Acc: 80.90%\n",
            "\n",
            "--- Run Finished --- Final Val Acc: 80.90%\n",
            "\n",
            "--- Testing Activation: LeakyReLU ---\n",
            "--- Run Config ---\n",
            "{'epochs': 1000, 'batch_size': 16, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'LeakyReLU'}\n",
            "\n",
            "Start Training for 1000 epochs...\n",
            "Epoch    1/1000, Train Loss: 0.6913, Acc: 53.30% | Val Loss: 0.6628, Acc: 64.61%\n",
            "Epoch  100/1000, Train Loss: 0.5810, Acc: 69.99% | Val Loss: 0.5878, Acc: 65.73%\n",
            "Epoch  200/1000, Train Loss: 0.5673, Acc: 72.65% | Val Loss: 0.5632, Acc: 71.91%\n",
            "Epoch  300/1000, Train Loss: 0.5340, Acc: 74.05% | Val Loss: 0.5311, Acc: 71.91%\n",
            "Epoch  400/1000, Train Loss: 0.4985, Acc: 76.44% | Val Loss: 0.5072, Acc: 77.53%\n",
            "Epoch  500/1000, Train Loss: 0.4771, Acc: 78.26% | Val Loss: 0.4145, Acc: 84.27%\n",
            "Epoch  600/1000, Train Loss: 0.4804, Acc: 77.84% | Val Loss: 0.4131, Acc: 83.15%\n",
            "Epoch  700/1000, Train Loss: 0.4680, Acc: 79.52% | Val Loss: 0.4021, Acc: 85.39%\n",
            "Epoch  800/1000, Train Loss: 0.4460, Acc: 80.93% | Val Loss: 0.4163, Acc: 85.96%\n",
            "Epoch  900/1000, Train Loss: 0.4632, Acc: 80.08% | Val Loss: 0.4255, Acc: 85.96%\n",
            "Epoch 1000/1000, Train Loss: 0.4579, Acc: 80.08% | Val Loss: 0.4196, Acc: 82.58%\n",
            "\n",
            "--- Run Finished --- Final Val Acc: 82.58%\n",
            "\n",
            "========================= Testing Batch Size: 32 =========================\n",
            "\n",
            "Train size: 713, Validation size: 178\n",
            "\n",
            "--- Testing Activation: Sigmoid ---\n",
            "--- Run Config ---\n",
            "{'epochs': 1000, 'batch_size': 32, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'Sigmoid'}\n",
            "\n",
            "Start Training for 1000 epochs...\n",
            "Epoch    1/1000, Train Loss: 0.6702, Acc: 59.33% | Val Loss: 0.6215, Acc: 70.79%\n",
            "Epoch  100/1000, Train Loss: 0.6571, Acc: 59.33% | Val Loss: 0.6204, Acc: 70.79%\n",
            "Epoch  200/1000, Train Loss: 0.6461, Acc: 59.33% | Val Loss: 0.6160, Acc: 70.79%\n",
            "Epoch  300/1000, Train Loss: 0.6355, Acc: 59.33% | Val Loss: 0.6127, Acc: 70.79%\n",
            "Epoch  400/1000, Train Loss: 0.6259, Acc: 68.16% | Val Loss: 0.6105, Acc: 67.42%\n",
            "Epoch  500/1000, Train Loss: 0.6179, Acc: 68.72% | Val Loss: 0.6095, Acc: 67.98%\n",
            "Epoch  600/1000, Train Loss: 0.6116, Acc: 69.14% | Val Loss: 0.6107, Acc: 66.85%\n",
            "Epoch  700/1000, Train Loss: 0.6070, Acc: 69.28% | Val Loss: 0.6127, Acc: 66.85%\n",
            "Epoch  800/1000, Train Loss: 0.6039, Acc: 69.00% | Val Loss: 0.6149, Acc: 66.29%\n",
            "Epoch  900/1000, Train Loss: 0.6018, Acc: 68.86% | Val Loss: 0.6156, Acc: 66.29%\n",
            "Epoch 1000/1000, Train Loss: 0.6003, Acc: 68.86% | Val Loss: 0.6155, Acc: 66.29%\n",
            "\n",
            "--- Run Finished --- Final Val Acc: 66.29%\n",
            "\n",
            "--- Testing Activation: ReLU ---\n",
            "--- Run Config ---\n",
            "{'epochs': 1000, 'batch_size': 32, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'ReLU'}\n",
            "\n",
            "Start Training for 1000 epochs...\n",
            "Epoch    1/1000, Train Loss: 0.8389, Acc: 62.69% | Val Loss: 0.6606, Acc: 69.66%\n",
            "Epoch  100/1000, Train Loss: 0.5963, Acc: 68.30% | Val Loss: 0.6125, Acc: 66.85%\n",
            "Epoch  200/1000, Train Loss: 0.5846, Acc: 69.85% | Val Loss: 0.5764, Acc: 71.91%\n",
            "Epoch  300/1000, Train Loss: 0.5750, Acc: 70.13% | Val Loss: 0.5765, Acc: 71.35%\n",
            "Epoch  400/1000, Train Loss: 0.5624, Acc: 71.11% | Val Loss: 0.5561, Acc: 71.91%\n",
            "Epoch  500/1000, Train Loss: 0.5548, Acc: 71.39% | Val Loss: 0.5438, Acc: 73.03%\n",
            "Epoch  600/1000, Train Loss: 0.5425, Acc: 71.81% | Val Loss: 0.5037, Acc: 77.53%\n",
            "Epoch  700/1000, Train Loss: 0.5258, Acc: 72.37% | Val Loss: 0.4951, Acc: 74.16%\n",
            "Epoch  800/1000, Train Loss: 0.5344, Acc: 72.65% | Val Loss: 0.6841, Acc: 72.47%\n",
            "Epoch  900/1000, Train Loss: 0.4895, Acc: 76.30% | Val Loss: 0.4453, Acc: 79.21%\n",
            "Epoch 1000/1000, Train Loss: 0.4813, Acc: 77.56% | Val Loss: 0.5268, Acc: 76.97%\n",
            "\n",
            "--- Run Finished --- Final Val Acc: 76.97%\n",
            "\n",
            "--- Testing Activation: ELU ---\n",
            "--- Run Config ---\n",
            "{'epochs': 1000, 'batch_size': 32, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'ELU'}\n",
            "\n",
            "Start Training for 1000 epochs...\n",
            "Epoch    1/1000, Train Loss: 0.7839, Acc: 50.49% | Val Loss: 0.6481, Acc: 69.66%\n",
            "Epoch  100/1000, Train Loss: 0.5661, Acc: 74.33% | Val Loss: 0.5788, Acc: 71.35%\n",
            "Epoch  200/1000, Train Loss: 0.5487, Acc: 73.77% | Val Loss: 0.5451, Acc: 75.28%\n",
            "Epoch  300/1000, Train Loss: 0.5320, Acc: 73.35% | Val Loss: 0.5467, Acc: 71.91%\n",
            "Epoch  400/1000, Train Loss: 0.5108, Acc: 73.77% | Val Loss: 0.6092, Acc: 70.79%\n",
            "Epoch  500/1000, Train Loss: 0.5061, Acc: 74.61% | Val Loss: 0.5957, Acc: 76.40%\n",
            "Epoch  600/1000, Train Loss: 0.4784, Acc: 77.84% | Val Loss: 0.5019, Acc: 80.34%\n",
            "Epoch  700/1000, Train Loss: 0.4786, Acc: 79.10% | Val Loss: 0.4330, Acc: 78.65%\n",
            "Epoch  800/1000, Train Loss: 0.4626, Acc: 79.94% | Val Loss: 0.5948, Acc: 75.28%\n",
            "Epoch  900/1000, Train Loss: 0.4561, Acc: 79.10% | Val Loss: 0.4307, Acc: 79.21%\n",
            "Epoch 1000/1000, Train Loss: 0.4568, Acc: 79.80% | Val Loss: 0.4402, Acc: 79.21%\n",
            "\n",
            "--- Run Finished --- Final Val Acc: 79.21%\n",
            "\n",
            "--- Testing Activation: LeakyReLU ---\n",
            "--- Run Config ---\n",
            "{'epochs': 1000, 'batch_size': 32, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'LeakyReLU'}\n",
            "\n",
            "Start Training for 1000 epochs...\n",
            "Epoch    1/1000, Train Loss: 1.0302, Acc: 34.64% | Val Loss: 0.9873, Acc: 28.09%\n",
            "Epoch  100/1000, Train Loss: 0.5787, Acc: 71.53% | Val Loss: 0.5802, Acc: 73.60%\n",
            "Epoch  200/1000, Train Loss: 0.5673, Acc: 71.81% | Val Loss: 0.5702, Acc: 71.91%\n",
            "Epoch  300/1000, Train Loss: 0.5508, Acc: 72.23% | Val Loss: 0.5877, Acc: 73.60%\n",
            "Epoch  400/1000, Train Loss: 0.5372, Acc: 72.79% | Val Loss: 0.5546, Acc: 71.91%\n",
            "Epoch  500/1000, Train Loss: 0.5224, Acc: 73.21% | Val Loss: 0.5551, Acc: 72.47%\n",
            "Epoch  600/1000, Train Loss: 0.5070, Acc: 73.35% | Val Loss: 0.5302, Acc: 74.16%\n",
            "Epoch  700/1000, Train Loss: 0.4796, Acc: 77.56% | Val Loss: 0.5382, Acc: 73.60%\n",
            "Epoch  800/1000, Train Loss: 0.4693, Acc: 77.70% | Val Loss: 0.4648, Acc: 77.53%\n",
            "Epoch  900/1000, Train Loss: 0.4517, Acc: 78.26% | Val Loss: 0.4511, Acc: 80.90%\n",
            "Epoch 1000/1000, Train Loss: 0.4621, Acc: 78.82% | Val Loss: 0.5774, Acc: 76.97%\n",
            "\n",
            "--- Run Finished --- Final Val Acc: 76.97%\n",
            "\n",
            "========================= Testing Batch Size: 64 =========================\n",
            "\n",
            "Train size: 713, Validation size: 178\n",
            "\n",
            "--- Testing Activation: Sigmoid ---\n",
            "--- Run Config ---\n",
            "{'epochs': 1000, 'batch_size': 64, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'Sigmoid'}\n",
            "\n",
            "Start Training for 1000 epochs...\n",
            "Epoch    1/1000, Train Loss: 0.6736, Acc: 59.33% | Val Loss: 0.6318, Acc: 70.79%\n",
            "Epoch  100/1000, Train Loss: 0.6677, Acc: 59.33% | Val Loss: 0.6300, Acc: 70.79%\n",
            "Epoch  200/1000, Train Loss: 0.6628, Acc: 59.33% | Val Loss: 0.6271, Acc: 70.79%\n",
            "Epoch  300/1000, Train Loss: 0.6578, Acc: 59.33% | Val Loss: 0.6238, Acc: 70.79%\n",
            "Epoch  400/1000, Train Loss: 0.6530, Acc: 59.33% | Val Loss: 0.6220, Acc: 70.79%\n",
            "Epoch  500/1000, Train Loss: 0.6479, Acc: 59.33% | Val Loss: 0.6200, Acc: 70.79%\n",
            "Epoch  600/1000, Train Loss: 0.6421, Acc: 59.33% | Val Loss: 0.6192, Acc: 70.79%\n",
            "Epoch  700/1000, Train Loss: 0.6366, Acc: 64.38% | Val Loss: 0.6167, Acc: 71.35%\n",
            "Epoch  800/1000, Train Loss: 0.6319, Acc: 65.78% | Val Loss: 0.6134, Acc: 69.10%\n",
            "Epoch  900/1000, Train Loss: 0.6269, Acc: 67.60% | Val Loss: 0.6124, Acc: 68.54%\n",
            "Epoch 1000/1000, Train Loss: 0.6219, Acc: 68.86% | Val Loss: 0.6117, Acc: 69.66%\n",
            "\n",
            "--- Run Finished --- Final Val Acc: 69.66%\n",
            "\n",
            "--- Testing Activation: ReLU ---\n",
            "--- Run Config ---\n",
            "{'epochs': 1000, 'batch_size': 64, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'ReLU'}\n",
            "\n",
            "Start Training for 1000 epochs...\n",
            "Epoch    1/1000, Train Loss: 0.7264, Acc: 38.99% | Val Loss: 0.7226, Acc: 35.96%\n",
            "Epoch  100/1000, Train Loss: 0.6095, Acc: 69.28% | Val Loss: 0.5938, Acc: 71.91%\n",
            "Epoch  200/1000, Train Loss: 0.6016, Acc: 70.69% | Val Loss: 0.5950, Acc: 70.22%\n",
            "Epoch  300/1000, Train Loss: 0.5977, Acc: 70.83% | Val Loss: 0.6009, Acc: 70.79%\n",
            "Epoch  400/1000, Train Loss: 0.5936, Acc: 70.69% | Val Loss: 0.5888, Acc: 70.79%\n",
            "Epoch  500/1000, Train Loss: 0.5908, Acc: 70.27% | Val Loss: 0.5867, Acc: 70.79%\n",
            "Epoch  600/1000, Train Loss: 0.5869, Acc: 70.83% | Val Loss: 0.5579, Acc: 71.35%\n",
            "Epoch  700/1000, Train Loss: 0.5833, Acc: 71.11% | Val Loss: 0.5754, Acc: 71.35%\n",
            "Epoch  800/1000, Train Loss: 0.5796, Acc: 70.83% | Val Loss: 0.5731, Acc: 71.35%\n",
            "Epoch  900/1000, Train Loss: 0.5769, Acc: 71.67% | Val Loss: 0.5810, Acc: 70.22%\n",
            "Epoch 1000/1000, Train Loss: 0.5727, Acc: 71.11% | Val Loss: 0.5629, Acc: 71.35%\n",
            "\n",
            "--- Run Finished --- Final Val Acc: 71.35%\n",
            "\n",
            "--- Testing Activation: ELU ---\n",
            "--- Run Config ---\n",
            "{'epochs': 1000, 'batch_size': 64, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'ELU'}\n",
            "\n",
            "Start Training for 1000 epochs...\n",
            "Epoch    1/1000, Train Loss: 1.0840, Acc: 36.33% | Val Loss: 0.8298, Acc: 29.78%\n",
            "Epoch  100/1000, Train Loss: 0.5953, Acc: 68.72% | Val Loss: 0.5960, Acc: 69.10%\n",
            "Epoch  200/1000, Train Loss: 0.5861, Acc: 70.55% | Val Loss: 0.5937, Acc: 68.54%\n",
            "Epoch  300/1000, Train Loss: 0.5770, Acc: 72.65% | Val Loss: 0.5614, Acc: 74.72%\n",
            "Epoch  400/1000, Train Loss: 0.5719, Acc: 73.21% | Val Loss: 0.5637, Acc: 73.60%\n",
            "Epoch  500/1000, Train Loss: 0.5663, Acc: 73.21% | Val Loss: 0.5569, Acc: 73.60%\n",
            "Epoch  600/1000, Train Loss: 0.5562, Acc: 73.35% | Val Loss: 0.5430, Acc: 74.72%\n",
            "Epoch  700/1000, Train Loss: 0.5529, Acc: 73.77% | Val Loss: 0.5739, Acc: 70.22%\n",
            "Epoch  800/1000, Train Loss: 0.5397, Acc: 75.04% | Val Loss: 0.5464, Acc: 72.47%\n",
            "Epoch  900/1000, Train Loss: 0.5310, Acc: 73.91% | Val Loss: 0.5050, Acc: 76.97%\n",
            "Epoch 1000/1000, Train Loss: 0.5193, Acc: 74.33% | Val Loss: 0.4839, Acc: 77.53%\n",
            "\n",
            "--- Run Finished --- Final Val Acc: 77.53%\n",
            "\n",
            "--- Testing Activation: LeakyReLU ---\n",
            "--- Run Config ---\n",
            "{'epochs': 1000, 'batch_size': 64, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'LeakyReLU'}\n",
            "\n",
            "Start Training for 1000 epochs...\n",
            "Epoch    1/1000, Train Loss: 0.8592, Acc: 41.09% | Val Loss: 0.8829, Acc: 28.09%\n",
            "Epoch  100/1000, Train Loss: 0.5991, Acc: 68.02% | Val Loss: 0.6108, Acc: 66.85%\n",
            "Epoch  200/1000, Train Loss: 0.5956, Acc: 67.88% | Val Loss: 0.5952, Acc: 70.22%\n",
            "Epoch  300/1000, Train Loss: 0.5919, Acc: 69.00% | Val Loss: 0.6001, Acc: 69.10%\n",
            "Epoch  400/1000, Train Loss: 0.5874, Acc: 69.57% | Val Loss: 0.5970, Acc: 69.66%\n",
            "Epoch  500/1000, Train Loss: 0.5842, Acc: 69.42% | Val Loss: 0.5794, Acc: 71.91%\n",
            "Epoch  600/1000, Train Loss: 0.5808, Acc: 70.13% | Val Loss: 0.5857, Acc: 71.35%\n",
            "Epoch  700/1000, Train Loss: 0.5769, Acc: 70.13% | Val Loss: 0.5738, Acc: 71.91%\n",
            "Epoch  800/1000, Train Loss: 0.5729, Acc: 69.85% | Val Loss: 0.5719, Acc: 71.91%\n",
            "Epoch  900/1000, Train Loss: 0.5706, Acc: 69.14% | Val Loss: 0.5838, Acc: 71.35%\n",
            "Epoch 1000/1000, Train Loss: 0.5661, Acc: 69.00% | Val Loss: 0.5709, Acc: 71.35%\n",
            "\n",
            "--- Run Finished --- Final Val Acc: 71.35%\n",
            "\n",
            "========================= Testing Batch Size: 128 =========================\n",
            "\n",
            "Train size: 713, Validation size: 178\n",
            "\n",
            "--- Testing Activation: Sigmoid ---\n",
            "--- Run Config ---\n",
            "{'epochs': 1000, 'batch_size': 128, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'Sigmoid'}\n",
            "\n",
            "Start Training for 1000 epochs...\n",
            "Epoch    1/1000, Train Loss: 0.8306, Acc: 40.67% | Val Loss: 0.9114, Acc: 29.21%\n",
            "Epoch  100/1000, Train Loss: 0.6670, Acc: 59.33% | Val Loss: 0.6379, Acc: 70.79%\n",
            "Epoch  200/1000, Train Loss: 0.6635, Acc: 59.33% | Val Loss: 0.6240, Acc: 70.79%\n",
            "Epoch  300/1000, Train Loss: 0.6614, Acc: 59.33% | Val Loss: 0.6221, Acc: 70.79%\n",
            "Epoch  400/1000, Train Loss: 0.6593, Acc: 59.33% | Val Loss: 0.6208, Acc: 70.79%\n",
            "Epoch  500/1000, Train Loss: 0.6570, Acc: 59.33% | Val Loss: 0.6201, Acc: 70.79%\n",
            "Epoch  600/1000, Train Loss: 0.6547, Acc: 59.33% | Val Loss: 0.6191, Acc: 70.79%\n",
            "Epoch  700/1000, Train Loss: 0.6523, Acc: 59.33% | Val Loss: 0.6182, Acc: 70.79%\n",
            "Epoch  800/1000, Train Loss: 0.6499, Acc: 59.33% | Val Loss: 0.6171, Acc: 70.79%\n",
            "Epoch  900/1000, Train Loss: 0.6475, Acc: 59.33% | Val Loss: 0.6158, Acc: 70.79%\n",
            "Epoch 1000/1000, Train Loss: 0.6451, Acc: 59.33% | Val Loss: 0.6150, Acc: 70.79%\n",
            "\n",
            "--- Run Finished --- Final Val Acc: 70.79%\n",
            "\n",
            "--- Testing Activation: ReLU ---\n",
            "--- Run Config ---\n",
            "{'epochs': 1000, 'batch_size': 128, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'ReLU'}\n",
            "\n",
            "Start Training for 1000 epochs...\n",
            "Epoch    1/1000, Train Loss: 0.6790, Acc: 60.73% | Val Loss: 0.6030, Acc: 70.79%\n",
            "Epoch  100/1000, Train Loss: 0.6034, Acc: 68.30% | Val Loss: 0.5948, Acc: 68.54%\n",
            "Epoch  200/1000, Train Loss: 0.5987, Acc: 68.72% | Val Loss: 0.5999, Acc: 66.85%\n",
            "Epoch  300/1000, Train Loss: 0.5963, Acc: 69.14% | Val Loss: 0.5871, Acc: 70.79%\n",
            "Epoch  400/1000, Train Loss: 0.5932, Acc: 69.42% | Val Loss: 0.5815, Acc: 71.35%\n",
            "Epoch  500/1000, Train Loss: 0.5901, Acc: 69.71% | Val Loss: 0.5901, Acc: 68.54%\n",
            "Epoch  600/1000, Train Loss: 0.5873, Acc: 69.99% | Val Loss: 0.5883, Acc: 69.10%\n",
            "Epoch  700/1000, Train Loss: 0.5858, Acc: 69.57% | Val Loss: 0.5862, Acc: 68.54%\n",
            "Epoch  800/1000, Train Loss: 0.5821, Acc: 70.13% | Val Loss: 0.5810, Acc: 69.10%\n",
            "Epoch  900/1000, Train Loss: 0.5816, Acc: 69.71% | Val Loss: 0.5769, Acc: 69.10%\n",
            "Epoch 1000/1000, Train Loss: 0.5766, Acc: 70.69% | Val Loss: 0.5608, Acc: 70.79%\n",
            "\n",
            "--- Run Finished --- Final Val Acc: 70.79%\n",
            "\n",
            "--- Testing Activation: ELU ---\n",
            "--- Run Config ---\n",
            "{'epochs': 1000, 'batch_size': 128, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'ELU'}\n",
            "\n",
            "Start Training for 1000 epochs...\n",
            "Epoch    1/1000, Train Loss: 0.7889, Acc: 37.73% | Val Loss: 0.7697, Acc: 29.78%\n",
            "Epoch  100/1000, Train Loss: 0.5878, Acc: 70.97% | Val Loss: 0.5935, Acc: 70.22%\n",
            "Epoch  200/1000, Train Loss: 0.5808, Acc: 73.35% | Val Loss: 0.5812, Acc: 71.91%\n",
            "Epoch  300/1000, Train Loss: 0.5762, Acc: 73.77% | Val Loss: 0.5748, Acc: 71.35%\n",
            "Epoch  400/1000, Train Loss: 0.5717, Acc: 73.63% | Val Loss: 0.5705, Acc: 71.35%\n",
            "Epoch  500/1000, Train Loss: 0.5683, Acc: 73.77% | Val Loss: 0.5673, Acc: 71.91%\n",
            "Epoch  600/1000, Train Loss: 0.5639, Acc: 73.91% | Val Loss: 0.5600, Acc: 71.91%\n",
            "Epoch  700/1000, Train Loss: 0.5599, Acc: 73.35% | Val Loss: 0.5591, Acc: 71.91%\n",
            "Epoch  800/1000, Train Loss: 0.5558, Acc: 73.91% | Val Loss: 0.5552, Acc: 71.91%\n",
            "Epoch  900/1000, Train Loss: 0.5520, Acc: 73.77% | Val Loss: 0.5458, Acc: 71.91%\n",
            "Epoch 1000/1000, Train Loss: 0.5480, Acc: 74.19% | Val Loss: 0.5414, Acc: 72.47%\n",
            "\n",
            "--- Run Finished --- Final Val Acc: 72.47%\n",
            "\n",
            "--- Testing Activation: LeakyReLU ---\n",
            "--- Run Config ---\n",
            "{'epochs': 1000, 'batch_size': 128, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'LeakyReLU'}\n",
            "\n",
            "Start Training for 1000 epochs...\n",
            "Epoch    1/1000, Train Loss: 1.6794, Acc: 59.33% | Val Loss: 0.7952, Acc: 70.79%\n",
            "Epoch  100/1000, Train Loss: 0.5979, Acc: 71.95% | Val Loss: 0.6040, Acc: 69.10%\n",
            "Epoch  200/1000, Train Loss: 0.5929, Acc: 71.95% | Val Loss: 0.5968, Acc: 72.47%\n",
            "Epoch  300/1000, Train Loss: 0.5892, Acc: 71.81% | Val Loss: 0.5942, Acc: 71.35%\n",
            "Epoch  400/1000, Train Loss: 0.5869, Acc: 71.53% | Val Loss: 0.5924, Acc: 71.35%\n",
            "Epoch  500/1000, Train Loss: 0.5837, Acc: 71.67% | Val Loss: 0.5881, Acc: 71.35%\n",
            "Epoch  600/1000, Train Loss: 0.5813, Acc: 71.95% | Val Loss: 0.5849, Acc: 71.91%\n",
            "Epoch  700/1000, Train Loss: 0.5789, Acc: 72.23% | Val Loss: 0.5816, Acc: 71.91%\n",
            "Epoch  800/1000, Train Loss: 0.5764, Acc: 72.23% | Val Loss: 0.5748, Acc: 71.91%\n",
            "Epoch  900/1000, Train Loss: 0.5747, Acc: 71.95% | Val Loss: 0.5770, Acc: 71.35%\n",
            "Epoch 1000/1000, Train Loss: 0.5725, Acc: 71.81% | Val Loss: 0.5709, Acc: 71.91%\n",
            "\n",
            "--- Run Finished --- Final Val Acc: 71.91%\n",
            "\n",
            "============================================================\n",
            " Hyperparameter Tuning Finished (Config Only) \n",
            "============================================================\n",
            "Overall Best Final Val Acc: 83.71%\n",
            "Best Config Found: {'epochs': 1000, 'batch_size': 16, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'ReLU'}\n",
            "Best hyperparameters saved to best_hyperparameters.json\n",
            "============================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# --- 명령줄 인자 처리 및 실행 ---\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"Find best hyperparameters (config only) for Titanic.\")\n",
        "    parser.add_argument(\"--wandb\", action=argparse.BooleanOptionalAction, default=False, help=\"Enable WandB logging\")\n",
        "    parser.add_argument(\"-e\", \"--epochs\", type=int, default=1000, help=\"Epochs per combination (default: 1000)\")\n",
        "\n",
        "    # Jupyter 환경 감지 및 처리\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        print(\"Running in interactive mode. Using default args: epochs=1000, wandb=False\")\n",
        "        args = argparse.Namespace(wandb=False, epochs=1000) # 기본값 설정\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "\n",
        "    find_hyperparameters(args) # 메인 함수 실행"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48261013-ce45-4903-ab77-0003a7869364",
      "metadata": {
        "id": "48261013-ce45-4903-ab77-0003a7869364"
      },
      "source": [
        "![학습2](https://github.com/O-E2/deep_learning/blob/ee3a2d4c8d369f2970393fc38fefdefbeedc8e62/dl_parameters_wandb.png)\n",
        "\n",
        "Wandb URL\n",
        "https://wandb.ai/cyun0407-korea-university-of-technology-and-education/titanic_hyperparameter_tuning/workspace?nw=nwusercyun0407"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fca0c956-a814-452d-ae3f-a1ca7d93608e",
      "metadata": {
        "id": "fca0c956-a814-452d-ae3f-a1ca7d93608e"
      },
      "source": [
        "# [요구사항 3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05c8417e-85dd-4501-9fdc-5c81e21f23bf",
      "metadata": {
        "id": "05c8417e-85dd-4501-9fdc-5c81e21f23bf"
      },
      "outputs": [],
      "source": [
        "import os, json, sys, argparse, copy\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn, optim # optim은 재학습 시 필요\n",
        "from torch.utils.data import Dataset, DataLoader, random_split # random_split은 재학습 시 필요\n",
        "from sklearn.preprocessing import LabelEncoder # 전처리 함수에 필요\n",
        "from pathlib import Path\n",
        "\n",
        "class TitanicDataset(Dataset):\n",
        "    def __init__(self, X, y): self.X = torch.FloatTensor(X); self.y = torch.LongTensor(y)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return {'input': self.X[idx], 'target': self.y[idx]}\n",
        "    def __str__(self): return f\"Data Size: {len(self.X)}, Input Shape: {self.X.shape}, Target Shape: {self.y.shape}\"\n",
        "\n",
        "class TitanicTestDataset(Dataset):\n",
        "    def __init__(self, X): self.X = torch.FloatTensor(X)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return {'input': self.X[idx]}\n",
        "    def __str__(self): return f\"Data Size: {len(self.X)}, Input Shape: {self.X.shape}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "363e62f7-413d-4a46-8158-c3717471bee9",
      "metadata": {
        "id": "363e62f7-413d-4a46-8158-c3717471bee9"
      },
      "outputs": [],
      "source": [
        "# ... 데이터 전처리 보조 함수 get_preprocessed_dataset_1 ~ _6 정의\n",
        "def get_preprocessed_dataset_1(all_df): # ... (동일) ...\n",
        "    Fare_mean = all_df[[\"Pclass\", \"Fare\"]].groupby(\"Pclass\").mean().reset_index(); Fare_mean.columns = [\"Pclass\", \"Fare_mean\"]\n",
        "    all_df = pd.merge(all_df, Fare_mean, on=\"Pclass\", how=\"left\"); all_df.loc[all_df[\"Fare\"].isnull(), \"Fare\"] = all_df[\"Fare_mean\"]\n",
        "    return all_df.drop(columns=[\"Fare_mean\"])\n",
        "# get_preprocessed_dataset_2 ~ _6 정의\n",
        "def get_preprocessed_dataset_2(all_df):\n",
        "    name_df = all_df[\"Name\"].str.split(\"[,.]\", n=2, expand=True)\n",
        "    if name_df.shape[1] == 3:\n",
        "        name_df.columns = [\"family_name\", \"title\", \"name\"]; name_df[\"family_name\"] = name_df[\"family_name\"].str.strip(); name_df[\"title\"] = name_df[\"title\"].str.strip(); name_df[\"name\"] = name_df[\"name\"].str.strip()\n",
        "        all_df = pd.concat([all_df, name_df], axis=1)\n",
        "    else: all_df['title'] = 'unknown'\n",
        "    return all_df\n",
        "def get_preprocessed_dataset_3(all_df):\n",
        "    if 'title' not in all_df.columns:\n",
        "        if all_df['Age'].isnull().any(): all_df['Age'] = all_df['Age'].fillna(all_df['Age'].median())\n",
        "        return all_df\n",
        "    title_age_median = all_df[[\"title\", \"Age\"]].groupby(\"title\").median().round().reset_index(); title_age_median.columns = [\"title\", \"title_age_median\"]\n",
        "    all_df = pd.merge(all_df, title_age_median, on=\"title\", how=\"left\"); all_df.loc[all_df[\"Age\"].isnull(), \"Age\"] = all_df[\"title_age_median\"]\n",
        "    if \"title_age_median\" in all_df.columns: all_df = all_df.drop([\"title_age_median\"], axis=1)\n",
        "    return all_df\n",
        "def get_preprocessed_dataset_4(all_df):\n",
        "    all_df[\"family_num\"] = all_df[\"Parch\"] + all_df[\"SibSp\"]; all_df.loc[all_df[\"family_num\"] == 0, \"alone\"] = 1; all_df[\"alone\"] = all_df[\"alone\"].fillna(0).astype(float)\n",
        "    cols_to_drop = [\"PassengerId\", \"Name\", \"family_name\", \"name\", \"Ticket\", \"Cabin\"]; existing_cols_to_drop = [col for col in cols_to_drop if col in all_df.columns]\n",
        "    return all_df.drop(existing_cols_to_drop, axis=1)\n",
        "def get_preprocessed_dataset_5(all_df):\n",
        "    if 'title' in all_df.columns: all_df.loc[~((all_df[\"title\"] == \"Mr\") | (all_df[\"title\"] == \"Miss\") | (all_df[\"title\"] == \"Mrs\") | (all_df[\"title\"] == \"Master\")), \"title\"] = \"other\"\n",
        "    all_df[\"Embarked\"] = all_df[\"Embarked\"].fillna(\"missing\")\n",
        "    return all_df\n",
        "def get_preprocessed_dataset_6(all_df):\n",
        "    category_features = all_df.select_dtypes(include=['object']).columns\n",
        "    for cat_feat in category_features:\n",
        "        le = LabelEncoder(); valid_indices = all_df[cat_feat].notna()\n",
        "        if valid_indices.any():\n",
        "            all_df.loc[valid_indices, cat_feat] = le.fit_transform(all_df.loc[valid_indices, cat_feat])\n",
        "            try: all_df[cat_feat] = pd.to_numeric(all_df[cat_feat])\n",
        "            except ValueError: pass\n",
        "    return all_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48a73b2f-02d4-4696-9f51-517520e7981a",
      "metadata": {
        "id": "48a73b2f-02d4-4696-9f51-517520e7981a"
      },
      "outputs": [],
      "source": [
        "def get_preprocessed_data_raw():\n",
        "    \"\"\"CSV 로드, 전처리, 최종 데이터 배열 반환 (테스트 ID 포함)\"\"\"\n",
        "    try: CURRENT_FILE_PATH = os.path.dirname(os.path.abspath(__file__))\n",
        "    except NameError: CURRENT_FILE_PATH = os.getcwd()\n",
        "    train_data_path = os.path.join(CURRENT_FILE_PATH, \"train.csv\")\n",
        "    test_data_path = os.path.join(CURRENT_FILE_PATH, \"test.csv\")\n",
        "    try:\n",
        "        train_df = pd.read_csv(train_data_path)\n",
        "        test_df = pd.read_csv(test_data_path)\n",
        "    except FileNotFoundError as e: print(f\"Error: {e}\"); sys.exit(1)\n",
        "\n",
        "    test_passenger_ids = test_df['PassengerId'] # 테스트 승객 ID 저장\n",
        "    all_df = pd.concat([train_df, test_df], sort=False)\n",
        "\n",
        "    # 전처리 단계 적용\n",
        "    all_df = get_preprocessed_dataset_1(all_df); all_df = get_preprocessed_dataset_2(all_df)\n",
        "    all_df = get_preprocessed_dataset_3(all_df); all_df = get_preprocessed_dataset_4(all_df)\n",
        "    all_df = get_preprocessed_dataset_5(all_df); all_df = get_preprocessed_dataset_6(all_df)\n",
        "\n",
        "    # 학습/테스트 분리\n",
        "    train_X_df = all_df[~all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1)\n",
        "    train_y = train_df[\"Survived\"]\n",
        "    test_X_df = all_df[all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1)\n",
        "\n",
        "    # 최종 데이터 확인 및 NaN 처리 (분리된 데이터프레임에 각각 적용)\n",
        "    print(\"\\n--- Final Data Check & Fill NaNs ---\")\n",
        "    columns_to_check = train_X_df.columns\n",
        "    for df in [train_X_df, test_X_df]:\n",
        "        df_name = \"Train\" if df is train_X_df else \"Test\"\n",
        "        for col in columns_to_check:\n",
        "            if df[col].dtype == 'object':\n",
        "                try: df[col] = pd.to_numeric(df[col])\n",
        "                except ValueError: df[col] = 0 # 변환 불가시 0으로 채움\n",
        "            if df[col].isnull().any(): df[col] = df[col].fillna(0) # NaN 0으로 채움\n",
        "\n",
        "    # 최종 인덱스 리셋 및 Numpy 배열 변환\n",
        "    train_X = train_X_df.reset_index(drop=True).values\n",
        "    test_X = test_X_df.reset_index(drop=True).values\n",
        "    train_y = train_y.values # Numpy 배열로 변환\n",
        "\n",
        "    print(\"Data preprocessing finished for submission generation.\")\n",
        "    return train_X, train_y, test_X, test_passenger_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c0a517c-3979-4a38-8208-ecd01771701b",
      "metadata": {
        "id": "6c0a517c-3979-4a38-8208-ecd01771701b"
      },
      "outputs": [],
      "source": [
        "# --- DataLoader 생성 함수 (get_dataloaders) ---\n",
        "def get_dataloaders(train_X, train_y, test_X, batch_size_config):\n",
        "    \"\"\"학습/검증/테스트 DataLoader 모두 생성\"\"\"\n",
        "    dataset = TitanicDataset(train_X, train_y)\n",
        "    generator = torch.Generator().manual_seed(42) # 시드 고정\n",
        "    train_dataset, validation_dataset = random_split(dataset, [0.8, 0.2], generator=generator)\n",
        "    test_dataset = TitanicTestDataset(test_X)\n",
        "\n",
        "    print(f\"\\nTrain size: {len(train_dataset)}, Validation size: {len(validation_dataset)}, Test size: {len(test_dataset)}\")\n",
        "\n",
        "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size_config, shuffle=True)\n",
        "    validation_loader = DataLoader(dataset=validation_dataset, batch_size=len(validation_dataset))\n",
        "    # 테스트 데이터 로더 생성 (비어있지 않을 때만)\n",
        "    test_loader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset)) if len(test_dataset) > 0 else None\n",
        "\n",
        "    return train_loader, validation_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e98f35b-459d-429c-8c94-e9811465ab99",
      "metadata": {
        "id": "7e98f35b-459d-429c-8c94-e9811465ab99"
      },
      "outputs": [],
      "source": [
        "# --- 신경망 모델 정의 (MyModel) ---\n",
        "class MyModel(nn.Module):\n",
        "    \"\"\"활성화 함수와 은닉층 크기를 인자로 받는 MLP 모델\"\"\"\n",
        "    def __init__(self, n_input, n_output, activation_fn_class, n_hidden1=20, n_hidden2=20):\n",
        "        super().__init__(); activation = activation_fn_class()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(n_input, n_hidden1), activation,\n",
        "            nn.Linear(n_hidden1, n_hidden2), activation,\n",
        "            nn.Linear(n_hidden2, n_output)\n",
        "        )\n",
        "    def forward(self, x): return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af291b7c-94cf-4f16-bde0-ad33f7af4fe1",
      "metadata": {
        "id": "af291b7c-94cf-4f16-bde0-ad33f7af4fe1"
      },
      "outputs": [],
      "source": [
        "# --- 모델 및 옵티마이저 생성 함수 (get_model_and_optimizer) ---\n",
        "def get_model_and_optimizer(activation_fn_class, learning_rate, n_hidden1=20, n_hidden2=20):\n",
        "    \"\"\"모델 객체와 SGD 옵티마이저 생성\"\"\"\n",
        "    model = MyModel(10, 2, activation_fn_class, n_hidden1, n_hidden2) # 입력 10개, 출력 2개 고정\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "    return model, optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0ae0167-8ade-44c5-ba7b-16519d5882bc",
      "metadata": {
        "id": "b0ae0167-8ade-44c5-ba7b-16519d5882bc"
      },
      "outputs": [],
      "source": [
        "# --- 모델 학습 및 검증 루프 ---\n",
        "def training_loop_with_early_stopping(model, optimizer, train_data_loader, validation_data_loader, n_epochs, loss_fn):\n",
        "    \"\"\"모델 학습/검증 수행 및 '최고 성능' 모델 상태 반환 (조기 종료 로직)\"\"\"\n",
        "    best_validation_accuracy = -1.0\n",
        "    best_model_state = None\n",
        "    best_epoch = 0\n",
        "\n",
        "    print(f\"\\nStart Final Training for {n_epochs} epochs (with early stopping logic)...\")\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        # --- 학습 ---\n",
        "        model.train(); loss_train_epoch = 0.0; correct_train_epoch = 0; total_train_samples = 0\n",
        "        for batch in train_data_loader:\n",
        "            input_data = batch['input']; target = batch['target']; optimizer.zero_grad(); output_train = model(input_data); loss = loss_fn(output_train, target); loss.backward(); optimizer.step()\n",
        "            loss_train_epoch += loss.item() * input_data.size(0); _, predicted = torch.max(output_train.data, 1); total_train_samples += target.size(0); correct_train_epoch += (predicted == target).sum().item()\n",
        "        avg_loss_train = loss_train_epoch / total_train_samples if total_train_samples else 0\n",
        "        train_accuracy = 100 * correct_train_epoch / total_train_samples if total_train_samples else 0\n",
        "\n",
        "        # --- 검증 ---\n",
        "        model.eval(); loss_val_epoch = 0.0; correct_val_epoch = 0; total_val_samples = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in validation_data_loader:\n",
        "                input_data = batch['input']; target = batch['target']; output_validation = model(input_data); loss = loss_fn(output_validation, target); loss_val_epoch += loss.item() * input_data.size(0)\n",
        "                _, predicted = torch.max(output_validation.data, 1); total_val_samples += target.size(0); correct_val_epoch += (predicted == target).sum().item()\n",
        "        avg_loss_validation = loss_val_epoch / total_val_samples if total_val_samples else 0\n",
        "        validation_accuracy = 100 * correct_val_epoch / total_val_samples if total_val_samples else 0\n",
        "\n",
        "        # 최고 성능 갱신 확인,조기 종료 로직\n",
        "        if validation_accuracy > best_validation_accuracy:\n",
        "            best_validation_accuracy = validation_accuracy\n",
        "            best_model_state = copy.deepcopy(model.state_dict())\n",
        "            best_epoch = epoch\n",
        "            print(f\" New best validation accuracy: {best_validation_accuracy:.2f}% at epoch {epoch}\")\n",
        "\n",
        "        # 주기적 출력\n",
        "        if epoch % 100 == 0 or epoch == 1:\n",
        "            print(f\"Epoch {epoch:>{len(str(n_epochs))}}/{n_epochs}, Train Loss: {avg_loss_train:.4f}, Acc: {train_accuracy:.2f}% | Val Loss: {avg_loss_validation:.4f}, Acc: {validation_accuracy:.2f}%{' *Best*' if epoch == best_epoch else ''}\")\n",
        "\n",
        "    print(f\"\\n--- Final Training Summary --- Best Val Acc: {best_validation_accuracy:.2f}% achieved at epoch {best_epoch}\")\n",
        "    # 최고 성능 모델 상태와 에포크 반환\n",
        "    return best_validation_accuracy, best_model_state, best_epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1110e837-5d69-443a-bff3-02b10c7aee6b",
      "metadata": {
        "id": "1110e837-5d69-443a-bff3-02b10c7aee6b"
      },
      "source": [
        "최적 하이퍼파라미터로 모델을 재학습시키는 함수입니다. 학습 중 매 에포크마다 검증 데이터 성능을 확인하고, 최고 성능을 보인 시점의 모델 상태와 에포크 번호를 기록하여 최종적으로 반환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dfaedad-038d-4fc9-ba30-2330f43071a9",
      "metadata": {
        "id": "4dfaedad-038d-4fc9-ba30-2330f43071a9"
      },
      "outputs": [],
      "source": [
        "# --- 테스트 함수 (test_model) ---\n",
        "# (find_best_hyperparameters.py 와 동일하게 정의)\n",
        "def test_model(model, test_data_loader):\n",
        "    \"\"\"학습된 모델로 테스트 데이터 예측 수행\"\"\"\n",
        "    model.eval(); predictions = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_data_loader: input_data = batch['input']; output = model(input_data); _, predicted = torch.max(output.data, 1); predictions.extend(predicted.cpu().numpy())\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b07bae1-78c8-411d-8943-36a641af838a",
      "metadata": {
        "id": "6b07bae1-78c8-411d-8943-36a641af838a"
      },
      "outputs": [],
      "source": [
        "# --- Submission 파일 생성 함수 (create_submission) ---\n",
        "# (find_best_hyperparameters.py 와 동일하게 정의)\n",
        "def create_submission(predictions, passenger_ids, output_file=\"submission.csv\"):\n",
        "    \"\"\"예측 결과와 승객 ID로 submission CSV 파일 생성\"\"\"\n",
        "    if len(predictions) != len(passenger_ids): print(f\"Error: Prediction count ({len(predictions)}) != Passenger ID count ({len(passenger_ids)})\"); return\n",
        "    submission_df = pd.DataFrame({\"PassengerId\": passenger_ids, \"Survived\": predictions})\n",
        "    try: submission_df.to_csv(output_file, index=False); print(f\"Submission file saved: {output_file}\")\n",
        "    except Exception as e: print(f\"Error saving submission file: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfc7e5e6-c574-4536-84d3-9adde74e523e",
      "metadata": {
        "id": "bfc7e5e6-c574-4536-84d3-9adde74e523e"
      },
      "outputs": [],
      "source": [
        "# --- 메인 실행 함수 (최적 모델 로드 및 예측) ---\n",
        "def generate_submission_main(config_path=\"best_hyperparameters.json\", retrain_epochs=None):\n",
        "    \"\"\"최적 설정 로드, 해당 설정으로 재학습(조기종료 적용), 예측, submission 파일 생성\"\"\"\n",
        "    # 최적 설정 로드\n",
        "    try:\n",
        "        with open(config_path, 'r') as f: best_config = json.load(f)\n",
        "    except FileNotFoundError: print(f\"Error: Config file '{config_path}' not found.\"); sys.exit(1)\n",
        "    except json.JSONDecodeError: print(f\"Error: Could not decode JSON from '{config_path}'.\"); sys.exit(1)\n",
        "\n",
        "    print(\"--- Loaded Best Config ---\"); print(best_config)\n",
        "\n",
        "    # 설정값 추출\n",
        "    best_activation_name = best_config.get('activation_function')\n",
        "    best_batch_size = best_config.get('batch_size')\n",
        "    learning_rate = best_config.get('learning_rate', 1e-3)\n",
        "    n_hidden1 = best_config.get('n_hidden_unit_list', [20, 20])[0]\n",
        "    n_hidden2 = best_config.get('n_hidden_unit_list', [20, 20])[1]\n",
        "    # 재학습 에포크 수 결정 (인자 우선 > config 값 > 기본값 1000)\n",
        "    epochs_to_train = retrain_epochs if retrain_epochs is not None else best_config.get('epochs', 1000)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 활성화 함수 클래스 가져오기\n",
        "    activation_functions_map = {\"Sigmoid\": nn.Sigmoid, \"ReLU\": nn.ReLU, \"ELU\": nn.ELU, \"LeakyReLU\": nn.LeakyReLU}\n",
        "    if not best_activation_name or best_activation_name not in activation_functions_map:\n",
        "        print(f\"Error: Invalid activation function '{best_activation_name}' in config.\"); sys.exit(1)\n",
        "    best_activation_class = activation_functions_map[best_activation_name]\n",
        "\n",
        "    print(\"\\nPreprocessing data for final training and submission...\")\n",
        "    train_X, train_y, test_X, test_passenger_ids = get_preprocessed_data_raw()\n",
        "\n",
        "    if not best_batch_size: print(\"Error: 'batch_size' not found in config.\"); sys.exit(1)\n",
        "    train_loader, validation_loader, test_loader = get_dataloaders(train_X, train_y, test_X, best_batch_size)\n",
        "    if test_loader is None: print(\"Error: Test data is empty, cannot generate submission.\"); sys.exit(1)\n",
        "\n",
        "    model, optimizer = get_model_and_optimizer(best_activation_class, learning_rate, n_hidden1, n_hidden2)\n",
        "\n",
        "    # === 최종 모델 재학습 (조기 종료 로직 적용) ===\n",
        "    final_best_accuracy, final_best_model_state, final_best_epoch = training_loop_with_early_stopping(\n",
        "        model, optimizer, train_loader, validation_loader, epochs_to_train, loss_fn\n",
        "    )\n",
        "\n",
        "    # 최고 성능 모델 상태 로드\n",
        "    if final_best_model_state:\n",
        "        model.load_state_dict(final_best_model_state)\n",
        "        print(f\"\\nLoaded model state from epoch {final_best_epoch} with validation accuracy {final_best_accuracy:.2f}%\")\n",
        "\n",
        "        # 예측 수행\n",
        "        print(\"Generating predictions with the best epoch model...\")\n",
        "        test_predictions = test_model(model, test_loader)\n",
        "\n",
        "        # Submission 파일 생성\n",
        "        submission_filename = f\"submission_final_{best_activation_name}_bs{best_batch_size}_epoch{final_best_epoch}.csv\"\n",
        "        create_submission(test_predictions, test_passenger_ids, output_file=submission_filename)\n",
        "    else:\n",
        "        print(\"Final training did not produce a best model state. Cannot generate submission.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f8c3355-f59c-4228-913d-b04eb6b9d761",
      "metadata": {
        "id": "4f8c3355-f59c-4228-913d-b04eb6b9d761"
      },
      "source": [
        "* best_hyperparameters.json 파일을 읽어 최적 조합 설정을 불러옵니다.\n",
        "\n",
        "* 전체 데이터를 다시 로드하고 최적 배치 크기로 DataLoader를 생성합니다.\n",
        "\n",
        "* 최적 활성화 함수로 모델과 옵티마이저를 생성합니다.\n",
        "\n",
        "* training_loop_with_early_stopping 함수를 호출하여 모델을 재학습시키고, 이 과정에서 검증 성능이 가장 좋았던 시점(조기 종료 시점)의 모델 상태와 에포크 번호를 얻습니다.\n",
        "\n",
        "* 얻어진 최적 시점의 모델 상태를 로드합니다.\n",
        "\n",
        "* test_model 함수로 테스트 데이터 예측을 수행합니다.\n",
        "\n",
        "* create_submission 함수로 최종 제출 파일을 생성합니다 (파일 이름에 최적 조합 및 에포크 포함)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdcff00d-75b7-4059-821b-12b1e508c14f",
      "metadata": {
        "id": "fdcff00d-75b7-4059-821b-12b1e508c14f",
        "outputId": "57efbcc9-3b93-48fa-c4b2-051b54e6e45a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running in interactive mode. Using default config path and epochs from config (or 1000).\n",
            "--- Loaded Best Config ---\n",
            "{'epochs': 1000, 'batch_size': 16, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_function': 'ReLU'}\n",
            "\n",
            "Preprocessing data for final training and submission...\n",
            "\n",
            "--- Final Data Check & Fill NaNs ---\n",
            "Data preprocessing finished for submission generation.\n",
            "\n",
            "Train size: 713, Validation size: 178, Test size: 418\n",
            "\n",
            "Start Final Training for 1000 epochs (with early stopping logic)...\n",
            " New best validation accuracy: 73.03% at epoch 1\n",
            "Epoch    1/1000, Train Loss: 0.6796, Acc: 61.71% | Val Loss: 0.5822, Acc: 73.03% *Best*\n",
            " New best validation accuracy: 73.60% at epoch 2\n",
            "Epoch  100/1000, Train Loss: 0.5821, Acc: 70.83% | Val Loss: 0.5581, Acc: 72.47%\n",
            " New best validation accuracy: 74.16% at epoch 198\n",
            "Epoch  200/1000, Train Loss: 0.5664, Acc: 71.67% | Val Loss: 0.5488, Acc: 71.35%\n",
            " New best validation accuracy: 75.28% at epoch 238\n",
            " New best validation accuracy: 76.40% at epoch 295\n",
            " New best validation accuracy: 76.97% at epoch 297\n",
            "Epoch  300/1000, Train Loss: 0.5509, Acc: 71.39% | Val Loss: 0.5355, Acc: 71.91%\n",
            " New best validation accuracy: 78.09% at epoch 359\n",
            " New best validation accuracy: 78.65% at epoch 375\n",
            "Epoch  400/1000, Train Loss: 0.5164, Acc: 74.89% | Val Loss: 0.5220, Acc: 74.72%\n",
            " New best validation accuracy: 79.21% at epoch 408\n",
            " New best validation accuracy: 79.78% at epoch 423\n",
            " New best validation accuracy: 80.34% at epoch 471\n",
            " New best validation accuracy: 80.90% at epoch 479\n",
            " New best validation accuracy: 82.02% at epoch 485\n",
            " New best validation accuracy: 84.27% at epoch 491\n",
            "Epoch  500/1000, Train Loss: 0.4872, Acc: 79.66% | Val Loss: 0.4902, Acc: 79.21%\n",
            " New best validation accuracy: 85.96% at epoch 513\n",
            " New best validation accuracy: 86.52% at epoch 539\n",
            "Epoch  600/1000, Train Loss: 0.4755, Acc: 78.68% | Val Loss: 0.5683, Acc: 75.84%\n",
            " New best validation accuracy: 87.08% at epoch 610\n",
            "Epoch  700/1000, Train Loss: 0.4557, Acc: 80.93% | Val Loss: 0.4164, Acc: 82.02%\n",
            "Epoch  800/1000, Train Loss: 0.4503, Acc: 79.80% | Val Loss: 0.4245, Acc: 83.15%\n",
            " New best validation accuracy: 87.64% at epoch 808\n",
            "Epoch  900/1000, Train Loss: 0.4456, Acc: 80.22% | Val Loss: 0.4110, Acc: 83.71%\n",
            "Epoch 1000/1000, Train Loss: 0.4528, Acc: 79.52% | Val Loss: 0.3969, Acc: 87.08%\n",
            "\n",
            "--- Final Training Summary --- Best Val Acc: 87.64% achieved at epoch 808\n",
            "\n",
            "Loaded model state from epoch 808 with validation accuracy 87.64%\n",
            "Generating predictions with the best epoch model...\n",
            "Submission file saved: submission_final_ReLU_bs16_epoch808.csv\n"
          ]
        }
      ],
      "source": [
        "# --- 명령줄 인자 처리 및 실행 ---\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"Retrain the best model with early stopping and generate submission.\")\n",
        "    parser.add_argument(\"--config\", default=\"best_hyperparameters.json\", help=\"Path to best hyperparameters JSON\")\n",
        "    # 재학습 에포크 수를 인자로 받을 수 있도록 추가 (선택사항)\n",
        "    parser.add_argument(\"-e\", \"--retrain_epochs\", type=int, default=None, help=\"Number of epochs for retraining (uses config value if not set)\")\n",
        "\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        print(\"Running in interactive mode. Using default config path and epochs from config (or 1000).\")\n",
        "        args = parser.parse_args([]) # 기본값 사용\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "\n",
        "    generate_submission_main(config_path=args.config, retrain_epochs=args.retrain_epochs) # 메인 함수 실행"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33182661-0876-4ba8-aeb9-e88dd078e835",
      "metadata": {
        "id": "33182661-0876-4ba8-aeb9-e88dd078e835"
      },
      "source": [
        "![캐글 제출 결과](https://github.com/O-E2/deep_learning/blob/0666538d13c2d34481a02ffd9c976f8979391f3e/dl_titanic_leaderboard.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c2a1186-65f9-4e65-9e2c-42ddddaf6112",
      "metadata": {
        "id": "4c2a1186-65f9-4e65-9e2c-42ddddaf6112"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}