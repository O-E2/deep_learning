{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29fcf32b-7b92-4cd3-8eec-b3d9e65b013d",
   "metadata": {},
   "source": [
    "# [요구사항 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e4879d-b21c-47e2-a824-38e9b5cd5b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "import argparse\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "class TitanicDataset(Dataset):\n",
    "  def __init__(self, X, y):\n",
    "    self.X = torch.FloatTensor(X)\n",
    "    self.y = torch.LongTensor(y)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    feature = self.X[idx]\n",
    "    target = self.y[idx]\n",
    "    return {'input': feature, 'target': target}\n",
    "\n",
    "  def __str__(self):\n",
    "    str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "      len(self.X), self.X.shape, self.y.shape\n",
    "    )\n",
    "    return str\n",
    "\n",
    "\n",
    "class TitanicTestDataset(Dataset):\n",
    "  def __init__(self, X):\n",
    "    self.X = torch.FloatTensor(X)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    feature = self.X[idx]\n",
    "    return {'input': feature}\n",
    "\n",
    "  def __str__(self):\n",
    "    str = \"Data Size: {0}, Input Shape: {1}\".format(\n",
    "      len(self.X), self.X.shape\n",
    "    )\n",
    "    return str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c95fbb-7f7d-4e0b-bdca-4fcba5346df1",
   "metadata": {},
   "source": [
    "__init__: 입력 데이터(특징 X, 타겟 y)를 파이토치가 사용하는 데이터 형식인 텐서(Tensor)로 변환합니다. 테스트 데이터셋은 y를 예측하는 것이 목표이므로 X만 가집니다.\n",
    "\n",
    "__len__: 데이터셋에 있는 총 샘플 수를 반환합니다.\n",
    "\n",
    "__getitem__: 인덱스(예: dataset[10])를 사용해 하나의 데이터 샘플(학습용은 특징과 타겟, 테스트용은 특징만)을 가져올 수 있게 합니다.\n",
    "\n",
    "__str__: print() 함수로 출력될 때 데이터셋 크기와 형태에 대한 간단한 문자열 설명을 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a86bed-144d-4055-a024-fe64d3067e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset():\n",
    "    # 스크립트가 실행되는 현재 파일 경로를 기준으로 CSV 파일 경로 설정\n",
    "    try:\n",
    "        CURRENT_FILE_PATH = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError:\n",
    "        # __file__이 정의되지 않은 환경(예: Jupyter)에서는 현재 작업 디렉터리를 사용\n",
    "        CURRENT_FILE_PATH = os.getcwd()\n",
    "\n",
    "    train_data_path = os.path.join(CURRENT_FILE_PATH, \"train.csv\")\n",
    "    test_data_path = os.path.join(CURRENT_FILE_PATH, \"test.csv\")\n",
    "\n",
    "    train_df = pd.read_csv(train_data_path)\n",
    "    test_df = pd.read_csv(test_data_path)\n",
    "\n",
    "    all_df = pd.concat([train_df, test_df], sort=False)\n",
    "\n",
    "    all_df = get_preprocessed_dataset_1(all_df)\n",
    "    all_df = get_preprocessed_dataset_2(all_df)\n",
    "    all_df = get_preprocessed_dataset_3(all_df)\n",
    "    all_df = get_preprocessed_dataset_4(all_df)\n",
    "    all_df = get_preprocessed_dataset_5(all_df)\n",
    "    all_df = get_preprocessed_dataset_6(all_df)\n",
    "\n",
    "    print(\"--- Preprocessed DataFrame Columns ---\")\n",
    "    print(all_df.columns)\n",
    "    print(\"--- Preprocessed DataFrame Head ---\")\n",
    "    print(all_df.head(5))\n",
    "\n",
    "    train_X = all_df[~all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1).reset_index(drop=True)\n",
    "    train_y = train_df[\"Survived\"]\n",
    "\n",
    "    test_X = all_df[all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1).reset_index(drop=True)\n",
    "\n",
    "    print(f\"\\nInput Features ({len(train_X.columns)}): {train_X.columns.tolist()}\")\n",
    "\n",
    "    dataset = TitanicDataset(train_X.values, train_y.values)\n",
    "    print(\"--- Full Train Dataset ---\")\n",
    "    print(dataset)\n",
    "\n",
    "    train_dataset, validation_dataset = random_split(dataset, [0.8, 0.2])\n",
    "    test_dataset = TitanicTestDataset(test_X.values)\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2658c92-0b0c-4da8-94c5-91f54cec0855",
   "metadata": {},
   "source": [
    "CSV 찾기 & 로딩: 판다스를 이용해 train.csv와 test.csv 파일을 찾아 읽습니다.\n",
    "\n",
    "결합: 학습 데이터와 테스트 데이터를 하나로 합칩니다. 이렇게 하면 전처리 단계(결측치 채우기, 범주 인코딩 등)를 양쪽 데이터에 일관되게 적용할 수 있습니다.\n",
    "\n",
    "전처리: 보조 함수들(_1부터 _6까지)을 순서대로 호출하여 데이터를 정제하고 변환합니다.\n",
    "\n",
    "분리: 결합했던 데이터를 다시 학습용 특징(train_X), 학습용 레이블(train_y), 테스트용 특징(test_X)으로 나눕니다.\n",
    "\n",
    "Dataset 생성: 사용자 정의 클래스(TitanicDataset, TitanicTestDataset)를 사용해 처리된 데이터를 감쌉니다.\n",
    "\n",
    "학습/검증 분할: 학습 데이터를 모델 훈련에 사용할 더 큰 세트와 훈련 중 성능 검증에 사용할 더 작은 세트로 나눕니다 (80/20 비율).\n",
    "\n",
    "반환: 파이토치의 DataLoader에서 바로 사용할 수 있는 최종 Dataset 객체들을 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fd7314-c2b5-4d11-a6c6-d11de2357e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset_1(all_df):\n",
    "    # Pclass별 Fare (요금) 평균값을 사용하여 Fare 결측치 메우기\n",
    "    Fare_mean = all_df[[\"Pclass\", \"Fare\"]].groupby(\"Pclass\").mean().reset_index()\n",
    "    Fare_mean.columns = [\"Pclass\", \"Fare_mean\"]\n",
    "    all_df = pd.merge(all_df, Fare_mean, on=\"Pclass\", how=\"left\")\n",
    "    all_df.loc[(all_df[\"Fare\"].isnull()), \"Fare\"] = all_df[\"Fare_mean\"]\n",
    "    all_df = all_df.drop(columns=[\"Fare_mean\"])\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_2(all_df):\n",
    "    # name을 세 개의 컬럼으로 분리하여 다시 all_df에 합침\n",
    "    name_df = all_df[\"Name\"].str.split(\"[,.]\", n=2, expand=True)\n",
    "    name_df.columns = [\"family_name\", \"title\", \"name\"]\n",
    "    name_df[\"family_name\"] = name_df[\"family_name\"].str.strip()\n",
    "    name_df[\"title\"] = name_df[\"title\"].str.strip()\n",
    "    name_df[\"name\"] = name_df[\"name\"].str.strip()\n",
    "    all_df = pd.concat([all_df, name_df], axis=1)\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_3(all_df):\n",
    "    # title별 Age 평균값을 사용하여 Age 결측치 메우기\n",
    "    title_age_mean = all_df[[\"title\", \"Age\"]].groupby(\"title\").median().round().reset_index()\n",
    "    title_age_mean.columns = [\"title\", \"title_age_mean\", ]\n",
    "    all_df = pd.merge(all_df, title_age_mean, on=\"title\", how=\"left\")\n",
    "    all_df.loc[(all_df[\"Age\"].isnull()), \"Age\"] = all_df[\"title_age_mean\"]\n",
    "    all_df = all_df.drop([\"title_age_mean\"], axis=1)\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_4(all_df):\n",
    "    # 가족수(family_num) 컬럼 새롭게 추가\n",
    "    all_df[\"family_num\"] = all_df[\"Parch\"] + all_df[\"SibSp\"]\n",
    "    # 혼자탑승(alone) 컬럼 새롭게 추가\n",
    "    all_df.loc[all_df[\"family_num\"] == 0, \"alone\"] = 1\n",
    "    all_df[\"alone\"].fillna(0, inplace=True)\n",
    "    # 학습에 불필요한 컬럼 제거\n",
    "    all_df = all_df.drop([\"PassengerId\", \"Name\", \"family_name\", \"name\", \"Ticket\", \"Cabin\"], axis=1)\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_5(all_df):\n",
    "    # title 값 개수 줄이기\n",
    "    all_df.loc[\n",
    "    ~(\n",
    "            (all_df[\"title\"] == \"Mr\") |\n",
    "            (all_df[\"title\"] == \"Miss\") |\n",
    "            (all_df[\"title\"] == \"Mrs\") |\n",
    "            (all_df[\"title\"] == \"Master\")\n",
    "    ),\n",
    "    \"title\"\n",
    "    ] = \"other\"\n",
    "    all_df[\"Embarked\"].fillna(\"missing\", inplace=True)\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_6(all_df):\n",
    "    # 카테고리 변수를 LabelEncoder를 사용하여 수치값으로 변경하기\n",
    "    category_features = all_df.columns[all_df.dtypes == \"object\"]\n",
    "    for category_feature in category_features:\n",
    "        le = LabelEncoder()\n",
    "        if all_df[category_feature].dtypes == \"object\":\n",
    "          le = le.fit(all_df[category_feature])\n",
    "          all_df[category_feature] = le.transform(all_df[category_feature])\n",
    "    return all_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fba863-8cde-4aec-a139-d0edd92f0f2f",
   "metadata": {},
   "source": [
    "_1: 누락된 요금 값을 각 승객 등급의 평균 요금으로 채웁니다. \n",
    "\n",
    "_2: Name 열에서 호칭을 추출합니다. \n",
    "\n",
    "_3: 누락된 나이값을 추출된 title과 연관된 나이의 중앙값으로 채웁니다. \n",
    "\n",
    "_4: 새로운 특징인 family_num과 alone을 만듭니다. 모델링에 불필요하다고 판단되는 열을 제거합니다.\n",
    "\n",
    "_5: 드문 호칭들을 'other'로 그룹화하여 title 열을 단순화합니다. 누락된 탑승 항구 값을 'missing'이라는 임시 값으로 채웁니다.\n",
    "\n",
    "_6: 범주형 문자열 열을 Label Encoding을 사용해 숫자 표현으로 변환합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "  # 1번 블록의 전처리 함수 호출 (이 함수는 다른 파일에 정의되어 있다고 가정)\n",
    "  train_dataset, validation_dataset, test_dataset = get_preprocessed_dataset()\n",
    "\n",
    "  print(f\"\\nTrain dataset size: {len(train_dataset)}\")\n",
    "  print(f\"Validation dataset size: {len(validation_dataset)}\")\n",
    "  print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "  # wandb.config에서 배치 크기를 가져와 DataLoader 생성\n",
    "  train_data_loader = DataLoader(dataset=train_dataset, batch_size=wandb.config.batch_size, shuffle=True)\n",
    "  validation_data_loader = DataLoader(dataset=validation_dataset, batch_size=len(validation_dataset))\n",
    "  test_data_loader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset))\n",
    "\n",
    "  return train_data_loader, validation_data_loader, test_data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a17a40-a384-43be-8bbb-1e24bfc112d3",
   "metadata": {},
   "source": [
    "데이터들을 DataLoader로 감싸서 모델 학습 시 데이터를 미니배치 단위로 효율적으로 공급할 수 있도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17d1c1c-9b50-4202-9183-476e204f2890",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "  def __init__(self, n_input, n_output):\n",
    "    super().__init__()\n",
    "\n",
    "    self.model = nn.Sequential(\n",
    "      nn.Linear(n_input, wandb.config.n_hidden_unit_list[0]),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(wandb.config.n_hidden_unit_list[0], wandb.config.n_hidden_unit_list[1]),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(wandb.config.n_hidden_unit_list[1], n_output),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.model(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_model_and_optimizer():\n",
    "  # 입력 피처 10개 (Pclass, Sex, Age, SibSp, Parch, Fare, Embarked, title, family_num, alone)\n",
    "  # 출력 클래스 2개 (0: 사망, 1: 생존)\n",
    "  my_model = MyModel(n_input=10, n_output=2)\n",
    "  optimizer = optim.SGD(my_model.parameters(), lr=wandb.config.learning_rate)\n",
    "\n",
    "  return my_model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157f3e50-c463-43a2-a0ac-af757451220e",
   "metadata": {},
   "source": [
    "__init__: 모델을 구성하는 층들을 정의합니다. 여기서는 입력층, 2개의 은닉층, 출력층으로 구성된 간단한 다층 퍼셉트론입니다. 각 층의 뉴런 수는 wandb.config에서 가져옵니다.\n",
    "\n",
    "forward: 입력 데이터(x)가 모델의 층들을 어떤 순서로 통과하여 최종 출력을 만들어내는지 정의합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca94715-daae-4c95-b640-0664499ea533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, optimizer, train_data_loader, validation_data_loader):\n",
    "  n_epochs = wandb.config.epochs\n",
    "  loss_fn = nn.CrossEntropyLoss()  # 분류 문제이므로 CrossEntropyLoss 사용\n",
    "  next_print_epoch = 100\n",
    "\n",
    "  for epoch in range(1, n_epochs + 1):\n",
    "    loss_train = 0.0\n",
    "    num_trains = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    model.train() # 모델을 학습 모드로 설정\n",
    "    for batch in train_data_loader:\n",
    "      # Dataset이 딕셔너리 형태이므로 키로 접근\n",
    "      input = batch['input']\n",
    "      target = batch['target']\n",
    "\n",
    "      output_train = model(input)\n",
    "      loss = loss_fn(output_train, target)\n",
    "      loss_train += loss.item()\n",
    "      num_trains += 1\n",
    "\n",
    "      # 정확도 계산\n",
    "      _, predicted = torch.max(output_train.data, 1)\n",
    "      total_train += target.size(0)\n",
    "      correct_train += (predicted == target).sum().item()\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    loss_validation = 0.0\n",
    "    num_validations = 0\n",
    "    correct_validation = 0\n",
    "    total_validation = 0\n",
    "\n",
    "    model.eval() # 모델을 평가 모드로 설정\n",
    "    with torch.no_grad():\n",
    "      for batch in validation_data_loader:\n",
    "        input = batch['input']\n",
    "        target = batch['target']\n",
    "\n",
    "        output_validation = model(input)\n",
    "        loss = loss_fn(output_validation, target)\n",
    "        loss_validation += loss.item()\n",
    "        num_validations += 1\n",
    "\n",
    "        # 정확도 계산\n",
    "        _, predicted = torch.max(output_validation.data, 1)\n",
    "        total_validation += target.size(0)\n",
    "        correct_validation += (predicted == target).sum().item()\n",
    "\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    validation_accuracy = 100 * correct_validation / total_validation\n",
    "\n",
    "    wandb.log({\n",
    "      \"Epoch\": epoch,\n",
    "      \"Training loss\": loss_train / num_trains,\n",
    "      \"Validation loss\": loss_validation / num_validations,\n",
    "      \"Training accuracy\": train_accuracy,\n",
    "      \"Validation accuracy\": validation_accuracy\n",
    "    })\n",
    "\n",
    "    if epoch % next_print_epoch == 0 or epoch == 1:\n",
    "      print(\n",
    "        f\"Epoch {epoch}, \"\n",
    "        f\"Training loss {loss_train / num_trains:.4f}, \"\n",
    "        f\"Validation loss {loss_validation / num_validations:.4f}, \"\n",
    "        f\"Training Acc {train_accuracy:.2f}%, \"\n",
    "        f\"Validation Acc {validation_accuracy:.2f}%\"\n",
    "      )\n",
    "      if epoch >= next_print_epoch:\n",
    "          next_print_epoch += 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38943a86-cdd2-48a4-924c-d406f05f6cba",
   "metadata": {},
   "source": [
    "* 에포크 반복: 정해진 횟수만큼 전체 데이터셋 학습을 반복합니다.\n",
    "\n",
    "* 학습 모드: model.train()으로 모델을 학습 상태로 설정합니다.\n",
    "\n",
    "* 미니배치 학습: train_data_loader에서 데이터를 미니배치 단위로 가져와 다음을 수행합니다.\n",
    "\n",
    "* 모델 예측 (model(input))\n",
    "\n",
    "* 손실 계산 (loss_fn)\n",
    "\n",
    "* 역전파 (loss.backward())\n",
    "\n",
    "* 가중치 업데이트 (optimizer.step())\n",
    "\n",
    "* 학습 손실과 정확도 누적 계산\n",
    "\n",
    "* 평가 모드: model.eval()으로 모델을 평가 상태로 설정합니다 (드롭아웃 등 비활성화).\n",
    "\n",
    "* 검증: validation_data_loader에서 데이터를 가져와 모델 예측을 수행하고, 검증 손실과 정확도를 계산합니다 (가중치 업데이트는 안 함).\n",
    "\n",
    "* 로깅: 각 에포크의 학습/검증 손실과 정확도를 wandb에 기록합니다.\n",
    "\n",
    "* 출력: 주기적으로 학습 진행 상황(손실, 정확도)을 화면에 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7edecf4-c9f1-4da6-9493-9dd64275b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "  current_time_str = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "  config = {\n",
    "    'epochs': args.epochs,\n",
    "    'batch_size': args.batch_size,\n",
    "    'learning_rate': 1e-3,\n",
    "    'n_hidden_unit_list': [20, 20], # 은닉층 설정은 그대로 사용\n",
    "  }\n",
    "\n",
    "  wandb.init(\n",
    "    mode=\"online\" if args.wandb else \"disabled\",\n",
    "    project=\"titanic_survival_prediction\", # wandb 프로젝트명 변경\n",
    "    notes=\"Titanic survival prediction with MLP\", # wandb 노트 변경\n",
    "    tags=[\"mlp\", \"titanic\"], # wandb 태그 변경\n",
    "    name=current_time_str,\n",
    "    config=config\n",
    "  )\n",
    "  print(\"--- wandb arguments ---\")\n",
    "  print(args)\n",
    "  print(\"--- wandb config ---\")\n",
    "  print(wandb.config)\n",
    "\n",
    "  # test_data_loader도 반환되지만, training_loop에서는 사용하지 않음\n",
    "  train_data_loader, validation_data_loader, test_data_loader = get_data()\n",
    "\n",
    "  linear_model, optimizer = get_model_and_optimizer()\n",
    "\n",
    "  print(\"\\n\" + \"#\" * 50)\n",
    "  print(\"Start Training...\")\n",
    "  training_loop(\n",
    "    model=linear_model,\n",
    "    optimizer=optimizer,\n",
    "    train_data_loader=train_data_loader,\n",
    "    validation_data_loader=validation_data_loader\n",
    "  )\n",
    "  print(\"Training Finished.\")\n",
    "  print(\"#\" * 50 + \"\\n\")\n",
    "\n",
    "  wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07456eb6-9fe5-4bf7-b9f5-174b145b372d",
   "metadata": {},
   "source": [
    "설정 로드: wandb 실험 설정(config)을 정의합니다 (에포크 수, 배치 크기 등).\n",
    "\n",
    "wandb 초기화: 실험 추적을 위해 wandb를 설정하고 시작합니다. 프로젝트 이름, 노트, 태그 등을 지정합니다.\n",
    "\n",
    "데이터 로딩: get_data() 함수를 호출하여 학습/검증/테스트 데이터 로더를 가져옵니다.\n",
    "\n",
    "모델/옵티마이저 생성: get_model_and_optimizer() 함수를 호출하여 모델과 옵티마이저를 준비합니다.\n",
    "\n",
    "학습 시작: training_loop() 함수를 호출하여 모델 학습 및 검증을 시작합니다.\n",
    "\n",
    "wandb 종료: 실험 기록을 마치고 wandb를 종료합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68787fd8-7b3e-4f84-b61d-b86b6b3fbf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  parser = argparse.ArgumentParser()\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"--wandb\", action=argparse.BooleanOptionalAction, default=False, help=\"True or False\"\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"-b\", \"--batch_size\", type=int, default=16, help=\"Batch size (int, default: 16)\" # 기본 배치 사이즈 16으로 변경\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"-e\", \"--epochs\", type=int, default=1_000, help=\"Number of training epochs (int, default:1_000)\"\n",
    "  )\n",
    "\n",
    "  args = parser.parse_args()\n",
    "\n",
    "  main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83765600-acf9-41c7-a241-8e4168faff7f",
   "metadata": {},
   "source": [
    "터미널에서 실행할 때 --epochs, --batch_size 같은 인자를 받을 수 있게 설정합니다. 받은 인자를 main 함수에 전달하여 실행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b2bc30-c744-4282-be83-da1778bd80d4",
   "metadata": {},
   "source": [
    "Wandb URL\n",
    "https://wandb.ai/cyun0407-korea-university-of-technology-and-education/titanic_survival_prediction/runs/i4pqcafr  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8138a5-2878-47bd-9a44-2853765f5a3c",
   "metadata": {},
   "source": [
    "[요구사항 2]\n",
    "\n",
    "Wansb URL \n",
    "https://wandb.ai/cyun0407-korea-university-of-technology-and-education/titanic_hyperparameter_tuning?nw=nwusercyun0407\n",
    "\n",
    "더 나은 성능을 산출하는 Activation Function : ReLU\n",
    "더 나은 성능을 산출하는 Batch Size : 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287d3b1d-cb29-48fc-b45e-6670319fe9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_best_hyperparameters.py\n",
    "import os, json, sys, argparse, copy\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pathlib import Path\n",
    "\n",
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, X, y): self.X = torch.FloatTensor(X); self.y = torch.LongTensor(y)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx): return {'input': self.X[idx], 'target': self.y[idx]}\n",
    "    def __str__(self): return f\"Data Size: {len(self.X)}, Input Shape: {self.X.shape}, Target Shape: {self.y.shape}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d55eed-b462-412b-86bc-1cff947973dc",
   "metadata": {},
   "source": [
    "파이토치 DataLoader가 사용할 수 있도록 데이터셋의 구조(데이터 로딩, 길이 반환, 특정 항목 접근 방법)를 정의합니다. TitanicDataset은 학습 및 검증 데이터를 다룹니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1814570a-a867-4058-b351-16f0fcd68e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_preprocessed_dataset_1(all_df): \n",
    "    Fare_mean = all_df[[\"Pclass\", \"Fare\"]].groupby(\"Pclass\").mean().reset_index(); Fare_mean.columns = [\"Pclass\", \"Fare_mean\"]\n",
    "    all_df = pd.merge(all_df, Fare_mean, on=\"Pclass\", how=\"left\"); all_df.loc[all_df[\"Fare\"].isnull(), \"Fare\"] = all_df[\"Fare_mean\"]\n",
    "    return all_df.drop(columns=[\"Fare_mean\"])\n",
    "def get_preprocessed_dataset_2(all_df): \n",
    "    name_df = all_df[\"Name\"].str.split(\"[,.]\", n=2, expand=True)\n",
    "    if name_df.shape[1] == 3:\n",
    "        name_df.columns = [\"family_name\", \"title\", \"name\"]; name_df[\"family_name\"] = name_df[\"family_name\"].str.strip(); name_df[\"title\"] = name_df[\"title\"].str.strip(); name_df[\"name\"] = name_df[\"name\"].str.strip()\n",
    "        all_df = pd.concat([all_df, name_df], axis=1)\n",
    "    else: all_df['title'] = 'unknown' # 이름 분리 실패 시 'unknown' title 추가\n",
    "    return all_df\n",
    "def get_preprocessed_dataset_3(all_df): \n",
    "    if 'title' not in all_df.columns:\n",
    "        if all_df['Age'].isnull().any(): all_df['Age'] = all_df['Age'].fillna(all_df['Age'].median())\n",
    "        return all_df\n",
    "    title_age_median = all_df[[\"title\", \"Age\"]].groupby(\"title\").median().round().reset_index(); title_age_median.columns = [\"title\", \"title_age_median\"]\n",
    "    all_df = pd.merge(all_df, title_age_median, on=\"title\", how=\"left\"); all_df.loc[all_df[\"Age\"].isnull(), \"Age\"] = all_df[\"title_age_median\"]\n",
    "    if \"title_age_median\" in all_df.columns: all_df = all_df.drop([\"title_age_median\"], axis=1)\n",
    "    return all_df\n",
    "def get_preprocessed_dataset_4(all_df): \n",
    "    all_df[\"family_num\"] = all_df[\"Parch\"] + all_df[\"SibSp\"]; all_df.loc[all_df[\"family_num\"] == 0, \"alone\"] = 1; all_df[\"alone\"] = all_df[\"alone\"].fillna(0).astype(float)\n",
    "    cols_to_drop = [\"PassengerId\", \"Name\", \"family_name\", \"name\", \"Ticket\", \"Cabin\"]; existing_cols_to_drop = [col for col in cols_to_drop if col in all_df.columns]\n",
    "    return all_df.drop(existing_cols_to_drop, axis=1)\n",
    "def get_preprocessed_dataset_5(all_df):\n",
    "    if 'title' in all_df.columns: all_df.loc[~((all_df[\"title\"] == \"Mr\") | (all_df[\"title\"] == \"Miss\") | (all_df[\"title\"] == \"Mrs\") | (all_df[\"title\"] == \"Master\")), \"title\"] = \"other\"\n",
    "    all_df[\"Embarked\"] = all_df[\"Embarked\"].fillna(\"missing\")\n",
    "    return all_df\n",
    "def get_preprocessed_dataset_6(all_df): \n",
    "    category_features = all_df.select_dtypes(include=['object']).columns\n",
    "    for cat_feat in category_features:\n",
    "        le = LabelEncoder(); valid_indices = all_df[cat_feat].notna()\n",
    "        if valid_indices.any():\n",
    "            all_df.loc[valid_indices, cat_feat] = le.fit_transform(all_df.loc[valid_indices, cat_feat])\n",
    "            try: all_df[cat_feat] = pd.to_numeric(all_df[cat_feat])\n",
    "            except ValueError: pass\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bdd747-11c6-4b6d-a3f3-11deb5c95ab7",
   "metadata": {},
   "source": [
    "타이타닉 원본 데이터를 단계별로 가공하는 함수들입니다. 결측치 처리, 특징 생성 및 추출, 범주형 데이터의 수치화 등을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ed857c-d889-4049-be19-e4349cee6b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_data_raw(): \n",
    "    \"\"\"CSV 로드, 전처리, 학습/검증용 데이터 배열 반환\"\"\"\n",
    "    try: CURRENT_FILE_PATH = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError: CURRENT_FILE_PATH = os.getcwd()\n",
    "    train_data_path = os.path.join(CURRENT_FILE_PATH, \"train.csv\")\n",
    "    test_data_path = os.path.join(CURRENT_FILE_PATH, \"test.csv\") # test는 로드만 하고 사용 안함\n",
    "    try:\n",
    "        train_df = pd.read_csv(train_data_path)\n",
    "        test_df = pd.read_csv(test_data_path)\n",
    "    except FileNotFoundError as e: print(f\"Error: {e}\"); sys.exit(1)\n",
    "\n",
    "    all_df = pd.concat([train_df, test_df], sort=False) # 전처리 일관성 위해 합침\n",
    "    all_df = get_preprocessed_dataset_1(all_df); all_df = get_preprocessed_dataset_2(all_df)\n",
    "    all_df = get_preprocessed_dataset_3(all_df); all_df = get_preprocessed_dataset_4(all_df)\n",
    "    all_df = get_preprocessed_dataset_5(all_df); all_df = get_preprocessed_dataset_6(all_df)\n",
    "    train_X_df = all_df[~all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1)\n",
    "    train_y = train_df[\"Survived\"]\n",
    "\n",
    "    # 학습 데이터에 대해서만 최종 NaN 및 타입 확인\n",
    "    print(\"\\n--- Final Data Check & Fill NaNs (Train Data Only) ---\")\n",
    "    columns_to_check = train_X_df.columns\n",
    "    df = train_X_df\n",
    "    for col in columns_to_check:\n",
    "        if df[col].dtype == 'object':\n",
    "            try: df[col] = pd.to_numeric(df[col])\n",
    "            except ValueError: df[col] = 0 # 변환 불가시 0으로 채움\n",
    "        if df[col].isnull().any(): df[col] = df[col].fillna(0) # NaN 0으로 채움\n",
    "    train_X = train_X_df.reset_index(drop=True)\n",
    "\n",
    "    print(\"Train data preprocessing finished.\")\n",
    "    return train_X.values, train_y.values # 학습 데이터(X, y)만 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc9033a-40eb-4db0-94e3-abb9f1127b09",
   "metadata": {},
   "source": [
    "CSV 파일을 로드하고 위의 전처리 함수들을 순서대로 호출합니다. 최종적으로 학습 데이터만 넘파이 배열 형태로 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bef9a48-c6d2-4eda-b441-0f4f570a9e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(train_X, train_y, batch_size_config): \n",
    "    \"\"\"학습/검증 DataLoader 생성\"\"\"\n",
    "    dataset = TitanicDataset(train_X, train_y)\n",
    "    generator = torch.Generator().manual_seed(42) # 데이터 분할 재현성 위한 시드 고정\n",
    "    train_dataset, validation_dataset = random_split(dataset, [0.8, 0.2], generator=generator)\n",
    "    print(f\"\\nTrain size: {len(train_dataset)}, Validation size: {len(validation_dataset)}\")\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size_config, shuffle=True)\n",
    "    validation_loader = DataLoader(dataset=validation_dataset, batch_size=len(validation_dataset)) # 검증은 전체 데이터 사용\n",
    "    return train_loader, validation_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3552c139-f80c-4061-8b6f-c789e8713075",
   "metadata": {},
   "source": [
    "전처리된 학습 데이터(X, y)와 배치 크기를 받아, 데이터를 학습용과 검증용으로 8:2 비율로 나눈 뒤 각각에 대한 DataLoader 객체를 생성하여 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a9fe2a-cfb2-4ad1-a35d-8a4ed5686014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module): \n",
    "    \"\"\"활성화 함수와 은닉층 크기를 인자로 받는 MLP 모델\"\"\"\n",
    "    def __init__(self, n_input, n_output, activation_fn_class, n_hidden1=20, n_hidden2=20):\n",
    "        super().__init__(); activation = activation_fn_class()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_input, n_hidden1), activation,\n",
    "            nn.Linear(n_hidden1, n_hidden2), activation,\n",
    "            nn.Linear(n_hidden2, n_output)\n",
    "        )\n",
    "    def forward(self, x): return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21456c8-02d3-4a63-a32c-af3be398f8f6",
   "metadata": {},
   "source": [
    "파이토치 nn.Module을 상속받아 간단한 모델 구조를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6302d8b-8c56-495f-90d5-579c118a788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_optimizer(activation_fn_class, learning_rate, n_hidden1=20, n_hidden2=20): # ... (이전과 동일) ...\n",
    "    \"\"\"모델 객체와 SGD 옵티마이저 생성\"\"\"\n",
    "    model = MyModel(10, 2, activation_fn_class, n_hidden1, n_hidden2) # 입력 10개, 출력 2개 고정\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303ad0e1-8893-42d9-8d1a-c90b0851a04c",
   "metadata": {},
   "source": [
    "MyModel 클래스를 이용해 모델 객체를 생성하고, 학습에 사용할 SGD 옵티마이저를 설정하여 함께 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7e520c-37a3-49ac-ad4a-084895580032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 학습 루프: 최종 정확도만 반환 ---\n",
    "def simplified_training_loop(model, optimizer, train_data_loader, validation_data_loader, n_epochs, loss_fn):\n",
    "    \"\"\"모델 학습/검증 수행 후 최종 검증 정확도 반환 (최고 기록 없음)\"\"\"\n",
    "    print(f\"\\nStart Training for {n_epochs} epochs...\")\n",
    "    final_validation_accuracy = 0.0 # 마지막 에포크 정확도 저장용\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # --- 학습 \n",
    "        model.train(); loss_train_epoch = 0.0; correct_train_epoch = 0; total_train_samples = 0\n",
    "        for batch in train_data_loader: # ... (학습 로직 동일) ...\n",
    "            input_data = batch['input']; target = batch['target']; optimizer.zero_grad(); output_train = model(input_data); loss = loss_fn(output_train, target); loss.backward(); optimizer.step()\n",
    "            loss_train_epoch += loss.item() * input_data.size(0); _, predicted = torch.max(output_train.data, 1); total_train_samples += target.size(0); correct_train_epoch += (predicted == target).sum().item()\n",
    "        avg_loss_train = loss_train_epoch / total_train_samples if total_train_samples else 0\n",
    "        train_accuracy = 100 * correct_train_epoch / total_train_samples if total_train_samples else 0\n",
    "\n",
    "        # --- 검증 \n",
    "        model.eval(); loss_val_epoch = 0.0; correct_val_epoch = 0; total_val_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in validation_data_loader: # ... (검증 로직 동일) ...\n",
    "                input_data = batch['input']; target = batch['target']; output_validation = model(input_data); loss = loss_fn(output_validation, target); loss_val_epoch += loss.item() * input_data.size(0)\n",
    "                _, predicted = torch.max(output_validation.data, 1); total_val_samples += target.size(0); correct_val_epoch += (predicted == target).sum().item()\n",
    "        avg_loss_validation = loss_val_epoch / total_val_samples if total_val_samples else 0\n",
    "        validation_accuracy = 100 * correct_val_epoch / total_val_samples if total_val_samples else 0\n",
    "        final_validation_accuracy = validation_accuracy # 마지막 값 갱신\n",
    "\n",
    "        # 로깅 및 출력 \n",
    "        if wandb.run: wandb.log({\"Epoch\": epoch, \"Training loss\": avg_loss_train, \"Validation loss\": avg_loss_validation, \"Training accuracy\": train_accuracy, \"Validation accuracy\": validation_accuracy})\n",
    "        if epoch % 100 == 0 or epoch == 1: print(f\"Epoch {epoch:>{len(str(n_epochs))}}/{n_epochs}, Train Loss: {avg_loss_train:.4f}, Acc: {train_accuracy:.2f}% | Val Loss: {avg_loss_validation:.4f}, Acc: {validation_accuracy:.2f}%\")\n",
    "\n",
    "    print(f\"\\n--- Run Finished --- Final Val Acc: {final_validation_accuracy:.2f}%\")\n",
    "    return final_validation_accuracy # 최종 검증 정확도만 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd276ad-6f18-44b4-9e7b-f2729f69d2b9",
   "metadata": {},
   "source": [
    "모델 학습과 검증을 지정된 에포크만큼 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21936d0-7ce6-4ab7-bc56-095143a45526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 메인 실행 함수 하이퍼파라미터 탐색 및 최적 조합 저장 ---\n",
    "def find_hyperparameters(args):\n",
    "    \"\"\"최적 하이퍼파라미터 '조합'만 찾아 json 파일로 저장\"\"\"\n",
    "    current_time_str = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    # 실험 설정\n",
    "    batch_sizes_to_test = [16, 32, 64, 128]\n",
    "    activation_functions_to_test = {\"Sigmoid\": nn.Sigmoid, \"ReLU\": nn.ReLU, \"ELU\": nn.ELU, \"LeakyReLU\": nn.LeakyReLU}\n",
    "    learning_rate = 1e-3; n_hidden_units = [20, 20]; loss_fn = nn.CrossEntropyLoss()\n",
    "    # 데이터 로딩\n",
    "    print(\"Preprocessing data for tuning...\"); train_X, train_y = get_preprocessed_data_raw()[:2] # 학습 데이터만 필요\n",
    "    # 최고 성능 추적\n",
    "    overall_best_accuracy = -1.0; overall_best_config = {}\n",
    "\n",
    "    # 실험 루프\n",
    "    for batch_size in batch_sizes_to_test:\n",
    "        print(f\"\\n{'='*25} Testing Batch Size: {batch_size} {'='*25}\")\n",
    "        train_loader, validation_loader = get_dataloaders(train_X, train_y, batch_size)\n",
    "        for activation_name, activation_fn_class in activation_functions_to_test.items():\n",
    "            print(f\"\\n--- Testing Activation: {activation_name} ---\")\n",
    "            config = {'epochs': args.epochs, 'batch_size': batch_size, 'learning_rate': learning_rate, 'n_hidden_unit_list': n_hidden_units, 'activation_function': activation_name}\n",
    "            run_name = f\"Tune_{activation_name}_BS{batch_size}_{current_time_str}\"\n",
    "            run = wandb.init(mode=\"online\" if args.wandb else \"disabled\", project=\"titanic_hyperparameter_tuning\", name=run_name, config=config, reinit=True)\n",
    "            print(\"--- Run Config ---\"); print(config)\n",
    "            model, optimizer = get_model_and_optimizer(activation_fn_class, config['learning_rate'], n_hidden1=config['n_hidden_unit_list'][0], n_hidden2=config['n_hidden_unit_list'][1])\n",
    "\n",
    "            # 간소화된 학습 루프 실행, 최종 정확도 받기\n",
    "            final_accuracy_run = simplified_training_loop(model, optimizer, train_loader, validation_loader, config['epochs'], loss_fn)\n",
    "\n",
    "            # 최고 조합 갱신 확인 (최종 정확도 기준)\n",
    "            if final_accuracy_run > overall_best_accuracy:\n",
    "                overall_best_accuracy = final_accuracy_run\n",
    "                overall_best_config = config # config만 저장\n",
    "                print(f\"🏅 New Best Config Found! Final Val Acc: {overall_best_accuracy:.2f}%, Config: {activation_name}/BS={batch_size}\")\n",
    "            run.finish()\n",
    "\n",
    "    # 최종 결과 요약 및 저장 (config만)\n",
    "    print(\"\\n\" + \"=\" * 60); print(\" Hyperparameter Tuning Finished (Config Only) \"); print(\"=\" * 60)\n",
    "    if overall_best_config:\n",
    "        print(f\"Overall Best Final Val Acc: {overall_best_accuracy:.2f}%\")\n",
    "        print(f\"Best Config Found: {overall_best_config}\")\n",
    "        best_config_path = \"best_hyperparameters.json\"\n",
    "        try:\n",
    "            with open(best_config_path, 'w') as f: json.dump(overall_best_config, f, indent=4)\n",
    "            print(f\"Best hyperparameters saved to {best_config_path}\")\n",
    "        except Exception as e: print(f\"Error saving best config: {e}\")\n",
    "    else: print(\"No successful training run recorded.\")\n",
    "    print(\"=\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db456e86-f7e7-4c8a-9dd2-0f785f5c4b33",
   "metadata": {},
   "source": [
    "지정된 배치 크기와 활성화 함수들의 모든 조합에 대해 루프를 돕니다. 각 조합마다 simplified_training_loop를 실행하여 최종 검증 정확도를 얻고, 이 정확도가 지금까지 기록된 최고 정확도보다 높으면 해당 조합의 설정을 overall_best_config에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3f2cbc-90c7-4fed-a961-230562e2a4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 명령줄 인자 처리 및 실행 ---\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Find best hyperparameters (config only) for Titanic.\")\n",
    "    parser.add_argument(\"--wandb\", action=argparse.BooleanOptionalAction, default=False, help=\"Enable WandB logging\")\n",
    "    parser.add_argument(\"-e\", \"--epochs\", type=int, default=1000, help=\"Epochs per combination (default: 1000)\")\n",
    "\n",
    "    # Jupyter 환경 감지 및 처리\n",
    "    if 'ipykernel' in sys.modules:\n",
    "        print(\"Running in interactive mode. Using default args: epochs=1000, wandb=False\")\n",
    "        args = argparse.Namespace(wandb=False, epochs=1000) # 기본값 설정\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    find_hyperparameters(args) # 메인 함수 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca0c956-a814-452d-ae3f-a1ca7d93608e",
   "metadata": {},
   "source": [
    "# [요구사항 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c8417e-85dd-4501-9fdc-5c81e21f23bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, sys, argparse, copy\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim # optim은 재학습 시 필요\n",
    "from torch.utils.data import Dataset, DataLoader, random_split # random_split은 재학습 시 필요\n",
    "from sklearn.preprocessing import LabelEncoder # 전처리 함수에 필요\n",
    "from pathlib import Path\n",
    "\n",
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, X, y): self.X = torch.FloatTensor(X); self.y = torch.LongTensor(y)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx): return {'input': self.X[idx], 'target': self.y[idx]}\n",
    "    def __str__(self): return f\"Data Size: {len(self.X)}, Input Shape: {self.X.shape}, Target Shape: {self.y.shape}\"\n",
    "\n",
    "class TitanicTestDataset(Dataset):\n",
    "    def __init__(self, X): self.X = torch.FloatTensor(X)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx): return {'input': self.X[idx]}\n",
    "    def __str__(self): return f\"Data Size: {len(self.X)}, Input Shape: {self.X.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363e62f7-413d-4a46-8158-c3717471bee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... 데이터 전처리 보조 함수 get_preprocessed_dataset_1 ~ _6 정의\n",
    "def get_preprocessed_dataset_1(all_df): # ... (동일) ...\n",
    "    Fare_mean = all_df[[\"Pclass\", \"Fare\"]].groupby(\"Pclass\").mean().reset_index(); Fare_mean.columns = [\"Pclass\", \"Fare_mean\"]\n",
    "    all_df = pd.merge(all_df, Fare_mean, on=\"Pclass\", how=\"left\"); all_df.loc[all_df[\"Fare\"].isnull(), \"Fare\"] = all_df[\"Fare_mean\"]\n",
    "    return all_df.drop(columns=[\"Fare_mean\"])\n",
    "# get_preprocessed_dataset_2 ~ _6 정의\n",
    "def get_preprocessed_dataset_2(all_df):\n",
    "    name_df = all_df[\"Name\"].str.split(\"[,.]\", n=2, expand=True)\n",
    "    if name_df.shape[1] == 3:\n",
    "        name_df.columns = [\"family_name\", \"title\", \"name\"]; name_df[\"family_name\"] = name_df[\"family_name\"].str.strip(); name_df[\"title\"] = name_df[\"title\"].str.strip(); name_df[\"name\"] = name_df[\"name\"].str.strip()\n",
    "        all_df = pd.concat([all_df, name_df], axis=1)\n",
    "    else: all_df['title'] = 'unknown'\n",
    "    return all_df\n",
    "def get_preprocessed_dataset_3(all_df):\n",
    "    if 'title' not in all_df.columns:\n",
    "        if all_df['Age'].isnull().any(): all_df['Age'] = all_df['Age'].fillna(all_df['Age'].median())\n",
    "        return all_df\n",
    "    title_age_median = all_df[[\"title\", \"Age\"]].groupby(\"title\").median().round().reset_index(); title_age_median.columns = [\"title\", \"title_age_median\"]\n",
    "    all_df = pd.merge(all_df, title_age_median, on=\"title\", how=\"left\"); all_df.loc[all_df[\"Age\"].isnull(), \"Age\"] = all_df[\"title_age_median\"]\n",
    "    if \"title_age_median\" in all_df.columns: all_df = all_df.drop([\"title_age_median\"], axis=1)\n",
    "    return all_df\n",
    "def get_preprocessed_dataset_4(all_df):\n",
    "    all_df[\"family_num\"] = all_df[\"Parch\"] + all_df[\"SibSp\"]; all_df.loc[all_df[\"family_num\"] == 0, \"alone\"] = 1; all_df[\"alone\"] = all_df[\"alone\"].fillna(0).astype(float)\n",
    "    cols_to_drop = [\"PassengerId\", \"Name\", \"family_name\", \"name\", \"Ticket\", \"Cabin\"]; existing_cols_to_drop = [col for col in cols_to_drop if col in all_df.columns]\n",
    "    return all_df.drop(existing_cols_to_drop, axis=1)\n",
    "def get_preprocessed_dataset_5(all_df):\n",
    "    if 'title' in all_df.columns: all_df.loc[~((all_df[\"title\"] == \"Mr\") | (all_df[\"title\"] == \"Miss\") | (all_df[\"title\"] == \"Mrs\") | (all_df[\"title\"] == \"Master\")), \"title\"] = \"other\"\n",
    "    all_df[\"Embarked\"] = all_df[\"Embarked\"].fillna(\"missing\")\n",
    "    return all_df\n",
    "def get_preprocessed_dataset_6(all_df):\n",
    "    category_features = all_df.select_dtypes(include=['object']).columns\n",
    "    for cat_feat in category_features:\n",
    "        le = LabelEncoder(); valid_indices = all_df[cat_feat].notna()\n",
    "        if valid_indices.any():\n",
    "            all_df.loc[valid_indices, cat_feat] = le.fit_transform(all_df.loc[valid_indices, cat_feat])\n",
    "            try: all_df[cat_feat] = pd.to_numeric(all_df[cat_feat])\n",
    "            except ValueError: pass\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a73b2f-02d4-4696-9f51-517520e7981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_data_raw():\n",
    "    \"\"\"CSV 로드, 전처리, 최종 데이터 배열 반환 (테스트 ID 포함)\"\"\"\n",
    "    try: CURRENT_FILE_PATH = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError: CURRENT_FILE_PATH = os.getcwd()\n",
    "    train_data_path = os.path.join(CURRENT_FILE_PATH, \"train.csv\")\n",
    "    test_data_path = os.path.join(CURRENT_FILE_PATH, \"test.csv\")\n",
    "    try:\n",
    "        train_df = pd.read_csv(train_data_path)\n",
    "        test_df = pd.read_csv(test_data_path)\n",
    "    except FileNotFoundError as e: print(f\"Error: {e}\"); sys.exit(1)\n",
    "\n",
    "    test_passenger_ids = test_df['PassengerId'] # 테스트 승객 ID 저장\n",
    "    all_df = pd.concat([train_df, test_df], sort=False)\n",
    "\n",
    "    # 전처리 단계 적용\n",
    "    all_df = get_preprocessed_dataset_1(all_df); all_df = get_preprocessed_dataset_2(all_df)\n",
    "    all_df = get_preprocessed_dataset_3(all_df); all_df = get_preprocessed_dataset_4(all_df)\n",
    "    all_df = get_preprocessed_dataset_5(all_df); all_df = get_preprocessed_dataset_6(all_df)\n",
    "\n",
    "    # 학습/테스트 분리\n",
    "    train_X_df = all_df[~all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1)\n",
    "    train_y = train_df[\"Survived\"]\n",
    "    test_X_df = all_df[all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1)\n",
    "\n",
    "    # 최종 데이터 확인 및 NaN 처리 (분리된 데이터프레임에 각각 적용)\n",
    "    print(\"\\n--- Final Data Check & Fill NaNs ---\")\n",
    "    columns_to_check = train_X_df.columns\n",
    "    for df in [train_X_df, test_X_df]:\n",
    "        df_name = \"Train\" if df is train_X_df else \"Test\"\n",
    "        for col in columns_to_check:\n",
    "            if df[col].dtype == 'object':\n",
    "                try: df[col] = pd.to_numeric(df[col])\n",
    "                except ValueError: df[col] = 0 # 변환 불가시 0으로 채움\n",
    "            if df[col].isnull().any(): df[col] = df[col].fillna(0) # NaN 0으로 채움\n",
    "\n",
    "    # 최종 인덱스 리셋 및 Numpy 배열 변환\n",
    "    train_X = train_X_df.reset_index(drop=True).values\n",
    "    test_X = test_X_df.reset_index(drop=True).values\n",
    "    train_y = train_y.values # Numpy 배열로 변환\n",
    "\n",
    "    print(\"Data preprocessing finished for submission generation.\")\n",
    "    return train_X, train_y, test_X, test_passenger_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0a517c-3979-4a38-8208-ecd01771701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DataLoader 생성 함수 (get_dataloaders) ---\n",
    "def get_dataloaders(train_X, train_y, test_X, batch_size_config):\n",
    "    \"\"\"학습/검증/테스트 DataLoader 모두 생성\"\"\"\n",
    "    dataset = TitanicDataset(train_X, train_y)\n",
    "    generator = torch.Generator().manual_seed(42) # 시드 고정\n",
    "    train_dataset, validation_dataset = random_split(dataset, [0.8, 0.2], generator=generator)\n",
    "    test_dataset = TitanicTestDataset(test_X)\n",
    "\n",
    "    print(f\"\\nTrain size: {len(train_dataset)}, Validation size: {len(validation_dataset)}, Test size: {len(test_dataset)}\")\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size_config, shuffle=True)\n",
    "    validation_loader = DataLoader(dataset=validation_dataset, batch_size=len(validation_dataset))\n",
    "    # 테스트 데이터 로더 생성 (비어있지 않을 때만)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset)) if len(test_dataset) > 0 else None\n",
    "\n",
    "    return train_loader, validation_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e98f35b-459d-429c-8c94-e9811465ab99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 신경망 모델 정의 (MyModel) ---\n",
    "class MyModel(nn.Module):\n",
    "    \"\"\"활성화 함수와 은닉층 크기를 인자로 받는 MLP 모델\"\"\"\n",
    "    def __init__(self, n_input, n_output, activation_fn_class, n_hidden1=20, n_hidden2=20):\n",
    "        super().__init__(); activation = activation_fn_class()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_input, n_hidden1), activation,\n",
    "            nn.Linear(n_hidden1, n_hidden2), activation,\n",
    "            nn.Linear(n_hidden2, n_output)\n",
    "        )\n",
    "    def forward(self, x): return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af291b7c-94cf-4f16-bde0-ad33f7af4fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 모델 및 옵티마이저 생성 함수 (get_model_and_optimizer) ---\n",
    "def get_model_and_optimizer(activation_fn_class, learning_rate, n_hidden1=20, n_hidden2=20):\n",
    "    \"\"\"모델 객체와 SGD 옵티마이저 생성\"\"\"\n",
    "    model = MyModel(10, 2, activation_fn_class, n_hidden1, n_hidden2) # 입력 10개, 출력 2개 고정\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ae0167-8ade-44c5-ba7b-16519d5882bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 모델 학습 및 검증 루프 ---\n",
    "def training_loop_with_early_stopping(model, optimizer, train_data_loader, validation_data_loader, n_epochs, loss_fn):\n",
    "    \"\"\"모델 학습/검증 수행 및 '최고 성능' 모델 상태 반환 (조기 종료 로직)\"\"\"\n",
    "    best_validation_accuracy = -1.0\n",
    "    best_model_state = None\n",
    "    best_epoch = 0\n",
    "\n",
    "    print(f\"\\nStart Final Training for {n_epochs} epochs (with early stopping logic)...\")\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # --- 학습 ---\n",
    "        model.train(); loss_train_epoch = 0.0; correct_train_epoch = 0; total_train_samples = 0\n",
    "        for batch in train_data_loader:\n",
    "            input_data = batch['input']; target = batch['target']; optimizer.zero_grad(); output_train = model(input_data); loss = loss_fn(output_train, target); loss.backward(); optimizer.step()\n",
    "            loss_train_epoch += loss.item() * input_data.size(0); _, predicted = torch.max(output_train.data, 1); total_train_samples += target.size(0); correct_train_epoch += (predicted == target).sum().item()\n",
    "        avg_loss_train = loss_train_epoch / total_train_samples if total_train_samples else 0\n",
    "        train_accuracy = 100 * correct_train_epoch / total_train_samples if total_train_samples else 0\n",
    "\n",
    "        # --- 검증 ---\n",
    "        model.eval(); loss_val_epoch = 0.0; correct_val_epoch = 0; total_val_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in validation_data_loader:\n",
    "                input_data = batch['input']; target = batch['target']; output_validation = model(input_data); loss = loss_fn(output_validation, target); loss_val_epoch += loss.item() * input_data.size(0)\n",
    "                _, predicted = torch.max(output_validation.data, 1); total_val_samples += target.size(0); correct_val_epoch += (predicted == target).sum().item()\n",
    "        avg_loss_validation = loss_val_epoch / total_val_samples if total_val_samples else 0\n",
    "        validation_accuracy = 100 * correct_val_epoch / total_val_samples if total_val_samples else 0\n",
    "\n",
    "        # 최고 성능 갱신 확인,조기 종료 로직\n",
    "        if validation_accuracy > best_validation_accuracy:\n",
    "            best_validation_accuracy = validation_accuracy\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = epoch\n",
    "            print(f\" New best validation accuracy: {best_validation_accuracy:.2f}% at epoch {epoch}\")\n",
    "\n",
    "        # 주기적 출력\n",
    "        if epoch % 100 == 0 or epoch == 1:\n",
    "            print(f\"Epoch {epoch:>{len(str(n_epochs))}}/{n_epochs}, Train Loss: {avg_loss_train:.4f}, Acc: {train_accuracy:.2f}% | Val Loss: {avg_loss_validation:.4f}, Acc: {validation_accuracy:.2f}%{' *Best*' if epoch == best_epoch else ''}\")\n",
    "\n",
    "    print(f\"\\n--- Final Training Summary --- Best Val Acc: {best_validation_accuracy:.2f}% achieved at epoch {best_epoch}\")\n",
    "    # 최고 성능 모델 상태와 에포크 반환\n",
    "    return best_validation_accuracy, best_model_state, best_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1110e837-5d69-443a-bff3-02b10c7aee6b",
   "metadata": {},
   "source": [
    "최적 하이퍼파라미터로 모델을 재학습시키는 함수입니다. 학습 중 매 에포크마다 검증 데이터 성능을 확인하고, 최고 성능을 보인 시점의 모델 상태와 에포크 번호를 기록하여 최종적으로 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfaedad-038d-4fc9-ba30-2330f43071a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 테스트 함수 (test_model) ---\n",
    "# (find_best_hyperparameters.py 와 동일하게 정의)\n",
    "def test_model(model, test_data_loader):\n",
    "    \"\"\"학습된 모델로 테스트 데이터 예측 수행\"\"\"\n",
    "    model.eval(); predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_data_loader: input_data = batch['input']; output = model(input_data); _, predicted = torch.max(output.data, 1); predictions.extend(predicted.cpu().numpy())\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b07bae1-78c8-411d-8943-36a641af838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Submission 파일 생성 함수 (create_submission) ---\n",
    "# (find_best_hyperparameters.py 와 동일하게 정의)\n",
    "def create_submission(predictions, passenger_ids, output_file=\"submission.csv\"):\n",
    "    \"\"\"예측 결과와 승객 ID로 submission CSV 파일 생성\"\"\"\n",
    "    if len(predictions) != len(passenger_ids): print(f\"Error: Prediction count ({len(predictions)}) != Passenger ID count ({len(passenger_ids)})\"); return\n",
    "    submission_df = pd.DataFrame({\"PassengerId\": passenger_ids, \"Survived\": predictions})\n",
    "    try: submission_df.to_csv(output_file, index=False); print(f\"Submission file saved: {output_file}\")\n",
    "    except Exception as e: print(f\"Error saving submission file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc7e5e6-c574-4536-84d3-9adde74e523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 메인 실행 함수 (최적 모델 로드 및 예측) ---\n",
    "def generate_submission_main(config_path=\"best_hyperparameters.json\", retrain_epochs=None):\n",
    "    \"\"\"최적 설정 로드, 해당 설정으로 재학습(조기종료 적용), 예측, submission 파일 생성\"\"\"\n",
    "    # 최적 설정 로드\n",
    "    try:\n",
    "        with open(config_path, 'r') as f: best_config = json.load(f)\n",
    "    except FileNotFoundError: print(f\"Error: Config file '{config_path}' not found.\"); sys.exit(1)\n",
    "    except json.JSONDecodeError: print(f\"Error: Could not decode JSON from '{config_path}'.\"); sys.exit(1)\n",
    "\n",
    "    print(\"--- Loaded Best Config ---\"); print(best_config)\n",
    "\n",
    "    # 설정값 추출\n",
    "    best_activation_name = best_config.get('activation_function')\n",
    "    best_batch_size = best_config.get('batch_size')\n",
    "    learning_rate = best_config.get('learning_rate', 1e-3)\n",
    "    n_hidden1 = best_config.get('n_hidden_unit_list', [20, 20])[0]\n",
    "    n_hidden2 = best_config.get('n_hidden_unit_list', [20, 20])[1]\n",
    "    # 재학습 에포크 수 결정 (인자 우선 > config 값 > 기본값 1000)\n",
    "    epochs_to_train = retrain_epochs if retrain_epochs is not None else best_config.get('epochs', 1000)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 활성화 함수 클래스 가져오기\n",
    "    activation_functions_map = {\"Sigmoid\": nn.Sigmoid, \"ReLU\": nn.ReLU, \"ELU\": nn.ELU, \"LeakyReLU\": nn.LeakyReLU}\n",
    "    if not best_activation_name or best_activation_name not in activation_functions_map:\n",
    "        print(f\"Error: Invalid activation function '{best_activation_name}' in config.\"); sys.exit(1)\n",
    "    best_activation_class = activation_functions_map[best_activation_name]\n",
    "\n",
    "    print(\"\\nPreprocessing data for final training and submission...\")\n",
    "    train_X, train_y, test_X, test_passenger_ids = get_preprocessed_data_raw()\n",
    "\n",
    "    if not best_batch_size: print(\"Error: 'batch_size' not found in config.\"); sys.exit(1)\n",
    "    train_loader, validation_loader, test_loader = get_dataloaders(train_X, train_y, test_X, best_batch_size)\n",
    "    if test_loader is None: print(\"Error: Test data is empty, cannot generate submission.\"); sys.exit(1)\n",
    "\n",
    "    model, optimizer = get_model_and_optimizer(best_activation_class, learning_rate, n_hidden1, n_hidden2)\n",
    "\n",
    "    # === 최종 모델 재학습 (조기 종료 로직 적용) ===\n",
    "    final_best_accuracy, final_best_model_state, final_best_epoch = training_loop_with_early_stopping(\n",
    "        model, optimizer, train_loader, validation_loader, epochs_to_train, loss_fn\n",
    "    )\n",
    "\n",
    "    # 최고 성능 모델 상태 로드\n",
    "    if final_best_model_state:\n",
    "        model.load_state_dict(final_best_model_state)\n",
    "        print(f\"\\nLoaded model state from epoch {final_best_epoch} with validation accuracy {final_best_accuracy:.2f}%\")\n",
    "\n",
    "        # 예측 수행\n",
    "        print(\"Generating predictions with the best epoch model...\")\n",
    "        test_predictions = test_model(model, test_loader)\n",
    "\n",
    "        # Submission 파일 생성\n",
    "        submission_filename = f\"submission_final_{best_activation_name}_bs{best_batch_size}_epoch{final_best_epoch}.csv\"\n",
    "        create_submission(test_predictions, test_passenger_ids, output_file=submission_filename)\n",
    "    else:\n",
    "        print(\"Final training did not produce a best model state. Cannot generate submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8c3355-f59c-4228-913d-b04eb6b9d761",
   "metadata": {},
   "source": [
    "* best_hyperparameters.json 파일을 읽어 최적 조합 설정을 불러옵니다.\n",
    "\n",
    "* 전체 데이터를 다시 로드하고 최적 배치 크기로 DataLoader를 생성합니다.\n",
    "\n",
    "* 최적 활성화 함수로 모델과 옵티마이저를 생성합니다.\n",
    "\n",
    "* training_loop_with_early_stopping 함수를 호출하여 모델을 재학습시키고, 이 과정에서 검증 성능이 가장 좋았던 시점(조기 종료 시점)의 모델 상태와 에포크 번호를 얻습니다.\n",
    "\n",
    "* 얻어진 최적 시점의 모델 상태를 로드합니다.\n",
    "\n",
    "* test_model 함수로 테스트 데이터 예측을 수행합니다.\n",
    "\n",
    "* create_submission 함수로 최종 제출 파일을 생성합니다 (파일 이름에 최적 조합 및 에포크 포함)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcff00d-75b7-4059-821b-12b1e508c14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 명령줄 인자 처리 및 실행 ---\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Retrain the best model with early stopping and generate submission.\")\n",
    "    parser.add_argument(\"--config\", default=\"best_hyperparameters.json\", help=\"Path to best hyperparameters JSON\")\n",
    "    # 재학습 에포크 수를 인자로 받을 수 있도록 추가 (선택사항)\n",
    "    parser.add_argument(\"-e\", \"--retrain_epochs\", type=int, default=None, help=\"Number of epochs for retraining (uses config value if not set)\")\n",
    "\n",
    "    if 'ipykernel' in sys.modules:\n",
    "        print(\"Running in interactive mode. Using default config path and epochs from config (or 1000).\")\n",
    "        args = parser.parse_args([]) # 기본값 사용\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    generate_submission_main(config_path=args.config, retrain_epochs=args.retrain_epochs) # 메인 함수 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33182661-0876-4ba8-aeb9-e88dd078e835",
   "metadata": {},
   "source": [
    "![캐글 제출 결](dl_titanic_leaderboard.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0434c03b-5203-4759-a11b-d3535aac822e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
