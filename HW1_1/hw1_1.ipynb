{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T03:48:37.172327Z",
     "start_time": "2025-09-26T03:48:37.163633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# torch.Tensor: PyTorch의 기본 텐서 클래스\n",
    "t1 = torch.Tensor([1, 2, 3], device='cpu')\n",
    "\n",
    "# 속성 확인 \n",
    "print(t1.dtype)   # dtype: torch.float32 (기본값)\n",
    "print(t1.device)  # device: cpu\n",
    "print(t1.requires_grad)  # requires_grad: False (기본값)\n",
    "print(t1.size())  # size: torch.Size([3])\n",
    "print(t1.shape)   # shape: torch.Size([3])\n",
    "\n",
    "# .cpu(): CPU 메모리로 텐서를 이동시킴\n",
    "t1_cpu = t1.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26edf27b-c2a5-4ad0-9c72-17a7cf255003",
   "metadata": {},
   "source": [
    "torch.Tensor는 클래스 생성자이다.\n",
    "\n",
    "주요 특징: 입력 데이터 타입과 관계없이 기본적으로 torch.float32 타입의 텐서를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b16c075-22d3-4369-a8a2-ba4233ad8262",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T03:48:39.955122Z",
     "start_time": "2025-09-26T03:48:39.937365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# torch.tensor: 텐서 생성을 위한 헬퍼 함수\n",
    "t2 = torch.tensor([1, 2, 3], device='cpu')\n",
    "\n",
    "# --- 속성 확인 ---\n",
    "print(t2.dtype)  # dtype: torch.int64 (입력 데이터로부터 추론)\n",
    "print(t2.device)  # device: cpu\n",
    "print(t2.requires_grad)  # requires_grad: False\n",
    "print(t2.size())  # size: torch.Size([3])\n",
    "print(t2.shape)  # shape: torch.Size([3])\n",
    "\n",
    "# 장치 이동\n",
    "t2_cpu = t2.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0638f512-0ea8-4780-8c1d-874b2ff8dcac",
   "metadata": {},
   "source": [
    "torch.tensor는 함수이다.\n",
    "\n",
    "특징: 입력된 데이터의 타입을 그대로 반영하여 텐서의 dtype을 결정한다.\n",
    "\n",
    "torch.Tensor와 달리 타입을 자동 추론해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3cb95660-e169-4d56-815c-75b13027be45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([]) 0\n",
      "torch.Size([1]) 1\n",
      "torch.Size([5]) 1\n",
      "torch.Size([5, 1]) 2\n",
      "torch.Size([3, 2]) 2\n",
      "torch.Size([3, 2, 1]) 3\n",
      "torch.Size([3, 1, 2, 1]) 4\n",
      "torch.Size([3, 1, 2, 3]) 4\n",
      "torch.Size([3, 1, 2, 3, 1]) 5\n",
      "torch.Size([4, 5]) 2\n",
      "torch.Size([4, 1, 5]) 3\n"
     ]
    }
   ],
   "source": [
    "# ndim: 차원의 수 (Number of Dimensions)\n",
    "a1 = torch.tensor(1)               # 0차원 (Scalar)\n",
    "print(a1.shape, a1.ndim)\n",
    "\n",
    "a2 = torch.tensor([1])             # 1차원 (Vector)\n",
    "print(a2.shape, a2.ndim)\n",
    "\n",
    "a3 = torch.tensor([1, 2, 3, 4, 5])   # 1차원\n",
    "print(a3.shape, a3.ndim)\n",
    "\n",
    "a4 = torch.tensor([[1], [2], [3], [4], [5]])   # 2차원 (Matrix), shape: (5, 1)\n",
    "print(a4.shape, a4.ndim)\n",
    "\n",
    "a5 = torch.tensor([                 # 2차원, shape: (3, 2)\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "])\n",
    "print(a5.shape, a5.ndim)\n",
    "\n",
    "a6 = torch.tensor([                 # 3차원, shape: (3, 2, 1)\n",
    "    [[1], [2]],\n",
    "    [[3], [4]],\n",
    "    [[5], [6]]\n",
    "])\n",
    "print(a6.shape, a6.ndim)\n",
    "\n",
    "a7 = torch.tensor([                 # 4차원, shape: (3, 1, 2, 1)\n",
    "    [[[1], [2]]],\n",
    "    [[[3], [4]]],\n",
    "    [[[5], [6]]]\n",
    "])\n",
    "print(a7.shape, a7.ndim)\n",
    "\n",
    "a8 = torch.tensor([                 # 4차원, shape: (3, 1, 2, 3)\n",
    "    [[[1, 2, 3], [2, 3, 4]]],\n",
    "    [[[3, 1, 1], [4, 4, 5]]],\n",
    "    [[[5, 6, 2], [6, 3, 1]]]\n",
    "])\n",
    "print(a8.shape, a8.ndim)\n",
    "\n",
    "\n",
    "a9 = torch.tensor([                 # 5차원, shape: (3, 1, 2, 3, 1)\n",
    "    [[[[1], [2], [3]], [[2], [3], [4]]]],\n",
    "    [[[[3], [1], [1]], [[4], [4], [5]]]],\n",
    "    [[[[5], [6], [2]], [[6], [3], [1]]]]\n",
    "])\n",
    "print(a9.shape, a9.ndim)\n",
    "\n",
    "# a10 변수에 2차원 텐서 할당\n",
    "a10 = torch.tensor([                 # 2차원, shape: (4, 5)\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "])\n",
    "print(a10.shape, a10.ndim)\n",
    "\n",
    "# a10 변수를 3차원 텐서로 덮어쓰기\n",
    "a10 = torch.tensor([                 # 3차원, shape: (4, 1, 5)\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "])\n",
    "print(a10.shape, a10.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef524a5d-1230-40cd-9ed3-9437338b3f7d",
   "metadata": {},
   "source": [
    "텐서의 차원(ndim)은 리스트의 중첩 깊이([]의 개수)와 일치한다.\n",
    "\n",
    "shape는 각 차원의 요소 개수를 나타내는 튜플이다.\n",
    "\n",
    "a3(shape=[5])과 a4(shape=[5, 1])는 데이터는 유사하나, 차원이 다르므로 완전히 다른 텐서이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5fad0abb-d74d-405b-99ab-3c45a87eb0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected sequence of length 3 at dim 3 (got 2)\n"
     ]
    }
   ],
   "source": [
    "#추가 코드: try-except 구문\n",
    "try:\n",
    "    # ValueError 발생 코드: 텐서는 모든 차원에서 요소의 개수가 동일해야 함\n",
    "    a11 = torch.tensor([\n",
    "        [[[1, 2, 3], [4, 5]]], # [1,2,3] (길이 3), [4,5] (길이 2) -> 길이가 다름\n",
    "        [[[1, 2, 3], [4, 5]]],\n",
    "        [[[1, 2, 3], [4, 5]]],\n",
    "        [[[1, 2, 3], [4, 5]]],\n",
    "    ])\n",
    "#추가 코드: 에러 발생 시 해당 에러 메시지만 출력함\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bec998-3e76-4139-9212-af74b3ea83e9",
   "metadata": {},
   "source": [
    "텐서는 모든 요소가 직사각형 형태의 구조를 가져야 한다.\n",
    "\n",
    "오류 원인: 동일한 차원에 있는 리스트 [1, 2, 3]과 [4, 5]의 길이가 서로 다르기 때문에 ValueError가 발생한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1442783f-f249-42c7-9160-d00652b427ed",
   "metadata": {},
   "source": [
    "2_텐서 초기화_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3fec5995-a88b-414b-b737-5749aa8da80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 1. torch.Tensor \n",
    "l1 = [1, 2, 3]\n",
    "# torch.Tensor: 데이터를 복사하여 새로운 텐서 생성\n",
    "t1 = torch.Tensor(l1)\n",
    "\n",
    "# 2. torch.tensor \n",
    "l2 = [1, 2, 3]\n",
    "# torch.tensor: 데이터를 복사하여 새로운 텐서 생성\n",
    "t2 = torch.tensor(l2)\n",
    "\n",
    "# 3. torch.as_tensor \n",
    "l3 = [1, 2, 3]\n",
    "# torch.as_tensor: Python 리스트는 메모리 공유가 불가능하므로, 데이터를 복사함\n",
    "t3 = torch.as_tensor(l3)\n",
    "\n",
    "# 원본 리스트의 첫 번째 요소를 100으로 변경\n",
    "l1[0] = 100\n",
    "l2[0] = 100\n",
    "l3[0] = 100\n",
    "\n",
    "# 세 경우 모두 데이터가 복사되었기 때문에 원본 리스트의 변경 사항이 텐서에 반영되지 않음\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a53d68a-902e-4ecd-8ed6-f92702d39df4",
   "metadata": {},
   "source": [
    "Python 리스트를 입력으로 사용할 경우, torch.Tensor, torch.tensor, torch.as_tensor 모두 데이터를 새로운 메모리 공간에 복사하여 텐서를 생성한다.\n",
    "\n",
    "따라서 원본 리스트(l1, l2, l3)의 값을 변경해도 이미 생성된 텐서(t1, t2, t3)에는 아무런 영향을 주지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "003b6fdf-18a5-47ab-ac97-c5338654f4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "tensor([100,   2,   3])\n"
     ]
    }
   ],
   "source": [
    "# --- 4. torch.Tensor ---\n",
    "l4 = np.array([1, 2, 3])\n",
    "# torch.Tensor: NumPy 배열을 입력받아도 데이터를 복사함\n",
    "t4 = torch.Tensor(l4)\n",
    "\n",
    "# --- 5. torch.tensor ---\n",
    "l5 = np.array([1, 2, 3])\n",
    "# torch.tensor: NumPy 배열을 입력받아도 데이터를 복사함\n",
    "t5 = torch.tensor(l5)\n",
    "\n",
    "# --- 6. torch.as_tensor ---\n",
    "l6 = np.array([1, 2, 3])\n",
    "# torch.as_tensor: NumPy 배열과는 메모리를 공유함. 데이터 복사 없음\n",
    "t6 = torch.as_tensor(l6)\n",
    "\n",
    "# 원본 NumPy 배열의 첫 번째 요소를 100으로 변경\n",
    "l4[0] = 100\n",
    "l5[0] = 100\n",
    "l6[0] = 100\n",
    "\n",
    "# t4, t5는 복사본이므로 영향 없음\n",
    "print(t4)\n",
    "print(t5)\n",
    "# t6는 원본 l6와 메모리를 공유하므로 변경 사항이 그대로 반영됨\n",
    "print(t6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d29d18f-9418-492d-a6f8-3ead66a27004",
   "metadata": {},
   "source": [
    "torch.as_tensor의 동작 방식이 핵심이다.\n",
    "\n",
    "torch.Tensor와 torch.tensor는 입력이 NumPy 배열이어도 항상 데이터를 복사한다.\n",
    "\n",
    "반면, torch.as_tensor는 입력 데이터가 NumPy 배열처럼 메모리 구조가 호환되는 경우, 데이터를 복사하지 않고 메모리 공간을 공유한다. 이를 Zero-copy라고 한다.\n",
    "\n",
    "메모리가 공유되었기 때문에 원본 NumPy 배열 l6의 값을 바꾸자 텐서 t6의 값도 함께 바뀌었다.\n",
    "\n",
    "결론: 데이터 복사를 피하고 싶을 때(효율성, 메모리 절약)는 torch.as_tensor나 torch.from_numpy를 사용하는 것이 좋다. 원본 데이터와 텐서를 완전히 분리하고 싶을 때는 torch.tensor를 사용하는 것이 안전하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd82ae9-5183-4331-9d68-aa61a18f5768",
   "metadata": {},
   "source": [
    "3. 텐서 초기화 constant value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "69546948-e767-48ca-8697-0014d4b3ceab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# torch.ones: 지정된 shape의 텐서를 생성하고 모든 요소를 1로 채움\n",
    "t1 = torch.ones(size=(5,))  # torch.ones(5)와 동일\n",
    "# torch.ones_like: 입력 텐서(t1)와 동일한 shape, dtype, device를 갖는 텐서를 생성하고 1로 채움\n",
    "t1_like = torch.ones_like(input=t1)\n",
    "\n",
    "print(t1)\n",
    "print(t1_like)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6402ab-3a2e-45e2-b8c4-20955187cc60",
   "metadata": {},
   "source": [
    "torch.ones: 특정 크기의 텐서를 만들고 모든 값을 1로 초기화할 때 사용된다.\n",
    "\n",
    "torch.ones_like: 기존 텐서의 속성(크기, 타입 등)은 그대로 유지하고 값만 1로 채운 새 텐서를 만들고 싶을 때 유용하다. 속성을 일일이 지정할 필요가 없어 코드가 간결해진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "033bbd78-cd60-42db-9259-b082ded8ca9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# torch.zeros: 지정된 shape의 텐서를 생성하고 모든 요소를 0으로 채움\n",
    "t2 = torch.zeros(size=(6,))\n",
    "# torch.zeros_like: 입력 텐서(t2)와 동일한 속성을 갖는 텐서를 생성하고 0으로 채움\n",
    "t2_like = torch.zeros_like(input=t2)\n",
    "\n",
    "print(t2)\n",
    "print(t2_like)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add4700e-4412-4711-b738-275d20954b91",
   "metadata": {},
   "source": [
    "torch.zeros: torch.ones와 유사하나 모든 요소를 0으로 초기화한다. 주로 텐서를 특정 값으로 채우기 전 초기 상태를 만들 때 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "313bde61-5fff-4208-8b3a-8f6e38608d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 0.])\n",
      "tensor([3.3832e+07, 1.4503e-42, 0.0000e+00, 0.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "# torch.empty: 지정된 shape의 텐서를 위한 메모리 공간만 할당하고, 값을 초기화하지 않음\n",
    "t3 = torch.empty(size=(4,))\n",
    "# torch.empty_like: 입력 텐서(t3)와 동일한 속성의 텐서를 위한 메모리 공간만 할당\n",
    "t3_like = torch.empty_like(input=t3)\n",
    "\n",
    "print(t3)\n",
    "print(t3_like)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca146fc2-f7f4-4939-b4f2-477bdada53a6",
   "metadata": {},
   "source": [
    "torch.empty는 값을 초기화하지 않는다는 점이 가장 중요하다.\n",
    "\n",
    "메모리 공간만 할당하기 때문에, 해당 메모리에 이전에 저장되어 있던 의미 없는 값(garbage value)이 그대로 남아있다. 실행할 때마다 결과가 다르게 나오는 이유이다.\n",
    "\n",
    "zeros나 ones에 비해 속도가 약간 빠르지만, 초기화되지 않은 값을 사용할 위험이 있어 주의가 필요하다. 생성 직후 다른 값으로 덮어 쓸 목적일 때만 사용하는 것이 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "399c1c8f-8f50-4f38-ba23-c6397e76f155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# torch.eye: n x n 크기의 단위 행렬을 생성함\n",
    "# 단위 행렬: 주 대각선의 요소는 1이고 나머지는 모두 0인 행렬\n",
    "t4 = torch.eye(n=3)\n",
    "\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac8db86-a094-4cde-a1ee-92f05776667f",
   "metadata": {},
   "source": [
    "torch.eye: 선형대수학에서 많이 사용되는 단위 행렬을 간편하게 생성하는 함수이다. n은 행과 열의 크기를 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d0ccaa-ac30-4db7-a5d9-16dfee4111b9",
   "metadata": {},
   "source": [
    "4. 랜덤값 텐서 초기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f1d5918f-2b0c-4bb7-9835-c3d011c822bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[13, 16]])\n",
      "tensor([[0.7058, 0.4571, 0.6146]])\n",
      "tensor([[ 0.1423,  1.2607, -0.0097]])\n",
      "tensor([[11.8543,  9.9867],\n",
      "        [10.0033,  9.1355],\n",
      "        [11.3935,  9.8811]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# torch.randint: low(포함)와 high(미포함) 사이의 정수로 텐서를 채움\n",
    "t1 = torch.randint(low=10, high=20, size=(1, 2))\n",
    "print(t1)\n",
    "\n",
    "# torch.rand: 0과 1 사이의 균등 분포에서 값을 추출\n",
    "t2 = torch.rand(size=(1, 3))\n",
    "print(t2)\n",
    "\n",
    "# torch.randn: 평균이 0, 표준편차가 1인 표준 정규 분포에서 값을 추출\n",
    "t3 = torch.randn(size=(1, 3))\n",
    "print(t3)\n",
    "\n",
    "# torch.normal: 지정된 평균과 표준편차를 갖는 정규 분포에서 값을 추출\n",
    "t4 = torch.normal(mean=10.0, std=1.0, size=(3, 2))\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b90c5b-6ad4-4336-a39e-311a6ad3a4c9",
   "metadata": {},
   "source": [
    "rand vs randn: 가장 중요한 차이점입니다. rand는 모든 숫자가 나올 확률이 동일한 균등 분포를 따르고, randn은 평균(0) 근처의 값이 더 자주 나오는 정규 분포를 따릅니다. 모델의 가중치를 초기화할 때 정규 분포가 더 자주 사용됩니다.\n",
    "\n",
    "torch.randint: 정수 형태의 랜덤 값이 필요할 때 사용합니다.\n",
    "\n",
    "torch.normal: 원하는 평균과 표준편차를 직접 지정하여 정규 분포를 만들고 싶을 때 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "48321a9d-d780-4a80-b99a-c1e31437816f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 2.5000, 5.0000])\n",
      "tensor([0, 1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# torch.linspace: start(포함)부터 end(포함)까지를 steps 개수로 균등하게 나눔\n",
    "t5 = torch.linspace(start=0.0, end=5.0, steps=3)\n",
    "print(t5)\n",
    "\n",
    "# torch.arange: 0부터 end(미포함)까지 1씩 증가하는 정수 텐서를 생성\n",
    "t6 = torch.arange(5)\n",
    "print(t6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0420a81-fc3e-4b82-a1c0-1d2dae6bbcb0",
   "metadata": {},
   "source": [
    "linspace vs arange: linspace는 구간과 개수가 중요하고, arange는 구간과 간격이 중요하다.\n",
    "\n",
    "linspace: steps로 지정한 개수에 맞춰 점을 찍기 때문에 간격을 자동으로 계산합니다. end 값을 포함하는 것이 특징이다.\n",
    "\n",
    "arange: 간격(기본값 1)에 맞춰 값을 채우기 때문에 개수가 자동으로 결정됩니다. 파이썬의 range 함수처럼 end 값을 포함하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "571a45e8-2f77-4b05-9890-835b3f420f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5891, 0.7140, 0.6211],\n",
      "        [0.3329, 0.8870, 0.0032]])\n",
      "tensor([[0.5885, 0.5380, 0.7015],\n",
      "        [0.1044, 0.4761, 0.7713]])\n",
      "\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n"
     ]
    }
   ],
   "source": [
    "# 시드를 고정하지 않았을 때: random1과 random2는 다름\n",
    "random1 = torch.rand(2, 3)\n",
    "print(random1)\n",
    "\n",
    "random2 = torch.rand(2, 3)\n",
    "print(random2)\n",
    "\n",
    "print()\n",
    "\n",
    "# torch.manual_seed: 난수 생성기의 시드를 고정함\n",
    "torch.manual_seed(1729)\n",
    "# 시드 고정 후 처음 생성한 random3\n",
    "random3 = torch.rand(2, 3)\n",
    "print(random3)\n",
    "\n",
    "# 이어서 생성한 random4\n",
    "random4 = torch.rand(2, 3)\n",
    "print(random4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea498da8-f29a-4e6e-9bfe-656f485269c7",
   "metadata": {},
   "source": [
    "torch.mauual_seed()로 시드를 고정하면 random값이 고정된다. \n",
    "\n",
    "다른 사람과 공유하는 프로젝트라면 재현을 위해 고정하는 것이 좋다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8be84e-2f58-4536-9af7-7c758530a5b9",
   "metadata": {},
   "source": [
    "5. 텐서 타입 변환 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1851a855-094e-4266-81f2-e9d094c3b0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "tensor([[18.0429,  7.2532, 19.6519],\n",
      "        [10.8626,  2.1505, 19.6913]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# dtype을 지정하지 않으면 기본값인 torch.float32로 생성된다.\n",
    "a = torch.ones((2, 3))\n",
    "print(a.dtype)\n",
    "\n",
    "# dtype=torch.int16으로 지정하여 정수형 텐서를 생성한다.\n",
    "b = torch.ones((2, 3), dtype=torch.int16)\n",
    "print(b)\n",
    "\n",
    "# dtype=torch.float64로 지정하여 배정밀도 부동소수점 텐서를 생성한다.\n",
    "c = torch.rand((2, 3), dtype=torch.float64) * 20.\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec0057d-eb63-4234-935c-2ba0e9a34f52",
   "metadata": {},
   "source": [
    "PyTorch 텐서의 기본 자료형은 torch.float32이다.\n",
    "\n",
    "텐서 생성 함수(ones, rand 등)에 dtype 인자를 전달하여 원하는 자료형으로 텐서를 만들 수 있다. 정수, 부동소수점 등 다양한 자료형을 지원한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "27e97897-2c27-4df8-aacf-3944e9be00e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int32\n",
      "torch.float64\n",
      "torch.int16\n"
     ]
    }
   ],
   "source": [
    "# b 텐서(int16)를 int32 자료형으로 변환한다.\n",
    "d = b.to(torch.int32)\n",
    "print(d.dtype)\n",
    "\n",
    "#.to() 메소드 사용\n",
    "double_d = torch.zeros(10, 2).to(torch.double)\n",
    "\n",
    "# .double(), .short() 등 약칭 메소드 사용\n",
    "short_e = torch.ones(10, 2).short()\n",
    "\n",
    "# .type() 메소드 사용 \n",
    "double_d = torch.zeros(10, 2).type(torch.double)\n",
    "\n",
    "print(double_d.dtype)\n",
    "print(short_e.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa1b4bb-962b-433e-beef-4c6a27fcf55c",
   "metadata": {},
   "source": [
    "텐서의 자료형을 변환하는 것을 타입 캐스팅이라 한다.\n",
    "\n",
    ".to() 메소드는 자료형뿐만 아니라 장치까지 한 번에 변경할 수 있어 가장 유연하고 많이 사용된다.\n",
    "\n",
    ".float(), .double(), .int(), .short() 등 직관적인 이름의 메소드도 자주 사용된다. (double은 float64, short는 int16을 의미한다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f2cf41e5-3da5-44f8-baf7-921138c08073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "# double_f는 float64, short_g는 int16 자료형을 갖는다.\n",
    "double_f = torch.rand(5, dtype=torch.double)\n",
    "short_g = double_f.to(torch.short)\n",
    "\n",
    "# float64와 int16 텐서를 곱하면, 더 넓은 표현 범위를 갖는 float64로 자동 변환된다.\n",
    "result = double_f * short_g\n",
    "print(result.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82344a7-4c36-44c5-a572-eafaf8510cf2",
   "metadata": {},
   "source": [
    "자료형이 다른 텐서 간의 연산에서 데이터 손실을 막기 위해, PyTorch는 표현 범위가 더 넓거나 정밀한 자료형으로 자동으로 타입을 맞춘다. 이를 type_conversion이라 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5854fe10-705d-4f18-baaf-ef0a7698f931",
   "metadata": {},
   "source": [
    "6. 텐서 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "64d18f00-2de2-4005-9fa4-9b5d8fbac178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.ones(size=(2, 3))\n",
    "t2 = torch.ones(size=(2, 3))\n",
    "\n",
    "# 덧셈: torch.add 함수와 + 연산자는 동일하다.\n",
    "t3 = torch.add(t1, t2)\n",
    "t4 = t1 + t2\n",
    "print(t3)\n",
    "print(t4)\n",
    "\n",
    "# 뺄셈: torch.sub 함수와 - 연산자는 동일하다.\n",
    "t5 = torch.sub(t1, t2)\n",
    "t6 = t1 - t2\n",
    "print(t5)\n",
    "print(t6)\n",
    "\n",
    "# 곱셈: torch.mul 함수와 * 연산자는 동일하다.\n",
    "t7 = torch.mul(t1, t2)\n",
    "t8 = t1 * t2\n",
    "print(t7)\n",
    "print(t8)\n",
    "\n",
    "# 나눗셈: torch.div 함수와 / 연산자는 동일하다.\n",
    "t9 = torch.div(t1, t2)\n",
    "t10 = t1 / t2\n",
    "print(t9)\n",
    "print(t10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d103c05-158d-4540-8e30-16ceec297349",
   "metadata": {},
   "source": [
    "위 연산들은 모두 요소별 연산이다. 이는 같은 위치에 있는 텐서의 요소끼리 개별적으로 계산하는 방식을 의미한다.\n",
    "\n",
    "torch.add와 +처럼 함수 방식과 연산자 방식은 기능적으로 완전히 동일한 결과를 반환하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01336bba-b133-46ba-9896-4dc65de1d442",
   "metadata": {},
   "source": [
    "7. 텐서 연산 mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "46da9b98-e6e5-4b8b-bc21-aa57d3eb06fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# torch.dot: 두 벡터의 각 요소별 곱의 합을 계산한다.\n",
    "# (2*2) + (3*1) = 7\n",
    "t1 = torch.dot(\n",
    "  torch.tensor([2, 3]), torch.tensor([2, 1])\n",
    ")\n",
    "print(t1, t1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891e66f0-e5b7-4e97-b1fe-f2a89ca8a5d6",
   "metadata": {},
   "source": [
    "torch.dot은 오직 1차원 텐서에만 사용할 수 있다. 입력 벡터들의 길이가 같아야 하며, 결과는 항상 원소가 하나인 0차원 텐서이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b1f6a38b-c09b-4eb3-ad34-c7b3f82b10bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.6750, 2.2840],\n",
      "        [0.0956, 1.0294]]) torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# (2, 3) 크기의 행렬 t2\n",
    "t2 = torch.randn(2, 3)\n",
    "# (3, 2) 크기의 행렬 t3\n",
    "t3 = torch.randn(3, 2)\n",
    "\n",
    "# torch.mm: (m, n) x (n, p) -> (m, p) 크기의 행렬을 반환한다.\n",
    "# 여기서는 (2, 3) x (3, 2) -> (2, 2) 크기의 행렬이 된다.\n",
    "t4 = torch.mm(t2, t3)\n",
    "print(t4, t4.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530c977c-d9ee-4ea0-97df-53ccb56f89d6",
   "metadata": {},
   "source": [
    "torch.mm은 2차원 텐서 전용 행렬 곱 함수이다. 첫 번째 행렬의 열 개수와 두 번째 행렬의 행 개수가 일치해야 연산이 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "01ed0a47-3649-42f2-8990-e530fdb47626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "# (batch_size=10, m=3, n=4) 크기의 3차원 텐서 t5\n",
    "t5 = torch.randn(10, 3, 4)\n",
    "# (batch_size=10, n=4, p=5) 크기의 3차원 텐서 t6\n",
    "t6 = torch.randn(10, 4, 5)\n",
    "\n",
    "# torch.bmm: 배치 내 각 행렬에 대해 행렬 곱을 수행한다.\n",
    "# (b, m, n) x (b, n, p) -> (b, m, p) 크기의 텐서를 반환한다.\n",
    "t7 = torch.bmm(t5, t6)\n",
    "print(t7.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82df90d1-2f92-4f0f-9297-6544c63705e2",
   "metadata": {},
   "source": [
    "torch.bmm은 3차원 텐서를 입력으로 받으며, 첫 번째 차원은 배치 크기를 의미한다. 이 배치 차원의 크기는 두 입력 텐서에서 동일해야 한다. 배치 내의 10개 행렬 쌍에 대해 각각 독립적으로 행렬 곱을 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94cd5a9-cf30-4e9a-b430-50bca5daffeb",
   "metadata": {},
   "source": [
    "8. 텐서 곱 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f55096d9-2699-45f3-8830-2505634bd69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([3])\n",
      "torch.Size([10, 3])\n",
      "torch.Size([10, 3, 5])\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Case 1: 1차원 텐서 × 1차원 텐서\n",
    "# 벡터 내적을 수행한다. torch.dot과 동일하다.\n",
    "t1 = torch.randn(3)\n",
    "t2 = torch.randn(3)\n",
    "print(torch.matmul(t1, t2).size())\n",
    "\n",
    "# Case 2: 2차원 텐서 × 1차원 텐서\n",
    "# 행렬과 벡터의 곱을 수행한다.\n",
    "t3 = torch.randn(3, 4)\n",
    "t4 = torch.randn(4)\n",
    "print(torch.matmul(t3, t4).size())\n",
    "\n",
    "# Case 3: 3차원 텐서 × 1차원 텐서\n",
    "# 배치 행렬과 벡터의 곱을 수행한다. 벡터가 브로드캐스팅된다.\n",
    "t5 = torch.randn(10, 3, 4)\n",
    "t6 = torch.randn(4)\n",
    "print(torch.matmul(t5, t6).size())\n",
    "\n",
    "# Case 4: 3차원 텐서 × 3차원 텐서\n",
    "# 배치 행렬 곱을 수행한다. torch.bmm과 동일하다.\n",
    "t7 = torch.randn(10, 3, 4)\n",
    "t8 = torch.randn(10, 4, 5)\n",
    "print(torch.matmul(t7, t8).size())\n",
    "\n",
    "# Case 5: 3차원 텐서 × 2차원 텐서\n",
    "# 배치 행렬과 일반 행렬의 곱을 수행한다. 2차원 행렬이 브로드캐스팅된다.\n",
    "t9 = torch.randn(10, 3, 4)\n",
    "t10 = torch.randn(4, 5)\n",
    "print(torch.matmul(t9, t10).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb33f315-255b-4df3-8242-8bbc99f9113c",
   "metadata": {},
   "source": [
    "torch.matmul은 입력 텐서의 차원에 따라 동작하는 통합 곱셈 함수이다.\n",
    "\n",
    "1차원끼리는 벡터 내적, 2차원끼리는 행렬 곱, 3차원끼리는 배치 행렬 곱을 수행하다.\n",
    "\n",
    "torch.matmul의 특징은 브로드캐스팅을 지원한다는 점이다. Case 3과 Case 5에서 볼 수 있듯이, 차원이 다른 텐서끼리 연산할 때 차원이 낮은 텐서(t6, t10)를 자동으로 확장하여 연산을 수행하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ffe63b-07e4-4639-b474-6cef7e5ff0e7",
   "metadata": {},
   "source": [
    "9. 브로드캐스팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d81ffd0b-f126-4f91-9f76-fbc4144b1940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6.])\n",
      "tensor([[-4, -4],\n",
      "        [-2, -1],\n",
      "        [ 6,  5]])\n",
      "tensor([[3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[0.5000, 1.0000],\n",
      "        [1.5000, 2.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1차원 텐서와 스칼라의 곱셈\n",
    "t1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "t2 = 2.0\n",
    "print(t1 * t2)\n",
    "\n",
    "# 2차원 텐서와 1차원 텐서의 뺄셈\n",
    "# t4가 t3의 각 행에 적용된다.\n",
    "t3 = torch.tensor([[0, 1], [2, 4], [10, 10]])\n",
    "t4 = torch.tensor([4, 5])\n",
    "print(t3 - t4)\n",
    "\n",
    "# 텐서와 스칼라의 사칙연산\n",
    "t5 = torch.tensor([[1., 2.], [3., 4.]])\n",
    "print(t5 + 2.0)\n",
    "print(t5 / 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6993b701-4bc0-4bb3-97d2-a8380e3e1052",
   "metadata": {},
   "source": [
    "기본 브로드캐스팅: 텐서와 스칼라\n",
    "가장 간단한 브로드캐스팅은 텐서와 스칼라 값의 연산이다. 스칼라 값이 텐서의 모든 요소에 적용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "28e8be01-073e-4275-a6e4-dc4c42d396d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 3],\n",
      "        [3, 4]])\n",
      "tensor([[6, 7],\n",
      "        [2, 5]])\n",
      "tensor([[8, 6],\n",
      "        [5, 3]])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "t7 = torch.tensor([[1, 2], [0, 3]])      # shape: (2, 2)\n",
    "t8 = torch.tensor([[3, 1]])              # shape: (1, 2)\n",
    "t9 = torch.tensor([[5], [2]])              # shape: (2, 1)\n",
    "\n",
    "# t7(2, 2) + t8(1, 2) -> t8이 첫 번째 차원에서 (2, 2)로 확장되어 연산\n",
    "print(t7 + t8)\n",
    "# t7(2, 2) + t9(2, 1) -> t9가 두 번째 차원에서 (2, 2)로 확장되어 연산\n",
    "print(t7 + t9)\n",
    "# t8(1, 2) + t9(2, 1) -> t8은 (2, 2)로, t9도 (2, 2)로 확장되어 연산\n",
    "print(t8 + t9)\n",
    "\n",
    "# 3차원 텐서와 2차원 텐서의 브로드캐스팅\n",
    "t11 = torch.ones(4, 3, 2)\n",
    "t12 = t11 * torch.rand(3, 2)  # (4, 3, 2) * (3, 2) -> (4, 3, 2)\n",
    "print(t12.shape)\n",
    "\n",
    "# 차원 크기가 1인 경우의 브로드캐스팅\n",
    "t13 = torch.ones(4, 3, 2)\n",
    "t14 = t13 * torch.rand(3, 1)  # (4, 3, 2) * (3, 1) -> (4, 3, 2)\n",
    "print(t14.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3be4234-d7ab-40b4-9955-34ba856ca74d",
   "metadata": {},
   "source": [
    "두 텐서 간의 브로드캐스팅은 다음 두 규칙을 따른다.\n",
    "\n",
    "두 텐서의 차원 수를 맞추기 위해, 차원이 적은 텐서의 왼쪽에 1을 추가하여 shape을 맞춘다.\n",
    "\n",
    "오른쪽 차원부터 시작하여 각 차원을 비교할 때, 두 텐서의 차원 크기가 같거나 둘 중 하나의 차원 크기가 1이면 호환 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "de7cb604-7c2d-4cca-8570-36b201bd87f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예상된 에러가 발생했습니다.\n",
      "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\n"
     ]
    }
   ],
   "source": [
    "# 추가 코드: 에러 확인을 위한 try-except 구문\n",
    "try:\n",
    "  # shape (5, 2, 4, 1)와 (3, 1, 1)의 연산\n",
    "  # 오른쪽부터 차원을 비교하면 1, 4은 호환되지만,\n",
    "  # 그 다음 차원인 2와 3은 같지도 않고 어느 한쪽이 1도 아니므로 에러가 발생한다.\n",
    "  t25 = torch.empty(5, 2, 4, 1)\n",
    "  t26 = torch.empty(3, 1, 1)\n",
    "  print((t25 + t26).size())\n",
    "except RuntimeError as e:\n",
    "  print(\"예상된 에러가 발생했습니다.\")\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcb3bee-0c81-4cd3-9f60-b633a72183a5",
   "metadata": {},
   "source": [
    "브로드캐스팅은 매우 강력한 기능이지만, 규칙을 정확히 이해하고 사용해야 의도치 않은 에러나 잘못된 연산을 피할 수 있다. 디버깅 시 텐서의 shape을 출력하여 확인하는 습관이 중요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5e0a10-c81a-4785-9912-c65013505997",
   "metadata": {},
   "source": [
    "10. 텐서 인덱싱 / 슬라이싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "05cc8ce1-93fc-4436-a47c-a1491f4b0802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 6, 7, 8, 9])\n",
      "tensor([ 1,  6, 11])\n",
      "tensor(7)\n",
      "tensor([[ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14]])\n",
      "tensor([[ 8,  9],\n",
      "        [13, 14]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(\n",
    "  [[0, 1, 2, 3, 4],\n",
    "   [5, 6, 7, 8, 9],\n",
    "   [10, 11, 12, 13, 14]]\n",
    ")\n",
    "\n",
    "# 인덱스 1의 행 전체 선택\n",
    "print(x[1])\n",
    "# 모든 행에 대해, 인덱스 1의 열 선택\n",
    "print(x[:, 1])\n",
    "# 인덱스 1인 행의 인덱스 2인 열에 있는 원소 선택\n",
    "print(x[1, 2])\n",
    "# 인덱스 1인 행부터 끝까지 선택\n",
    "print(x[1:])\n",
    "# 인덱스 1인 행부터 끝까지, 그리고 인덱스 3인 열부터 끝까지 선택\n",
    "print(x[1:, 3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b71c74-f514-440f-9ab1-68025e346c7a",
   "metadata": {},
   "source": [
    "대괄호 []와 인덱스, 그리고 콜론 :을 조합하여 텐서의 특정 부분을 선택할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b8f644c0-c4a9-4c1d-a16f-f849009acb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 0, 0, 5],\n",
      "        [5, 0, 0, 8]])\n"
     ]
    }
   ],
   "source": [
    "# 6x6 크기의 0으로 채워진 텐서 생성\n",
    "y = torch.zeros((6, 6))\n",
    "\n",
    "# 행은 1~3, 열은 2인 영역에 값 1을 할당한다.\n",
    "# 스칼라 값 1이 해당 영역 전체에 브로드캐스팅된다.\n",
    "y[1:4, 2] = 1\n",
    "print(y)\n",
    "\n",
    "# 3x4 크기의 텐서 생성\n",
    "z = torch.tensor(\n",
    "  [[1, 2, 3, 4],\n",
    "   [2, 3, 4, 5],\n",
    "   [5, 6, 7, 8]]\n",
    ")\n",
    "\n",
    "# 행은 1부터 끝까지, 열은 1~2인 영역에 값 0을 할당한다.\n",
    "z[1:, 1:3] = 0\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3478d452-0b0c-46fd-a2d9-814884a0a369",
   "metadata": {},
   "source": [
    "슬라이싱은 단순히 값을 가져오는 것뿐만 아니라, 특정 영역에 새로운 값을 일괄적으로 할당하는 데에도 사용한다.\n",
    ": 기호는 해당 차원의 모든 요소를 선택하는 것을 의미하다.\n",
    "start:end 형식은 start 인덱스부터 end-1 인덱스까지의 범위를 선택하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc03de43-0b0f-4539-be99-3395d7cc860e",
   "metadata": {},
   "source": [
    "11. 텐서 모양 바꾸기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c3e245f1-0a84-4d21-a584-2c9ef0ad25d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1, 2, 3, 4, 5, 6]])\n",
      "tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "# t1(2, 3)의 shape을 (3, 2)로 변경한다.\n",
    "t2 = t1.view(3, 2)\n",
    "# t1(2, 3)의 shape을 (1, 6)으로 변경한다.\n",
    "t3 = t1.reshape(1, 6)\n",
    "print(t2)\n",
    "print(t3)\n",
    "\n",
    "# arange로 생성한 텐서에 바로 view를 적용할 수도 있다.\n",
    "t4 = torch.arange(8).view(2, 4)\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7f6d04-27f4-4723-b4dc-84b4ec4e1cd2",
   "metadata": {},
   "source": [
    "view와 reshape는 기능적으로 매우 유사하나 미세한 차이가 있다. view는 메모리 상에서 연속적인 데이터에만 사용 가능하며 원본 텐서와 데이터를 공유한다. reshape는 더 유연하여, 필요시 데이터를 복사해서라도 shape을 변경해준다. 일반적으로 reshape를 사용하는 것이 더 안전하고 편리하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2ec82a37-44c3-4c63-b292-0f2f163606d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n"
     ]
    }
   ],
   "source": [
    "# (1, 3, 1) 크기의 텐서 생성\n",
    "t6 = torch.tensor([[[1], [2], [3]]])\n",
    "\n",
    "# squeeze(): 크기가 1인 모든 차원을 제거한다. (1, 3, 1) -> (3,)\n",
    "t7 = t6.squeeze()\n",
    "\n",
    "# squeeze(0): 0번 차원만 제거한다. (1, 3, 1) -> (3, 1)\n",
    "t8 = t6.squeeze(0)\n",
    "print(t7)\n",
    "print(t8)\n",
    "\n",
    "\n",
    "# (3,) 크기의 텐서 생성\n",
    "t9 = torch.tensor([1, 2, 3])\n",
    "\n",
    "# unsqueeze(1): 1번 위치에 새로운 차원을 추가한다. (3,) -> (3, 1)\n",
    "t10 = t9.unsqueeze(1)\n",
    "print(t10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561ebcae-cd07-4b7b-950c-a24c8f1b3243",
   "metadata": {},
   "source": [
    "이 함수들은 모델의 입출력 shape을 맞추거나 브로드캐스팅을 위해 차원을 조작할 때 매우 유용하다. squeeze는 불필요한 차원을 없애고, unsqueeze는 연산을 위해 차원을 추가하는 역할을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0540dbf2-f109-4729-b825-8cce47fa1d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "t13 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "# flatten(): 모든 원소를 1차원으로 펼친다.\n",
    "t14 = t13.flatten()\n",
    "print(t14)\n",
    "\n",
    "t15 = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]) # shape: (2, 2, 2)\n",
    "# flatten(start_dim=1): 1번 차원부터 끝까지를 펼친다.\n",
    "# (2, 2, 2) -> (2, 4)\n",
    "t17 = torch.flatten(t15, start_dim=1)\n",
    "print(t17)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e46f31-9a78-4181-b72a-6f0a88445a57",
   "metadata": {},
   "source": [
    "flatten은 모든 원소를 1차원으로 펼친다.\n",
    "start_dim을 지정하면 배치 차원 등 특정 차원은 유지한 채 나머지만 펼칠 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1d818190-fa74-4eb5-85c4-f4d649d326fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2, 3])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "t18 = torch.randn(2, 3, 5)\n",
    "# permute(2, 0, 1): (2, 3, 5) -> (5, 2, 3) 순서로 차원을 재배열한다.\n",
    "print(torch.permute(t18, (2, 0, 1)).size())\n",
    "\n",
    "\n",
    "t19 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "# permute(1, 0): 0번과 1번 차원의 순서를 바꾼다. (2, 3) -> (3, 2)\n",
    "t21 = torch.permute(t19, dims=(1, 0))\n",
    "print(t21)\n",
    "\n",
    "# transpose(0, 1): 0번과 1번 차원을 맞바꾼다. permute(1, 0)과 동일하다.\n",
    "t22 = torch.transpose(t19, 0, 1)\n",
    "print(t22)\n",
    "\n",
    "# t(): 2차원 텐서의 전치 행렬을 구한다. transpose(0, 1)의 단축형이다.\n",
    "t23 = torch.t(t19)\n",
    "print(t23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eda8d36-ea89-46f0-a5fe-8315d5d9742e",
   "metadata": {},
   "source": [
    "permute는 모든 차원의 순서를 자유롭게 재배열할 수 있는 함수이다.\n",
    "\n",
    "transpose는 두 개의 특정 차원만 맞바꾸는 연산이다.\n",
    "\n",
    "t는 2차원 텐서에만 사용할 수 있는 transpose의 축약형이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decd5b6b-8c38-4971-8ee2-e93a2e06c990",
   "metadata": {},
   "source": [
    "12. 텐서 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c2593cac-a2b9-443d-b012-d6eb8df68ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "torch.Size([8])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "torch.Size([4, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "torch.Size([2, 6])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8],\n",
      "        [ 3,  4,  5,  9, 10, 11]])\n",
      "torch.Size([6, 3])\n",
      "torch.Size([2, 9])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([1, 4, 3])\n",
      "torch.Size([1, 2, 6])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.zeros([2, 1, 3])\n",
    "t2 = torch.zeros([2, 3, 3])\n",
    "t3 = torch.zeros([2, 2, 3])\n",
    "\n",
    "# dim=1을 기준으로 t1, t2, t3를 연결한다.\n",
    "# (2, 1, 3), (2, 3, 3), (2, 2, 3) -> (2, 1+3+2, 3)\n",
    "t4 = torch.cat([t1, t2, t3], dim=1)\n",
    "print(t4.shape)\n",
    "\n",
    "t5 = torch.arange(0, 3)\n",
    "t6 = torch.arange(3, 8)\n",
    "\n",
    "# 1차원 텐서를 연결한다.\n",
    "t7 = torch.cat((t5, t6), dim=0)\n",
    "print(t7.shape)\n",
    "print(t7)\n",
    "\n",
    "t8 = torch.arange(0, 6).reshape(2, 3)\n",
    "t9 = torch.arange(6, 12).reshape(2, 3)\n",
    "\n",
    "# 2차원 텐서를 0번 차원(행) 기준으로 연결한다.\n",
    "t10 = torch.cat((t8, t9), dim=0)\n",
    "print(t10.size())\n",
    "print(t10)\n",
    "\n",
    "# 2차원 텐서를 1번 차원(열) 기준으로 연결한다.\n",
    "t11 = torch.cat((t8, t9), dim=1)\n",
    "print(t11.size())\n",
    "print(t11)\n",
    "\n",
    "t12 = torch.arange(0, 6).reshape(2, 3)\n",
    "t13 = torch.arange(6, 12).reshape(2, 3)\n",
    "t14 = torch.arange(12, 18).reshape(2, 3)\n",
    "\n",
    "# 3개의 2차원 텐서를 0번 차원 기준으로 연결한다.\n",
    "t15 = torch.cat((t12, t13, t14), dim=0)\n",
    "print(t15.size())\n",
    "\n",
    "# 3개의 2차원 텐서를 1번 차원 기준으로 연결한다.\n",
    "t16 = torch.cat((t12, t13, t14), dim=1)\n",
    "print(t16.size())\n",
    "\n",
    "t17 = torch.arange(0, 6).reshape(1, 2, 3)\n",
    "t18 = torch.arange(6, 12).reshape(1, 2, 3)\n",
    "\n",
    "# 3차원 텐서를 0, 1, 2번 각 차원 기준으로 연결한다.\n",
    "t19 = torch.cat((t17, t18), dim=0)\n",
    "print(t19.size())\n",
    "t20 = torch.cat((t17, t18), dim=1)\n",
    "print(t20.size())\n",
    "t21 = torch.cat((t17, t18), dim=2)\n",
    "print(t21.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d9834c-4390-4e27-b8cc-41eb97f40a58",
   "metadata": {},
   "source": [
    "torch.cat은 텐서들의 리스트 또는 튜플을 입력으로 받는다.\n",
    "\n",
    "dim으로 지정된 차원을 제외한 나머지 모든 차원의 크기는 반드시 동일해야 한다.\n",
    "\n",
    "결과 텐서의 dim 차원 크기는 입력 텐서들의 해당 차원 크기들의 합이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3898725c-d007-484f-ae8c-9d0161a98029",
   "metadata": {},
   "source": [
    "13. 텐서 쌓기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f95bbfbf-c593-4576-9fcc-37756981565e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 3, 2]) True\n",
      "torch.Size([2, 3])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "torch.Size([3, 2])\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "# dim=0: 0번 차원을 새로 만들어 쌓는다. (2, 3) -> (2, 2, 3)\n",
    "t3 = torch.stack([t1, t2], dim=0)\n",
    "t4 = torch.cat([t1.unsqueeze(dim=0), t2.unsqueeze(dim=0)], dim=0)\n",
    "print(t3.shape, t3.equal(t4))\n",
    "\n",
    "# dim=1: 1번 차원을 새로 만들어 쌓는다. (2, 3) -> (2, 2, 3)\n",
    "t5 = torch.stack([t1, t2], dim=1)\n",
    "t6 = torch.cat([t1.unsqueeze(dim=1), t2.unsqueeze(dim=1)], dim=1)\n",
    "print(t5.shape, t5.equal(t6))\n",
    "\n",
    "# dim=2: 2번 차원을 새로 만들어 쌓는다. (2, 3) -> (2, 3, 2)\n",
    "t7 = torch.stack([t1, t2], dim=2)\n",
    "t8 = torch.cat([t1.unsqueeze(dim=2), t2.unsqueeze(dim=2)], dim=2)\n",
    "print(t7.shape, t7.equal(t8))\n",
    "\n",
    "t9 = torch.arange(0, 3)\n",
    "t10 = torch.arange(3, 6)\n",
    "\n",
    "# 1차원 텐서를 0번 차원 기준으로 쌓는다. (3,) -> (2, 3)\n",
    "t11 = torch.stack((t9, t10), dim=0)\n",
    "print(t11.size())\n",
    "print(t11)\n",
    "\n",
    "# 1차원 텐서를 1번 차원 기준으로 쌓는다. (3,) -> (3, 2)\n",
    "t13 = torch.stack((t9, t10), dim=1)\n",
    "print(t13.size())\n",
    "print(t13)\n",
    "\n",
    "# stack과 unsqueeze+cat의 동작이 동일한지 확인\n",
    "t12 = torch.cat((t9.unsqueeze(0), t10.unsqueeze(0)), dim=0)\n",
    "print(t11.equal(t12))\n",
    "t14 = torch.cat((t9.unsqueeze(1), t10.unsqueeze(1)), dim=1)\n",
    "print(t13.equal(t14))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e117e4-ba72-4581-a53e-9383642e1cc4",
   "metadata": {},
   "source": [
    "torch.stack에 입력되는 모든 텐서는 크기가 완전히 동일해야 한다.\n",
    "\n",
    "torch.cat이 기존 차원을 따라 텐서를 연결하는 반면, torch.stack은 새로운 차원을 추가하여 텐서를 연결한다.\n",
    "\n",
    "결과 텐서는 입력 텐서보다 차원이 하나 더 많아진다.\n",
    "\n",
    "torch.stack([t1, t2], dim=N) 연산은 각 텐서에 unsqueeze(N)를 적용한 뒤 torch.cat을 dim=N으로 수행한 것과 동일하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc9b433-e076-4d66-8625-dcd8ee4c4a0c",
   "metadata": {},
   "source": [
    "14. 텐서 쌓기 방향"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "41374084-c940-4705-8523-d54d50e4d99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6]])\n",
      "torch.Size([4, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1차원 텐서들은 각각 행으로 취급되어 쌓인다. (3,) -> (2, 3)\n",
    "t1 = torch.tensor([1, 2, 3])\n",
    "t2 = torch.tensor([4, 5, 6])\n",
    "t3 = torch.vstack((t1, t2))\n",
    "print(t3)\n",
    "\n",
    "# 2차원 텐서들은 0번 차원을 기준으로 연결된다. (3, 1) -> (6, 1)\n",
    "t4 = torch.tensor([[1], [2], [3]])\n",
    "t5 = torch.tensor([[4], [5], [6]])\n",
    "t6 = torch.vstack((t4, t5))\n",
    "print(t6)\n",
    "\n",
    "# 3차원 텐서들도 0번 차원을 기준으로 연결된다. (2, 2, 3) -> (4, 2, 3)\n",
    "t7 = torch.tensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n",
    "t8 = torch.tensor([[[13, 14, 15], [16, 17, 18]], [[19, 20, 21], [22, 23, 24]]])\n",
    "t9 = torch.vstack([t7, t8])\n",
    "print(t9.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5ef1a3-2474-436b-b447-00d651a66535",
   "metadata": {},
   "source": [
    "torch.vstack은 torch.cat을 dim=0으로 설정하여 수행하는 것과 유사한 동작을 한다.\n",
    "\n",
    "입력 텐서들의 0번 차원을 제외한 나머지 차원들의 크기는 모두 동일해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9dd4118d-6fb8-4552-944f-e2795a3a9b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "torch.Size([2, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "# 1차원 텐서들은 그대로 이어 붙는다. (3,) -> (6,)\n",
    "t10 = torch.tensor([1, 2, 3])\n",
    "t11 = torch.tensor([4, 5, 6])\n",
    "t12 = torch.hstack((t10, t11))\n",
    "print(t12)\n",
    "\n",
    "# 2차원 텐서들은 1번 차원(열)을 기준으로 연결된다. (3, 1) -> (3, 2)\n",
    "t13 = torch.tensor([[1], [2], [3]])\n",
    "t14 = torch.tensor([[4], [5], [6]])\n",
    "t15 = torch.hstack((t13, t14))\n",
    "print(t15)\n",
    "\n",
    "# 3차원 텐서들은 1번 차원을 기준으로 연결된다. (2, 2, 3) -> (2, 4, 3)\n",
    "t16 = torch.tensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n",
    "t17 = torch.tensor([[[13, 14, 15], [16, 17, 18]], [[19, 20, 21], [22, 23, 24]]])\n",
    "t18 = torch.hstack([t16, t17])\n",
    "print(t18.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d0356d-e37a-48ef-8480-258aed82cc52",
   "metadata": {},
   "source": [
    "torch.hstack은 입력 텐서가 1차원일 경우 0번 축을 따라, 2차원 이상일 경우 1번 축을 따라 텐서를 연결한다.\n",
    "\n",
    "연결되는 차원을 제외한 나머지 차원들의 크기는 모두 동일해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5feed5a-27d0-4413-95e8-cf4d92866608",
   "metadata": {},
   "source": [
    "숙제 후기 \n",
    "딥러닝의 기초가 되는 텐서의 연산에 대해 알아보며 파이토치에 대해 잘 알 수 있었고 shape의 중요성을 느끼게 되는 숙제였던 것 같다. \n",
    "불평을 조금 해보자면 코드가 너무 많았다 텐서를 하나하나 새로 생성하고 출력하다보니 양이 많았던 것 같다.\n",
    "하고싶은 말은 수업의 템포이다. 이전에 ai기초 강의를 들었을 땐 수업이 너무 빨라 대강 이해하고 넘어가는 경우가 다반사였지만 한연희 교수님의 수업을 들으니 하나의 개념이라도 확실히 짚고 넘어가다 보니 이해가 착착 되어 좋았다. \n",
    "앞으로도 좋은 수업 부탁드립니다. 항상 잘 듣고 있습니다!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
